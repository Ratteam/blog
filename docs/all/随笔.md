<TOC />

### CRUD
```
什么是CRUD？ CRUD的操作
  CRUD是指在做计算处理时的增加(Create)、读取查询(Retrieve)、更新(Update)和删除(Delete)几个单词的首字母简写。主要被用在描述软件系统中DataBase或者持久层的基本操作功能。

数据库CRUD操作

一、删除表   
drop table 表名称


二、修改表  

alter table 表名称 add 列名 数据类型（add表示添加一列）
alter table 表名称 drop column 列名称（column表示列 drop表示删除）

三、删除数据库

drop database 数据库


四、CRUD操作（create 添加数据read读取数据 update 修改数据delete删除数据）

1、添加数据(create)
a: insert into + nation values（'n002 ','回族 '）--加单引号是转为字符串，英文的
b: insert into nation values('n003',' ')只添加一列后面的是空给所有的添加可以用
c: insert into nation(code,) values('n004')给某一列添加可以用
d:给多列添加insert into nation(code,name) values('n004','维吾尔族')
e: 专门添加自增长列的 insert into 表名 values('p001','p006') 自增长列不用管，直接写第二列


2、删除数据（delete）

delete from +表名称--删除表中所有内容
delete from +表名称 where ids=5  (删除此行)---where后面跟一个条件


3、修改数据（uodate）
update +表名称 set +列名称=' 'set(设置)---修改所有的内容这一列的
update +表名称 set +列名称='p006 ' where ids=6
update +表名称 set +列名称='p006 ',列名称='p002' where ids=6-----用逗号隔开可以修改多列
整数型（int）的不需要加单引号 0 (false)1(true)


4、查询数据(10种)
a1:简单查询
select * from 表名称 ——查询表中所有数据 *代表所有列
select code,name from 表名称——查询指定列数据
select code,name from 表名称——查指定列的数据
select code as'代号',name as'姓名' from 表名称——给列指定别名
a2:条件查询
select * from 表名 where code=' '   查这一行
select * from 表名 where sex='true' and nation=' '   表示并列，--多条件并的关系
select * from 表        名 where sex='true' or nation=' ' --多条件或的关系
a3:范围查询
select * from 表名 where 列名>40 and 列名<50
select * from 表名 where 列名 between 40 and 50  --专用于范围查询
a4:离散查询
select * from 表名 where 列名 in (' ',' ',' ')
select * from 表名 where 列名 not in (' ',' ',' ')  反选，不在里面的
a5:模糊查询
select * from 表名 where 列名 like '%宝马%'——查包含宝马的
select * from 表名 where 列名 like '宝马%'——查以宝马开头的
select * from 表名 where 列名 like '%宝马'——查以宝马结尾的
select * from 表名 where 列名 like '宝马'——查等于宝马的
select * from 表名 where 列名 like '--E'——查第三个是E的
% 代表是任意多个字符
- 下划线 代表是一个字符
a6:排序查询
select * from 表名 order by 列名——默认升序排序
select * from 表名 order by 列名 desc——降序排列
select * from 表名 order by 列名 desc, 列名 asc——多个条件排序   ， 前面是主条件 后面是次要条件
desc 降序  ，asc 升序， order by  排序  根据哪一列排序
a7:分页查询
select top 5 * from 表名——查询前5条数据
select top 5 * from 表名 where code not in (select top 5 code from car)
a8:去重查询（去掉重复的）
select distinct 列名 from
a9:分组查询
select Brand from 表名 group by Brand having count(*)>2

group by    having ——表示根据一列分组 ，count(*)>2——每一组的数量
a10:聚合函数（统计查询）
select count (*) from 表名——查询所有数据条数（每一列的）
select count (列名主键) from 表名——查询这列的所有数据条数（执行快）
select sum (列名) from 表名——求和
select avg  (列名) from 表名——求平均值
select max (列名) from 表名——求最大值
select min (列名) from 表名——求最小值
```

### JOIN、LEFT JOIN 、RIGHT JOIN、INNER JOIN
```
通俗讲：
left以 left join 左侧的表为主表
right 以 right join 右侧表为主表
inner join 查找的数据是左右两张表共有的

left join(左联接) 返回包括左表中的所有记录和右表中联结字段相等的记录
right join(右联接) 返回包括右表中的所有记录和左表中联结字段相等的记录
inner join(等值连接) 只返回两个表中联结字段相等的行

举例如下：
--------------------------------------------
表A记录如下：
aID　　　　　aNum
1　　　　　a20050111
2　　　　　a20050112
3　　　　　a20050113
4　　　　　a20050114
5　　　　　a20050115

表B记录如下:
bID　　　　　bName
1　　　　　2006032401
2　　　　　2006032402
3　　　　　2006032403
4　　　　　2006032404
8　　　　　2006032408

--------------------------------------------
1.left join
sql语句如下:
select * from A
left join B
on A.aID = B.bID

结果如下:
aID　　　　　aNum　　　　　bID　　　　　bName
1　　　　　a20050111　　　　1　　　　　2006032401
2　　　　　a20050112　　　　2　　　　　2006032402
3　　　　　a20050113　　　　3　　　　　2006032403
4　　　　　a20050114　　　　4　　　　　2006032404
5　　　　　a20050115　　　　NULL　　　　　NULL

（所影响的行数为 5 行）
结果说明:
left join是以A表的记录为基础的,A可以看成左表,B可以看成右表,left join是以左表为准的.
换句话说,左表(A)的记录将会全部表示出来,而右表(B)只会显示符合搜索条件的记录(例子中为: A.aID = B.bID).
B表记录不足的地方均为NULL.
--------------------------------------------
2.right join
sql语句如下:
select * from A
right join B
on A.aID = B.bID

结果如下:
aID　　　　　aNum　　　　　bID　　　　　bName
1　　　　　a20050111　　　　1　　　　　2006032401
2　　　　　a20050112　　　　2　　　　　2006032402
3　　　　　a20050113　　　　3　　　　　2006032403
4　　　　　a20050114　　　　4　　　　　2006032404
NULL　　　　　NULL　　　　　8　　　　　2006032408

（所影响的行数为 5 行）
结果说明:
仔细观察一下,就会发现,和left join的结果刚好相反,这次是以右表(B)为基础的,A表不足的地方用NULL填充.
--------------------------------------------
3.inner join
sql语句如下:
select * from A
innerjoin B
on A.aID = B.bID

结果如下:
aID　　　　　aNum　　　　　bID　　　　　bName
1　　　　　a20050111　　　　1　　　　　2006032401
2　　　　　a20050112　　　　2　　　　　2006032402
3　　　　　a20050113　　　　3　　　　　2006032403
4　　　　　a20050114　　　　4　　　　　2006032404

结果说明:
很明显,这里只显示出了 A.aID = B.bID的记录.这说明inner join并不以谁为基础,它只显示符合条件的记录.
--------------------------------------------
注:
LEFT JOIN操作用于在任何的 FROM 子句中，组合来源表的记录。使用 LEFT JOIN 运算来创建一个左边外部联接。左边外部联接将包含了从第一个（左边）开始的两个表中的全部记录，即使在第二个（右边）表中并没有相符值的记录。

语法：FROM table1 LEFT JOIN table2 ON table1.field1 compopr table2.field2

说明：table1, table2参数用于指定要将记录组合的表的名称。
field1, field2参数指定被联接的字段的名称。且这些字段必须有相同的数据类型及包含相同类型的数据，但它们不需要有相同的名称。
compopr参数指定关系比较运算符："="， "<"， ">"， "<="， ">=" 或 "<>"。
如果在INNER JOIN操作中要联接包含Memo 数据类型或 OLE Object 数据类型数据的字段，将会发生错误.
```
### UNION
```
SQL UNION 操作符
SQL UNION 操作符合并两个或多个 SELECT 语句的结果。

SQL UNION 操作符
UNION 操作符用于合并两个或多个 SELECT 语句的结果集。

请注意，UNION 内部的每个 SELECT 语句必须拥有相同数量的列。列也必须拥有相似的数据类型。同时，每个 SELECT 语句中的列的顺序必须相同。

SQL UNION 语法
SELECT column_name(s) FROM table1
UNION
SELECT column_name(s) FROM table2;
注释：默认地，UNION 操作符选取不同的值。如果允许重复的值，请使用 UNION ALL。

SQL UNION ALL 语法
SELECT column_name(s) FROM table1
UNION ALL
SELECT column_name(s) FROM table2;
注释：UNION 结果集中的列名总是等于 UNION 中第一个 SELECT 语句中的列名。

演示数据库
在本教程中，我们将使用 RUNOOB 样本数据库。

下面是选自 "Websites" 表的数据：

mysql> SELECT * FROM Websites;
+----+--------------+---------------------------+-------+---------+
| id | name         | url                       | alexa | country |
+----+--------------+---------------------------+-------+---------+
| 1  | Google       | https://www.google.cm/    | 1     | USA     |
| 2  | 淘宝          | https://www.taobao.com/   | 13    | CN      |
| 3  | 菜鸟教程      | http://www.runoob.com/    | 4689  | CN      |
| 4  | 微博          | http://weibo.com/         | 20    | CN      |
| 5  | Facebook     | https://www.facebook.com/ | 3     | USA     |
| 7  | stackoverflow | http://stackoverflow.com/ |   0 | IND     |
+----+---------------+---------------------------+-------+---------+
下面是 "apps" APP 的数据：

mysql> SELECT * FROM apps;
+----+------------+-------------------------+---------+
| id | app_name   | url                     | country |
+----+------------+-------------------------+---------+
|  1 | QQ APP     | http://im.qq.com/       | CN      |
|  2 | 微博 APP | http://weibo.com/       | CN      |
|  3 | 淘宝 APP | https://www.taobao.com/ | CN      |
+----+------------+-------------------------+---------+
3 rows in set (0.00 sec)
SQL UNION 实例
下面的 SQL 语句从 "Websites" 和 "apps" 表中选取所有不同的country（只有不同的值）：

实例
SELECT country FROM Websites
UNION
SELECT country FROM apps
ORDER BY country;
执行以上 SQL 输出结果如下：


注释：UNION 不能用于列出两个表中所有的country。如果一些网站和APP来自同一个国家，每个国家只会列出一次。UNION 只会选取不同的值。请使用 UNION ALL 来选取重复的值！

SQL UNION ALL 实例
下面的 SQL 语句使用 UNION ALL 从 "Websites" 和 "apps" 表中选取所有的country（也有重复的值）：

实例
SELECT country FROM Websites
UNION ALL
SELECT country FROM apps
ORDER BY country;
执行以上 SQL 输出结果如下：



带有 WHERE 的 SQL UNION ALL
下面的 SQL 语句使用 UNION ALL 从 "Websites" 和 "apps" 表中选取所有的中国(CN)的数据（也有重复的值）：

实例
SELECT country, name FROM Websites
WHERE country='CN'
UNION ALL
SELECT country, app_name FROM apps
WHERE country='CN'
ORDER BY country;
```

### GROUP BY + COUNT + WHERE 组合案例
```
1 需求是 求订单表1个月内 订单累计费用超过500的有多少人 
根据题意 最先写出的sql是这样的 

SELECT SUM(totalfee)AS n  FROM sendorder WHERE  `addtime` > UNIX_TIMESTAMP('2015-12-01') AND `addtime` < UNIX_TIMESTAMP('2016-01-01')  AND state IN(1,2,3,8) GROUP BY uid HAVING  n > 500

但是这样算出来的只是一个数组 不好处理 所有 把这里查询的结果做为一个临时表 算这个表个数

select count(*)from (sum group  having) as t

```
### [常用 MySQL 函数，如：now()、md5()、concat()、uuid()等](https://www.w3schools.com/sql/sql_ref_mysql.asp)
```
length()表示返回字节长度。
char_length()表示返回字符个数。
rand()表示返回0-1之间的随机小数。
round()表示四舍五入。
date_add(now(),interval 10 day)表示10天后的日期。
date_add(now(),interval -10 day)表示10天前的日期。
date_add(now(),interval 10 minute)表示20分钟后的时间。
date_add(now(),interval 10 week)表示10周后的日期
datediff()表示两个日期之间相差的天数。
timestampdiff(hour,now(),'2018-01-21 12:10:20')表示返回两个日期之间的小时差。
date_format(now(),'%Y-%m-%d %H:%i:%s')表示格式化一个日期时间。
from_unixtime()表示将一个时间戳转换为一个日期。
unix_timestamp()表示将一个日期转换为一个时间戳。
from_unixtime(时间戳,'%Y-%m-%d %H:%i:%s')表示将时间戳按指定的格式转换为一个日期。
concat()连接字符串。
repeat('abc',2)表示一个字符串重复几次。
space(20)表示产生20个空格的字符串。
md5(),sha(),sha1(),password()表示加密函数。
supper()表示将字符串转换为大写。
lower()表示将字符串转换为小写。
left()表示从左截取多少长度的子字符串。
right()表示从右截取多少长度的子字符串。
mid()表示从指定位置截取多少长度的子字符串。索引从1开始。
ifnull(表达式1，表达式2)如果表达式1不为空就显示表达式1，否则显示表达式2。
if(表达式，结果1，结果2)如果表达式为真就显示结果1，否则显示结果2。
uuid()表示返回36位唯一随机字符串。
format(1000,2)表示格式化一个数字。
inet-aton()将一个ip地址的字符串转换为一个数字。
inet-ntoa()将一个数字转换为一个ip地址。
```

### 了解触发器是什么，说个使用场景
```
触发器，需要触发条件，当条件满足后做什么操作。
触发器用处：比如校内网、开心网、Facebook,你发一个日志，自动通知好友，其实就是在增加日志时做一个后触发，再向通知表中写入条目。

其是一种特殊的存储过程。一般的存储过程是通过存储过程名直接调用，而触发器主要是通过事件(增、删、改)进行触发而被执行的。其在表中数据发生变化时自动强制执行。常见的触发器有两种：after(for)、instead of,用于insert、update、delete事件。
after(for)       表示执行代码后，执行触发器
instead of       表示执行代码前，用已经写好的触发器代替你的操作

触发器语法：
　　create trigger 触发器的名字   on 操作表
　　for|after        instead of
　　update|insert|delete
　　as
　　SQL语句

触发器示例
--禁止用户插入数据(实际上是先插入，然后立刻将其删除！)
　　create trigger tr_insert on bank
　　for         --for表示执行之后的操作
　　insert      --即先执行了插入操作，同时在临时表中保存了插入记录
　　as
 　　--执行完插入之后，在新生成的表中将刚刚插入的那条记录删除，
　　 --而此时得到的刚刚插入的记录的id是通过临时表 inserted得到的
　　delete * from bank where cid=(select cid from inserted)
　　生成上面的触发器后，当用户再输入insert语句后就见不到效果了！
　　如：insert into bank values('0004',10000),是插入不进数据库的。
```

### 数据库优化手段
```
关于数据库优化，网上有不少资料和方法，但是不少质量参差不齐，有些总结的不够到位，内容冗杂。
　　偶尔发现了这篇文章，总结得很经典，文章流量也很大，所以拿到自己的总结文集中，积累优质文章，提升个人能力，希望对大家今后开发中也有帮助

1、选取最适用的字段属性
MySQL可以很好的支持大数据量的存取，但是一般说来，数据库中的表越小，在它上面执行的查询也就会越快。因此，在创建表的时候，为了获得更好的性能，我们可以将表中字段的宽度设得尽可能小。

例如，在定义邮政编码这个字段时，如果将其设置为CHAR(255),显然给数据库增加了不必要的空间，甚至使用VARCHAR这种类型也是多余的，因为CHAR(6)就可以很好的完成任务了。同样的，如果可以的话，我们应该使用MEDIUMINT而不是BIGIN来定义整型字段。

另外一个提高效率的方法是在可能的情况下，应该尽量把字段设置为NOT NULL，这样在将来执行查询的时候，数据库不用去比较NULL值。

对于某些文本字段，例如“省份”或者“性别”，我们可以将它们定义为ENUM类型。因为在MySQL中，ENUM类型被当作数值型数据来处理，而数值型数据被处理起来的速度要比文本类型快得多。这样，我们又可以提高数据库的性能。

2、使用连接（JOIN）来代替子查询(Sub-Queries)
MySQL从4.1开始支持SQL的子查询。这个技术可以使用SELECT语句来创建一个单列的查询结果，然后把这个结果作为过滤条件用在另一个查询中。例如，我们要将客户基本信息表中没有任何订单的客户删除掉，就可以利用子查询先从销售信息表中将所有发出订单的客户ID取出来，然后将结果传递给主查询，如下所示：

DELETE  FROM  customerinfo

WHERE  CustomerID  NOT  in  (SELECT customerid  FROM  salesinfo)
使用子查询可以一次性的完成很多逻辑上需要多个步骤才能完成的SQL操作，同时也可以避免事务或者表锁死，并且写起来也很容易。但是，有些情况下，子查询可以被更有效率的连接（JOIN）..替代。例如，假设我们要将所有没有订单记录的用户取出来，可以用下面这个查询完成：

SELECT  *  FROM  customerinfo

WHERE  customerid  NOT IN (SELECT customerid   FROM   salesinfo)
如果使用连接（JOIN）..来完成这个查询工作，速度将会快很多。尤其是当salesinfo表中对CustomerID建有索引的话，性能将会更好，查询如下：

SELECT  *  FROM  customerinfo

LEFT  JOIN  salesinfo  ON   customerinfo.customerid =salesinfo.customerid

WHERE  salesinfo.customerid   IS NULL
连接（JOIN）..之所以更有效率一些，是因为MySQL不需要在内存中创建临时表来完成这个逻辑上的需要两个步骤的查询工作。

3、使用联合(UNION)来代替手动创建的临时表
MySQL从4.0的版本开始支持union查询，它可以把需要使用临时表的两条或更多的select查询合并的一个查询中。在客户端的查询会话结束的时候，临时表会被自动删除，从而保证数据库整齐、高效。使用union来创建查询的时候，我们只需要用UNION作为关键字把多个select语句连接起来就可以了，要注意的是所有select语句中的字段数目要想同。下面的例子就演示了一个使用UNION的查询。

SELECT   name,phone  FROM  client UNION

SELECT  name,birthdate  FROM  author  UNION

SELECT  name,supplier FROM product
4、事务
尽管我们可以使用子查询（Sub-Queries）、连接（JOIN）和联合（UNION）来创建各种各样的查询，但不是所有的数据库操作都可以只用一条或少数几条SQL语句就可以完成的。更多的时候是需要用到一系列的语句来完成某种工作。但是在这种情况下，当这个语句块中的某一条语句运行出错的时候，整个语句块的操作就会变得不确定起来。设想一下，要把某个数据同时插入两个相关联的表中，可能会出现这样的情况：第一个表中成功更新后，数据库突然出现意外状况，造成第二个表中的操作没有完成，这样，就会造成数据的不完整，甚至会破坏数据库中的数据。要避免这种情况，就应该使用事务，它的作用是：要么语句块中每条语句都操作成功，要么都失败。换句话说，就是可以保持数据库中数据的一致性和完整性。事物以BEGIN关键字开始，COMMIT关键字结束。在这之间的一条SQL操作失败，那么，ROLLBACK命令就可以把数据库恢复到BEGIN开始之前的状态。

BEGIN;
  INSERT   INTO   salesinfo   SET   customerid=14;
  UPDATE   inventory   SET   quantity =11   WHERE   item='book';
COMMIT;
事务的另一个重要作用是当多个用户同时使用相同的数据源时，它可以利用锁定数据库的方法来为用户提供一种安全的访问方式，这样可以保证用户的操作不被其它的用户所干扰。

5、锁定表
尽管事务是维护数据库完整性的一个非常好的方法，但却因为它的独占性，有时会影响数据库的性能，尤其是在很大的应用系统中。由于在事务执行的过程中，数据库将会被锁定，因此其它的用户请求只能暂时等待直到该事务结束。如果一个数据库系统只有少数几个用户来使用，事务造成的影响不会成为一个太大的问题；但假设有成千上万的用户同时访问一个数据库系统，例如访问一个电子商务网站，就会产生比较严重的响应延迟。

其实，有些情况下我们可以通过锁定表的方法来获得更好的性能。下面的例子就用锁定表的方法来完成前面一个例子中事务的功能。

LOCK TABLE inventory WRITE SELECT quantity  FROM   inventory   WHERE Item='book';

...

UPDATE   inventory   SET   Quantity=11   WHERE  Item='book';UNLOCKTABLES
这里，我们用一个select语句取出初始数据，通过一些计算，用update语句将新值更新到表中。包含有WRITE关键字的LOCKTABLE语句可以保证在UNLOCKTABLES命令被执行之前，不会有其它的访问来对inventory进行插入、更新或者删除的操作。

6、使用外键
锁定表的方法可以维护数据的完整性，但是它却不能保证数据的关联性。这个时候我们就可以使用外键。

例如，外键可以保证每一条销售记录都指向某一个存在的客户。在这里，外键可以把customerinfo表中的customerid映射到salesinfo表中customerid，任何一条没有合法customerid的记录都不会被更新或插入到salesinfo中。

CREATE  TABLE   customerinfo( customerid   int primary key) engine = innodb;

CREATE  TABLE   salesinfo( salesid int not null,customerid  int not null, primary key(customerid,salesid),foreign key(customerid)  references  customerinfo(customerid) on delete cascade)engine = innodb;

注意例子中的参数“on delete cascade”。该参数保证当customerinfo表中的一条客户记录被删除的时候，salesinfo表中所有与该客户相关的记录也会被自动删除。如果要在MySQL中使用外键，一定要记住在创建表的时候将表的类型定义为事务安全表InnoDB类型。该类型不是MySQL表的默认类型。定义的方法是在CREATE TABLE语句中加上engine=INNODB。如例中所示。

7、使用索引
索引是提高数据库性能的常用方法，它可以令数据库服务器以比没有索引快得多的速度检索特定的行，尤其是在查询语句当中包含有MAX(),MIN()和ORDERBY这些命令的时候，性能提高更为明显。

那该对哪些字段建立索引呢？

一般说来，索引应建立在那些将用于JOIN,WHERE判断和ORDERBY排序的字段上。尽量不要对数据库中某个含有大量重复的值的字段建立索引。对于一个ENUM类型的字段来说，出现大量重复值是很有可能的情况

例如customerinfo中的“province”..字段，在这样的字段上建立索引将不会有什么帮助；相反，还有可能降低数据库的性能。我们在创建表的时候可以同时创建合适的索引，也可以使用ALTERTABLE或CREATEINDEX在以后创建索引。此外，MySQL从版本3.23.23开始支持全文索引和搜索。全文索引在MySQL中是一个FULLTEXT类型索引，但仅能用于MyISAM类型的表。对于一个大的数据库，将数据装载到一个没有FULLTEXT索引的表中，然后再使用ALTERTABLE或CREATEINDEX创建索引，将是非常快的。但如果将数据装载到一个已经有FULLTEXT索引的表中，执行过程将会非常慢。

8、优化的查询语句
绝大多数情况下，使用索引可以提高查询的速度，但如果SQL语句使用不恰当的话，索引将无法发挥它应有的作用。

下面是应该注意的几个方面。

a、 首先，最好是在相同类型的字段间进行比较的操作

在MySQL3.23版之前，这甚至是一个必须的条件。例如不能将一个建有索引的INT字段和BIGINT字段进行比较；但是作为特殊的情况，在CHAR类型的字段和VARCHAR类型字段的字段大小相同的时候，可以将它们进行比较。

b、 其次，在建有索引的字段上尽量不要使用函数进行操作

例如，在一个DATE类型的字段上使用YEAE()函数时，将会使索引不能发挥应有的作用。所以，下面的两个查询虽然返回的结果一样，但后者要比前者快得多。

ｃ、第三，在搜索字符型字段时，我们有时会使用LIKE关键字和通配符，这种做法虽然简单，但却也是以牺牲系统性能为代价的

例如下面的查询将会比较表中的每一条记录。


SELECT  *  FROM  books  WHERE  name  like   "MySQL%"

但是如果换用下面的查询，返回的结果一样，但速度就要快上很多：


SELECT  *  FROM  books  WHERE  name ＞=  "MySQL"  and  name  ＜"MySQM"
最后，应该注意避免在查询中让MySQL进行自动类型转换，因为转换过程也会使索引变得不起作用。
```

### 索引、联合索引（命中条件）
```
首先明确：为什么要用联合索引？

对于查询语句“SELECT E.* FROM E WHERE E.e1=1 AND E.e3=2”涉及到两列，这个时候我们一般采用一个联合索引(e1, e3)；而不用两个单列索引，这是因为一条查询语句往往应为mysql优化器的关系只用一个索引，就算你有两个索引，他也只用一个；在只用一个的基础之上，联合索引是会比单列索引要快的；

下面讲讲联合索引的使用规则和哪些情况会命中不了联合索引


示例如下。首先创建表：
CREATE TABLE E (e1 INT, e2 VARCHAR(9), e3 INT, PRIMARY KEY(e1, e3));
这样就建立了一个联合索引：e1,e3

测试数据

INSERT INTO E
(e1, e2, e3)
VALUES(1, 'aa', 2);


触发联合索引是有条件的：
1、使用联合索引的全部索引键，可触发索引的使用。
例如：SELECT E.* FROM E WHERE E.e1=1 AND E.e3=2

2、使用联合索引的前缀部分索引键，如“key_part_1 <op>常量”，可触发索引的使用。
例如：SELECT E.* FROM E WHERE E.e1=1

3、使用部分索引键，但不是联合索引的前缀部分，如“key_part_2 <op>常量”，不可触发索引的使用。
例如：SELECT E.* FROM E WHERE E.e3=1

4、使用联合索引的全部索引键，但索引键不是AND操作，不可触发索引的使用。
例如：SELECT E.* FROM E WHERE E.e3=2 OR E.e1=1

以上通过explain测试即可看出效果
```

### 分库分表（`水平分表`、`垂直分表`）
```
https://blog.csdn.net/weixin_44062339/article/details/100491744
https://www.cnblogs.com/technologykai/articles/10830458.html
分库分表是什么
下边以电商系统中的例子来说明，下图是电商系统卖家模块的表结构：
在这里插入图片描述
通过以下SQL能够获取到商品相关的店铺信息、地理区域信息：

SELECT p.*,r.[地理区域名称],s.[店铺名称],s.[信誉]
FROM [商品信息] p 
LEFT JOIN [地理区域] r ON p.[产地] = r.[地理区域编码]
LEFT JOIN [店铺信息] s ON p.id = s.[所属店铺]
WHERE p.id = ?
1
2
3
4
5
随着公司业务快速发展，数据库中的数据量猛增，访问性能也变慢了，优化迫在眉睫。分析一下问题出现在哪儿呢？ 关系型数据库本身比较容易成为系统瓶颈，单机存储容量、连接数、处理能力都有限。当单表的数据量达到1000W或100G以后，由于查询维度较多，即使添加从库、优化索引，做很多操作时性能仍下降严重。

方案1：

通过提升服务器硬件能力来提高数据处理能力，比如增加存储容量 、CPU等，这种方案成本很高，并且如果瓶颈在MySQL本身那么提高硬件也是有很的。

方案2：

把数据分散在不同的数据库中，使得单一数据库的数据量变小来缓解单一数据库的性能问题，从而达到提升数据库性能的目的，如下图：将电商数据库拆分为若干独立的数据库，并且对于大表也拆分为若干小表，通过这种数据库拆分的方法来解决数据库的性能问题。
在这里插入图片描述
分库分表就是为了解决由于数据量过大而导致数据库性能降低的问题，将原来独立的数据库拆分成若干数据库组成 ，将数据大表拆分成若干数据表组成，使得单一数据库、单一数据表的数据量变小，从而达到提升数据库性能的目的。

垂直分表
分库分表包括分库和分表两个部分，在生产中通常包括：垂直分库、水平分库、垂直分表、水平分表四种方式。
先说 垂直分表：
通常在商品列表中是不显示商品详情信息的，如下图：
在这里插入图片描述
用户在浏览商品列表时，只有对某商品感兴趣时才会查看该商品的详细描述。因此，商品信息中商品描述字段访问频次较低，且该字段存储占用空间较大，访问单个数据IO时间较长；商品信息中商品名称、商品图片、商品价格等其他字段数据访问频次较高。

由于这两种数据的特性不一样，因此他考虑将商品信息表拆分如下：

将访问频次低的商品描述信息单独存放在一张表中，访问频次较高的商品基本信息单独放在一张表中。
在这里插入图片描述
商品列表可采用以下sql：

SELECT p.*,r.[地理区域名称],s.[店铺名称],s.[信誉]
FROM [商品信息] p 
LEFT JOIN [地理区域] r ON p.[产地] = r.[地理区域编码]
LEFT JOIN [店铺信息] s ON p.id = s.[所属店铺]
WHERE...ORDER BY...LIMIT...
1
2
3
4
5
需要获取商品描述时，再通过以下sql获取：

SELECT *
FROM [商品描述] 
WHERE [商品ID] = ?
1
2
3
垂直分表定义：将一个表按照字段分成多表，每个表存储其中一部分字段。
它带来的提升是：

1.为了避免IO争抢并减少锁表的几率，查看详情的用户与商品信息浏览互不影响

2.充分发挥热门数据的操作效率，商品信息的操作的高效率不会被商品描述的低效率所拖累。

为什么大字段IO效率低：第一是由于数据量本身大，需要更长的读取时间；第二是跨页，页是数据库存储单位，很多查找及定位操作都是以页为单位，单页内的数据行越多数据库整体性能越好，而大字段占用空间大，单页内存储行数少，因此IO效率较低。第三，数据库以行为单位将数据加载到内存中，这样表中字段长度较短且访问频率较高，内存能加载更多的数据，命中率更高，减少了磁盘IO，从而提升了数据库性能。

一般来说，某业务实体中的各个数据项的访问频次是不一样的，部分数据项可能是占用存储空间比较大的BLOB或是TEXT。例如上例中的商品描述。所以，当表数据量很大时，可以将表按字段切开，将热门字段、冷门字段分开放置在不同库中，这些库可以放在不同的存储设备上，避免IO争抢。垂直切分带来的性能提升主要集中在热门数据的操作效率上，而且磁盘争用情况减少。

通常我们按以下原则进行垂直拆分:

把不常用的字段单独放在一张表;
把text，blob等大字段拆分出来放在附表中;
经常组合查询的列放在一张表中;
垂直分库
通过垂直分表性能得到了一定程度的提升，但是还没有达到要求，并且磁盘空间也快不够了，因为数据还是始终限制在一台服务器，库内垂直分表只解决了单一表数据量过大的问题，但没有将表分布到不同的服务器上，因此每个表还是竞争同一个物理机的CPU、内存、网络IO、磁盘。

经过思考，他把原有的SELLER_DB(卖家库)，分为了PRODUCT_DB(商品库)和STORE_DB(店铺库)，并把这两个库分散到不同服务器，如下图：
在这里插入图片描述
由于商品信息与商品描述业务耦合度较高，因此一起被存放在PRODUCT_DB(商品库)；而店铺信息相对独立，因此单独被存放在STORE_DB(店铺库)。

垂直分库是指按照业务将表进行分类，分布到不同的数据库上面，每个库可以放在不同的服务器上，它的核心理念是专库专用。

它带来的提升是：

解决业务层面的耦合，业务清晰

能对不同业务的数据进行分级管理、维护、监控、扩展等

高并发场景下，垂直分库一定程度的提升IO、数据库连接数、降低单机硬件资源的瓶颈

垂直分库通过将表按业务分类，然后分布在不同数据库，并且可以将这些数据库部署在不同服务器上，从而达到多个服务器共同分摊压力的效果，但是依然没有解决单表数据量过大的问题。

水平分库
经过垂直分库后，数据库性能问题得到一定程度的解决，但是随着业务量的增长，PRODUCT_DB(商品库)单库存储数据已经超出预估。粗略估计，目前有8w店铺，每个店铺平均150个不同规格的商品，再算上增长，那商品数量得往1500w+上预估，并且PRODUCT_DB(商品库)属于访问非常频繁的资源，单台服务器已经无法支撑。此时该如何优化？

再次分库？但是从业务角度分析，目前情况已经无法再次垂直分库。

尝试水平分库，将店铺ID为单数的和店铺ID为双数的商品信息分别放在两个库中。

在这里插入图片描述
也就是说，要操作某条数据，先分析这条数据所属的店铺ID。如果店铺ID为双数，将此操作映射至RRODUCT_DB1(商品库1)；如果店铺ID为单数，将操作映射至RRODUCT_DB2(商品库2)。此操作要访问数据库名称的表达式为RRODUCT_DB[店铺ID%2 + 1] 。

水平分库是把同一个表的数据按一定规则拆到不同的数据库中，每个库可以放在不同的服务器上。

垂直分库是把不同表拆到不同数据库中，它是对数据行的拆分，不影响表结构

它带来的提升是：

解决了单库大数据，高并发的性能瓶颈。
提高了系统的稳定性及可用性。
稳定性体现在IO冲突减少，锁定减少，可用性指某个库出问题，部分可用`

当一个应用难以再细粒度的垂直切分，或切分后数据量行数巨大，存在单库读写、存储性能瓶颈，这时候就需要进行水平分库了，经过水平切分的优化，往往能解决单库存储量及性能瓶颈。但由于同一个表被分配在不同的数据库，需要额外进行数据操作的路由工作，因此大大提升了系统复杂度。

水平分表
按照水平分库的思路对他把PRODUCT_DB_X(商品库)内的表也可以进行水平拆分，其目的也是为解决单表数据量大的问题，如下图：
在这里插入图片描述
与水平分库的思路类似，不过这次操作的目标是表，商品信息及商品描述被分成了两套表。如果商品ID为双数，将此操作映射至商品信息1表；如果商品ID为单数，将操作映射至商品信息2表。此操作要访问表名称的表达式为商品信息[商品ID%2 + 1] 。

水平分表是在同一个数据库内，把同一个表的数据按一定规则拆到多个表中。

它带来的提升是：

优化单一表数据量过大而产生的性能问题

避免IO争抢并减少锁表的几率

库内的水平分表，解决了单一表数据量过大的问题，分出来的小表中只包含一部分数据，从而使得单个表的数据量变小，提高检索性能。

总结
垂直分表：可以把一个宽表的字段按访问频次、是否是大字段的原则拆分为多个表，这样既能使业务清晰，还能提升部分性能。拆分后，尽量从业务角度避免联查，否则性能方面将得不偿失。

垂直分库：可以把多个表按业务耦合松紧归类，分别存放在不同的库，这些库可以分布在不同服务器，从而使访问压力被多服务器负载，大大提升性能，同时能提高整体架构的业务清晰度，不同的业务库可根据自身情况定制优化方案。但是它需要解决跨库带来的所有复杂问题。

水平分库：可以把一个表的数据(按数据行)分到多个不同的库，每个库只有这个表的部分数据，这些库可以分布在不同服务器，从而使访问压力被多服务器负载，大大提升性能。它不仅需要解决跨库带来的所有复杂问题，还要解决数据路由的问题(数据路由问题后边介绍)。

水平分表：可以把一个表的数据(按数据行)分到多个同一个数据库的多张表中，每个表只有这个表的部分数据，这样做能小幅提升性能，它仅仅作为水平分库的一个补充优化。

一般来说，在系统设计阶段就应该根据业务耦合松紧来确定垂直分库，垂直分表方案，在数据量及访问压力不是特别大的情况，首先考虑缓存、读写分离、索引技术等方案。若数据量极大，且持续增长，再考虑水平分库水平分表方案。
```

### 分区
```
https://baijiahao.baidu.com/s?id=1655581234130331974&wfr=spider&for=pc
分区表是mysql5.1之后的新特性，合并表已经存在很长时间了。这篇文章主要介绍这两个概念以及他们基本的操作。

一、合并表

合并表说实话是一种将要被淘汰的技术，但是掌握了合并表的概念再去看分区表就比较容易理解一点。

合并表其实就是合并了多个子表的逻辑表，子表使用了myisam存储引擎物理子表，合并表使用merge存储引擎，逻辑表和子表的结构完全相同（包括字段、索引等）。

删除一个合并表，它的子表不会受任何影响，而如果删除其中一个子表则可能会有不同的后果，这要视操作系统而定。

下面我们进行实操一下：创建量两张物理子表t1和t2，然后创建他们俩的合并表。


在上面我们进行了一些初始化操作。而且我们在创建合并表的时候，指定了insert_method为last，意思就是在最后一张物理表的末尾插入真实数据，这里最后一张真实物理表就是t2。此时我们插入一个数据5会发现：t1没有，t2有。


合并表的内容很简单，也很容易理解。既然表能合并肯定也能分开。我们接着看分区表：

二、分区表

分区表就是把一张表分开，对用户来说，分区表是一个独立的逻辑表，但是底层由多个物理子表组成。实现分区的代码实际上是对一组底层表的句柄对象的封装。对分区表的请求，都会通过句柄对象转化成对存储引擎的接口调用。分区表的每一个分区都是有索引的独立表。

分区表发挥大作用的场景：

（1）表非常大以至于无法全部都放在内存中，或者只在表的最后部分有热点数据，其他均是历史数据。

（2）分区表的数据更容易维护。

（3）分区表的数据可以分布在不同的物理设备上。

（4）可以使用分区表来避免某些特殊的瓶颈，例如InnoDB单个索引的互斥访问。

（5）如果需要，还可以备份和恢复独立的分区，这在非常大的数据集的场景下效果非常好。

分开的方式有三种：

（1）水平分区：根据行切分，也就是把记录分开。

（2）垂直分区：根据列切分，也就是把字段分开。

（3）复合分区：水平分区和垂直分区的结合。

我们按照这两种方式来实际操作一下：

1、水平分区

（1）range分区

range分区是基于连续的范围值。


在这里需要注意的是，alter形式删除分区比delete形式更加的高效。一般都是基于日期时间进行分区。

（2）List分区

range分区是基于连续的范围，list是基于确定值的范围，就好比集合。


这种不是指的范围，而是指的具体的值，10号和20在n1分区，30在n2分区。

（3）Hash分区

hash分区指的是根据hash运算的模，最终确定在哪一个分区。比如2020/4=0，就落在分区0上。


此时我们2020/4=0，就会保存在分区0中。

（4）线性Hash分区

线性hash指的是使用2的幂运算法则。运算起来比较麻烦。但是优点是可以使得数据分布均匀。举个例子。假设分区个数num=6，N表示数据最终存储的分区：

第一步：V = power(2, ceiling(log(2, num)))，log是计算NUM以2为底的对数，ceiling()向上取整，power()是取2的次方值；

第二步：N=values&(V-1)，&位与运算，

第三步：while N>=num，此时N =N & (CEIL(V/ 2) - 1)

比如插入2020-01-20，V=8，N=（2020）& （8-1）=4。4<6，所以保存在分区4。

代码就不演示了，区别就是by hash换成by linear hash。

2、垂直分区

垂直分区比较少，直接通过key字段名进行划分即可。


垂直分区相当简单。

3、复合分区

上面介绍了水平和复合的方式，复合分区的方式是进行组合。你可以随意搭配。这里演示一种。


父分区使用list，子分区使用垂直分区。

在这里介绍了分区表的一些概念和基础的使用方法。其实分区表也有很多限制。

分区表的限制：

（1）一个表最多只能有1024个分区。

（2）如果分区字段中有主键或者唯一索引的列，那么所有主键列和唯一索引列都必须包含进来。

（3）分区表中无法使用外键约束。

（4）所有分区都必须使用相同的存储引擎。

（5）某些存储引擎不支持分区。比如说merge、InnoDB、CSV、联合存储引擎等。

MERGE存储引擎。 用户定义的分区和MERGE 存储引擎不兼容。分区表无法合并。

联合存储引擎。 FEDERATED不支持 分区表; 不可能创建分区 FEDERATED表。

CSV存储引擎。 CSV不支持使用存储引擎的分区表; 不可能创建分区CSV表。

InnoDB存储引擎。 InnoDB外键和MySQL分区不兼容。分区 InnoDB表不能有外键引用，也不能有外键引用的列。InnoDB具有或由外键引用的表不能分区。

（6）对于MyISAM表，使用分区表时需要打开更多的文件描述符。

所以在使用的时候一定要注意。
```

### 会使用 `explain` 分析 SQL 性能问题，了解各参数含义
```
https://zhuanlan.zhihu.com/p/127948040
本文目的
帮助大家认识explain,遇到上述问题的时候可以到此来查阅执行计划中每个字段的意思
能根据慢查询的执行计划快速找到问题所在
提供常见的问题原因以及解决方案
春招指南之“性能调优”：MySQL+Tomcat+JVM+面试+笔记+书籍等
​
shimo.im
图标
explain能干嘛
在了解explain之前，不妨先看下mysql服务大致的逻辑架构图，以对其有一个整体的认识


从图中可以看出,我们的sql在查询的时候主要需要经历以下步骤:

与mysql建立连接
查询缓存是否存在,如果有则直接返回结果
解析器,主要是对sql进行解析
查询优化器,主要对sql进行各种优化,包括重写查询、决定表的读取顺序以及选择合适的索引等等。。并产生执行计划
去存储引擎查询结果
而我们使用explain即是去查询优化器查询执行计划

explain字段解释
看一条简单的执行计划

explain select * from t_user where id = 1;



我们可以看到，一个执行计划会展示12个相关的字段,下面我们对主要字段以及这些字段常见的值进行解释:

id
含义：是一组数字，表示的是查询中执行select子句或者是操作表的顺序

规则：

id不相同的，id值越大越先执行
id相同的,从上到下顺序执行
select_type
常见的值以及描述如下

值描述SIMPLE简单的SELECT语句（不包括UNION操作或子查询操作）PRIMARY查询中最外层的SELECT（如两表做UNION或者存在子查询的外层的表操作为PRIMARY，内层的操作为UNION）UNIONUNION操作中，查询中处于内层的SELECT，即被union的SELECTSUBQUERY子查询中的SELECTDERIVED表示包含在 From 子句中的 Select 查询UNION RESULTunion的结果,此时id为NULL

table
涉及的表

type（重要）
这列很重要,显示了连接使用哪种类型,有无使用索引， 常见的值从最好到最差如下 system > const > eq_ref > ref > range > index > all

各值的描述如下


possible_keys
表示可能用到的索引

key
表示最终用到的key

ref
显示索引的哪一列被使用了，有时候会是一个常量：表示哪些列或常量被用于查找索引列上的值

rows
估算出结果集行数，表示MySQL根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数, 原则上 rows 越少越好。

filtered
查询结果的行数占上面rows的百分比

Extra(重要)
这一列也很重要,主要展示额外的信息说明,能够给出让我们深入理解执行计划进一步的细节信息

常见的值及描述如下


优化原则
通常有以下几种优化原则:

让主要查询语句使用到合适的索引,type出现ALL(全表扫描)需格外注意,同时建立合适的索引以减少possible_keys的数量
type最好能达到ref级别
Extra列出现Using temporary、Using filesort（文件排序）务必去除
优化思路
针对上面提到的几点优化原则,提供如下的优化思路

针对优化原则1，2
上述1,2点其实都可以通过优化索引来达到目的,而要想让我们建的索引达到最优,则需要依据一个原则: 三星索引原则

简单描述就是

☆: where后条件匹配的索引列越多扫描的数据将越少

比如组合索引(a,b,c),最好在where后面能同时用到索引上的a,b,c这三列

☆: 避免再次排序

简单来说,就是排序字段尽量使用索引字段,因为索引默认是排好序的,使用索引字段排序可以避免再次排序

☆: 索引行包含查询语句中所有的列,即覆盖索引

基于这一点，我们应该少用select*来查询，以增加覆盖索引的可能性

如果你的索引能集齐上述三颗星,则说明你的索引是最优的索引！

针对优化原则3
我们创建如下表，并插入一些数据

用户表

CREATE TABLE `t_user`  (
  `id` bigint(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(50) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `age` int(11) DEFAULT NULL,
  `group_id` bigint(20) DEFAULT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  INDEX `idx_name`(`name`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 1240277101395107842 CHARACTER SET = utf8mb4 COLLATE = utf8mb4_unicode_ci ROW_FORMAT = Dynamic;
分组表

CREATE TABLE `t_group`  (
  `id` bigint(20) NOT NULL,
  `group_name` varchar(50) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  PRIMARY KEY (`id`) USING BTREE
) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_unicode_ci ROW_FORMAT = Dynamic;
Using filesort
order by 的字段不在where条件中
下面这条sql会出现Using filesort
select * from t_user where group_id = 2 and age = 32 order by name;

但是下面这条sql不会

```
select * from t_user where group_id = 2 and age = 32 order by group_id ;

```

组合索引跨列
举例:给t_user表创建索引(name,age,group_id)
下面这条sql排序会出现Using filesort
select * from t_user where name= '李A' order by group_id;

但是下面这条就不会

```
select * from t_user where name = '李A' order by age;

```

因为第一条查询order by跳过了age,直接使用了group_id;删除索引(name,age,group_id);
由于group by第一步默认进行了排序,所以当group by 的字段满足上述条件是,也会出现Using filesort,可以在group by后面加上order by null取消排序
Using temporary
临时表的出现对性能影响是很大的,主要会出现在以下情况中

分组字段不在where条件后面,并且group by字段不是最终使用到的索引,原因有点类似于上面的Using filesort
下面这条sql会出现Using temporary
select * from t_user where group_id = 2 and name= '李A' group by age;



但是下面这条sql不会

```
select * from t_user where name = '李A' and age = 21 group by age;

```

**结论: where哪些字段,就group by 哪些字段**
表连接中，order by的列不是驱动表中的
如下sql是会创建临时表的
explain select * from t_user t1 left join t_group t2 on t1.group_id = t2.id order by t2.id;



因为t1和t2连接的时候,t1是驱动表,但是排序使用了被驱动表t2中的字段。改为t1的字段排序就不会出现临时表了,这里就不举例了。

**结论: 连接查询的时候，排序字段使用驱动表的字段**
order by和group by的子句不一样时
explain select * from t_user group by group_id order by `name`;



这种情况只能尽量使用同一个字段来分组和排序了，否则无法避免
distinct查询并且加上order by时
explain select DISTINCT(`name`) from t_user order by age;
这种情况有时候无法避免，只能尽量将distinct的字段和order by的字段使用相同的索引。还有会出现临时表的情况有: from 中的子查询、union，这里就不一一举例了。
总结
sql优化已经是我们后端开发的内化技能之一了，在学习框架，设计思想的同时，不要忘记打牢基础，希望各位能够有所收获。
```

### Slow Log（有什么用，什么时候需要）
```
https://www.cnblogs.com/hjqjk/p/Mysqlslowlog.html
MySQL慢查询日志是MySQL提供的一种日志记录，用来记录执行时长超过指定时长的查询语句，具体指运行时间超过 long_query_time 值的 SQL 语句，则会被记录到慢查询日志中。

long_query_time 默认值是 10 ，单位是 s，即默认是 10秒 。默认情况下，MySQL数据库并不会开启慢查询日志，需要手动设置这个参数。

通过慢查询日志，可以查找出哪些查询语句的执行效率很低，以便进行优化。一般建议开启，它对服务器性能的影响微乎其微，但是可以记录MySQL服务器上执行了很长时间的查询语句。慢查询日志可以帮助我们定位mysql性能问题所在。

MySQL慢查询日志
慢查询日志相关参数
slow_query_log : 是否启用慢查询日志，[1 | 0] 或者 [ON | OFF]

slow_query_log_file : MySQL数据库（5.6及以上版本）慢查询日志存储路径。
                    可以不设置该参数，系统则会默认给一个缺省的文件 HOST_NAME-slow.log

long_query_time : 慢查询的阈值，当查询时间超过设定的阈值时，记录该SQL语句到慢查询日志。

log_queries_not_using_indexes ：设置为 ON ，可以捕获到所有未使用索引的SQL语句(不建议启用)

log_output : 日志存储方式。
            log_output='FILE'，表示将日志存入文件，默认值是'FILE'。      
            log_output='TABLE'，表示将日志存入数据库，这样日志信息就会被写入到 mysql.slow_log 表中。
            MySQL数据库支持同时两种日志存储方式，配置的时候以逗号隔开即可，如：log_output='FILE,TABLE'。
            日志记录到系统的专用日志表中，要比记录到文件耗费更多的系统资源，因此对于需要启用慢查询日志，又需要能够获得更高的系统性能，那么建议优先记录到文件。
5.6之前的版本，有些参数名字不一样：

log-slow-queries : MySQL数据库（5.6以下版本）慢查询日志存储路径。
开启日志
立即生效，重启失效
mysql> set global slow_query_log=ON;
mysql> set global slow_query_log_file='/xxx/mysql-slow.log';
永久生效
修改 my.cnf ：

[mysqld]
slow_query_log           = 1
slow_query_log_file      = /xxx/mysql-slow.log
long_query_time          = 1

# 也可以写成这种形式
slow-query-log           = 1
slow-query-log-file      = /xxx/mysql-slow.log
long-query-time          = 1
重启mysql服务。

关闭日志
临时关闭，重启失效：

mysql> set global slow_query_log=OFF;
永久关闭，修改 my.cnf，重启mysql服务：

[mysqld]
slow_query_log           = 0
MySQL慢查询日志分析
慢查询日志格式说明
打开慢查询日志 mysql-slow.log ，内容都是以下格式：

# Time: 2017-11-22T12:22:32.554299Z
# User@Host: www[www] @  [192.168.10.2]  Id: 580785559
# Query_time: 24.354270  Lock_time: 0.000238 Rows_sent: 1  Rows_examined: 511156
SET timestamp=1511353352;
SELECT * FROM mo_user WHERE email = 'chxxx@hotmail.com' LIMIT 1;
其中参数说明如下：

log 记录的时间：# Time: 2017-11-22T12:22:32.554299Z
SQL 的执行主机：# User@Host: www[www] @ [192.168.10.2] Id: 580785559
SQL 的执行信息（执行时间(单位：s)，锁时间，返回结果行数，查询总行数）：# Query_time: 24.354270 Lock_time: 0.000238 Rows_sent: 1 Rows_examined: 511156;
SQL 执行发生的时间：SET timestamp=1511353352;
SQL 的执行内容：SELECT * FROM mo_user WHERE email = 'chxxx@hotmail.com' LIMIT 1;
mysqldumpslow
mysqldumpslow 是MySQL自带的慢查询日志分析工具(perl脚本)。执行命令 mysqldumpslow --help，显示命令参数如下：

Usage: mysqldumpslow [ OPTS... ] [ LOGS... ]

Parse and summarize the MySQL slow query log. Options are

  --verbose    verbose
  --debug      debug
  --help       write this text to standard output

  -v           verbose
  -d           debug
  -s ORDER     what to sort by (al, at, ar, c, l, r, t), 'at' is default
                al: average lock time
                ar: average rows sent
                at: average query time
                 c: count
                 l: lock time
                 r: rows sent
                 t: query time
  -r           reverse the sort order (largest last instead of first)
  -t NUM       just show the top n queries
  -a           don't abstract all numbers to N and strings to 'S'
  -n NUM       abstract numbers with at least n digits within names
  -g PATTERN   grep: only consider stmts that include this string
  -h HOSTNAME  hostname of db server for *-slow.log filename (can be wildcard),
               default is '*', i.e. match all
  -i NAME      name of server instance (if using mysql.server startup script)
  -l           don't subtract lock time from total time
参数说明：

-v、--verbose : 在详细模式下运行，打印有关该程序的更多信息。

-d、--debug : 在调试模式下运行。

--help : 显示帮助信息并退出程序

-s [sort_type] : sort_type 是信息排序的依据

al：average lock time，按平均等待锁的时间排序
ar：average rows sent，按平均发给客户端的行总数排序
at：average query time，按平均查询时间排序
c：count，按出现总次数排序
l：lock time，按等待锁的时间排序
r：rows sent，按扫描的行总数排序
t：query time，按累计总耗费时间排序

-r : 倒序信息排序

-t NUM: 只显示前 n 个查询，降序

-a : 不把数字抽象为'N'，不把字符串抽象为'S'

-n NUM : 「abstract numbers with at least n digits within names」

-g PATTERN : 根据字符串筛选慢查询日志，可写正则匹配，大小写不敏感。

-h HOSTNAME : 根据服务器名称选择慢查询日志

-i NAME : 根据服务器 MySQL 实例名称选择慢查询日志

-l : 不要将总时间减去锁定时间

mysqldumpslow 分析的结果如下:



Count : 出现次数(Count)
Time : 执行最长时间(Time) 和 累计总耗费时间(Time)
Lock : 等待锁的时间(Lock)
Rows : 发送给客户端的行总数(Rows) 和 扫描的行总数(Rows)
root[root]@localhost : 用户
SHOw FULL ... : SQL语句本身(抽象了格式, 比如 limit 1, 20 用 limit N,N 表示。'N'表示数字，'S'表示字符串)。
例子:
返回记录数最多的10个SQL

mysqldumpslow -s r -t 10 mysql-slow.log

mysqlsla
mysqlsla是 hackmysql.com 推出的一款日志分析工具(该网站还维护了 mysqlreport, mysqlidxchk 等比较实用的mysql工具)
整体来说, 功能非常强大。数据报表,非常有利于分析慢查询的原因, 包括执行频率, 数据量, 查询消耗等。

但是，hackmysql.com官方已经在2015年1月份放弃了对mysqlsla的维护。



安装
解决依赖关系
# yum install  perl-DBI perl-DBD-MySQL 
可能会遇到的问题：Can't locate ExtUtils/MakeMaker.pm，解决如下：

# yum install  perl-ExtUtils-CBuilder  perl-ExtUtils-MakeMaker
可能会遇到的问题：Can't locate Time/HiRes.pm in @INC，解决如下：

# yum install perl-Time-HiRes
下载mysqlsla
当前 mysqlsla 的最新版本为 2.03，到官网下载（官方链接已经失效），可以去这个 有效下载地址 下载。

编译安装
# tar xvfz mysqlsla-2.03.tar.gz 
# cd mysqlsla-2.03
# perl Makefile.PL
# make
# make install
使用
# mysqlsla -lt slow mysql-slow.log 

或者 
# mysqlsla -lt slow mysql-slow.log -sf "+SELECT" -db dbName -top 10 -sort t_sum
参数意义 ：

-lt ：表示日志类型，有: slow, general, binary, msl, udl
-sf ：[+-][TYPE]，包括|不包括，过滤sql语句的类型 [TYPE]有 SELECT, CREATE, DROP, UPDATE, INSERT，例如 "+SELECT,INSERT"，不出现的默认是 - ，即不包括。
-db ：要处理哪个库的日志。
-top ：表示取按规则排序的前多少条。
-sort ：按某种规则排序，t_sum 按总时间排序， c_sum 按总次数排序。c_sum_p : sql语句执行次数占总执行次数的百分比。
对慢查询日志文件的分析，最简化的调用方式如下：

# mysqlsla -lt slow [SlowLogFilePath] > [ResultFilePath]


格式说明如下:

总查询次数 (queries total), 去重后的sql数量 (unique)
输出报表的内容排序方式(sorted by)
最重大的慢sql统计信息, 包括平均执行时间, 等待锁时间, 结果行的总数, 扫描的行总数。
Count： sql的执行次数及占总的slow log数量的百分比.
Time：执行时间, 包括总时间, 平均时间, 最小, 最大时间, 时间占总慢sql时间的百分比.
95% of Time：去除最快和最慢的sql, 覆盖率占95%的sql的执行时间.
Lock Time：等待锁的时间.
95% of Lock ：95%的慢sql等待锁时间.
Rows sent：结果行统计数量, 包括平均, 最小, 最大数量.
Rows examined： 扫描的行数量.
Database：属于哪个数据库
Users：哪个用户,IP, 占到所有用户执行的sql百分比
Query abstract：抽象后的sql语句
Query sample：sql语句个例
pt-query-digest
percona-toolkit 工具介绍
percona-toolkit 是一组高级命令行工具的集合，用来执行各种通过手工执行非常复杂和麻烦的mysql和系统任务。这些任务包括：

检查master和slave数据的一致性
有效地对记录进行归档
查找重复的索引
对服务器信息进行汇总
分析来自日志和tcpdump的查询
当系统出问题的时候收集重要的系统信息
Percona Toolkit整个工具箱提供了非常多实用的工具，具体的使用方法可以参看 官方文档


percona-toolkit安装
安装 percona-toolkit 非常简单，到 官网 下载 .tar.gz 包：

# wget percona.com/get/percona-toolkit.tar.gz
# tar -zxvf percona-toolkit-2.2.5.tar.gz
然后依次执行下面的命令：

# perl Makefile.PL
# make
# make install
默认的会被安装在 /usr/local/bin 目录下。执行 man percona-toolkit 可以查看安装了哪些工具。

运行工具可能会遇到下面的错误：


这是因为缺少相应包，.pm包实际上是perl的包，运行下面的命令安装即可：

yum install -y perl-Time-HiRes
如果安装过程中出现 Error Downloading Packages 错误，尝试 yum clean all 后再安装。使用其Percona Toolkit中其他工具也可能会遇到类似的问题，按照提示安装相应的perl包就可以了。

问题：Can't locate Digest/MD5.pm in @INC
解决：# yum install perl-Digest-MD5

问题：Can't locate ExtUtils/MakeMaker.pm in @INC
解决：# yum install perl-ExtUtils-CBuilder perl-ExtUtils-MakeMaker

pt-query-digest使用
pt-query-digest 可以从普通MySQL日志，慢查询日志以及二进制日志中分析查询，甚至可以从 SHOW PROCESSLIST; 和MySQL协议的tcpdump中进行分析，如果没有指定文件，它从标准输入流（STDIN）中读取数据。

最简单的用法如下：

# pt-query-digest mysql-slow.log
输出信息大致如下:


整个输出分为三大部分：

整体概要（Overall）


这个部分是一个大致的概要信息(类似loadrunner给出的概要信息)，通过它可以对当前MySQL的查询性能做一个初步的评估，比如各个指标的最大值(max)，平均值(min)，95%分布值，中位数(median)，标准偏差(stddev)。
这些指标有查询的执行时间（Exec time），锁占用的时间（Lock time），MySQL执行器需要检查的行数（Rows examine），最后返回给客户端的行数（Rows sent），查询的大小。

查询的汇总信息（Profile）
这个部分对所有 "重要" 的查询(通常是比较慢的查询)做了个一览表:


每个查询都有一个Query ID，这个ID通过Hash计算出来的。pt-query-digest 是根据这个所谓的Fingerprint来group by的。举例下面两个查询的Fingerprint是一样的都是 select * from table1 where column1 = ?，工具箱中也有一个与之相关的工具 pt-fingerprint。

select * from table1 where column1 = 2
select * from table1 where column1 = 3
Rank整个分析中该“语句”的排名，一般也就是性能最常的。
Response time “语句”的响应时间以及整体占比情况。
Calls 该“语句”的执行次数。
R/Call 每次执行的平均响应时间。
V/M 响应时间的差异平均对比率。
在尾部有一行输出，显示了其他2个占比较低而不值得单独显示的查询的统计数据。

详细信息
这个部分会列出Profile表中每个查询的详细信息：（默认是按照总的Exec time排序，降序）



包括Overall中有的信息、查询响应时间的分布情况以及该查询 "入榜" 的理由，最底下会显示该查询SQL语句（真实显示，非抽象格式）。

pt-query-digest 还有很多复杂的操作，这里就不一一介绍了。比如：从PROCESSLIST中查询某个MySQL中最慢的查询：

# pt-query-digest –processlist h=host1
从tcpdump中分析：

# tcpdump -s 65535 -x -nn -q -tttt -i any -c 1000 port 3306 > mysql.tcp.txt
# pt-query-digest --type tcpdump mysql.tcp.txt
从一台机器上将 slow log 保存到另外一台机器上待稍后详细分析：

# pt-query-digest --review h=host2 --no-report slow.log
还可以跟一些过滤条件。详见 官方文档

另外结合一些第三方工具还能生成相应的报表，可以 参考这里

建议 ：当 slow log 很大的时候最好还是将日志文件移到其他机器上进行分析，避免分析时过度消耗该服务器资源。
```

### MSSQL(了解)
```
美国Microsoft公司推出的一种关系型数据库系统。SQL Server是一个可扩展的、高性能的、为分布式客户机/服务器计算所设计的数据库管理系统，实现了与WindowsNT的有机结合，提供了基于事务的企业级信息管理系统方案。
（1）高性能设计，可充分利用WindowsNT的优势。
（2）系统管理先进，支持Windows图形化管理工具，支持本地和远程的系统管理和配置。
（3）强壮的事务处理功能，采用各种方法保证数据的完整性。
（4）支持对称多处理器结构、存储过程、ODBC，并具有自主的SQL语言。 SQLServer以其内置的数据复制功能、强大的管理工具、与Internet的紧密集成和开放的系统结构为广大的用户、开发人员和系统集成商提供了一个出众的数据库平台。
上面是百度百科的资料。

mssql和sqlserver有着很大的不同，sqlserver的结构比较复杂，注入语句也比较复杂。但是如果在注入当中数据库是采用sa用户来运行的，那么我们就可以很轻松的拿下webshell。mssql可以直接启用存储过程来执行命令。

我们来看一下mssql的系统自带数据库的作用。



 

 

 

master 数据库存放着一切对象的信息，sa或者其他用户的密码 以密文存储
model： 存在创建用户数据库的模板
msdb:用户数据库，存放所有的任务调度
tmpdb：临时数据库，在注入时候如果受限制显示位，存在在某表里面，然后再爆数据。重启会清空tempdb的数据
数据库自带用户介绍：

前面带＃号的是mssql 内部用户数据库仅用数据库内部使用
nt开头的是数据库安装的时候创建的


 

 

 

如果msql 执行命令是使用nt serivice\mssqlserver 这个 服务来执行命令 。

 

下面我们来查询个看看，来查看与mysql 不同的地方。

select * from master.dbo.sysobjects where xtype = 'u';
matster是指定数据库名点号后面跟上的是他的架构 再加上表名。

那么我们发现其实这个数据库里面根本没有这张表是怎么回事？

其实这个只是我们的视图，而不是我们真正的表。

sysobjects 是系统的视图，用于存放改数据库内创建的所有对象、如约束、默认值、日志、规则、存储过程，

xtype是代表对象类型：
U：表（用户自定义的表）
V：视图
P ：存储过程
X ：扩展存储过程
我们可以来查询我们所有的数据库名字

select * from master..sysdatabases;


 

 

 

这个查询的也是我们的视图 在mssql里面information这个数据库也都是以视图的形式存在的

查看是否站库分离

 and (select host_name()) = (select @@servername)) 
查询数据库的名字，这个函数可以遍历 在括号里面输入数字可以查询对应的数据库名字

SELECT DB_NAME();
 



 

 

 

这里再来说到mssql的权限划分，

sa：sysadmin      超级管理员权限
dbo : db_owner   数据库管理员权限
public :访问用户权限
 

 

0x02  存储过程

存储过程（Stored Procedure）是在大型数据库系统中，
一组为了完成特定功能的SQL 语句集，它存储在数据库中，一次编译后永久有效
，用户通过指定存储过程的名字并给出参数（如果该存储过程带有参数）来执行它。
存储过程是数据库中的一个重要对象。
在数据量特别庞大的情况下利用存储过程能达到倍速的效率提升
那么我们在实战当中就可以利用到存储过程来直接执行cmd命令

常用的存储过程有xp_cmdshell,sp_oacreate  sp_oacreate这个存储过程执行命令无回显，需要输出到txt文件然后进行查看。

在00版本是默认开启xp_cmdshell的 05版本后需要手工开启。

 

0x03  结尾

本次文章耗时3小时，记录一下时间点。下篇文章写mssql各类注入语法与bypass手法
```

### 查询最新5条数据
```
select   *   from   表名   order   by   列名 desc （降序）   limit    显示的条数
```
### NOSQL
```
NoSQL 简介
NoSQL(NoSQL = Not Only SQL )，意即"不仅仅是SQL"。

在现代的计算系统上每天网络上都会产生庞大的数据量。

这些数据有很大一部分是由关系数据库管理系统（RDBMS）来处理。 1970年 E.F.Codd's提出的关系模型的论文 "A relational model of data for large shared data banks"，这使得数据建模和应用程序编程更加简单。

通过应用实践证明，关系模型是非常适合于客户服务器编程，远远超出预期的利益，今天它是结构化数据存储在网络和商务应用的主导技术。

NoSQL 是一项全新的数据库革命性运动，早期就有人提出，发展至2009年趋势越发高涨。NoSQL的拥护者们提倡运用非关系型的数据存储，相对于铺天盖地的关系型数据库运用，这一概念无疑是一种全新的思维的注入。

关系型数据库遵循ACID规则
事务在英文中是transaction，和现实世界中的交易很类似，它有如下四个特性：

1、A (Atomicity) 原子性

原子性很容易理解，也就是说事务里的所有操作要么全部做完，要么都不做，事务成功的条件是事务里的所有操作都成功，只要有一个操作失败，整个事务就失败，需要回滚。

比如银行转账，从A账户转100元至B账户，分为两个步骤：1）从A账户取100元；2）存入100元至B账户。这两步要么一起完成，要么一起不完成，如果只完成第一步，第二步失败，钱会莫名其妙少了100元。

2、C (Consistency) 一致性

一致性也比较容易理解，也就是说数据库要一直处于一致的状态，事务的运行不会改变数据库原本的一致性约束。

例如现有完整性约束a+b=10，如果一个事务改变了a，那么必须得改变b，使得事务结束后依然满足a+b=10，否则事务失败。

3、I (Isolation) 独立性

所谓的独立性是指并发的事务之间不会互相影响，如果一个事务要访问的数据正在被另外一个事务修改，只要另外一个事务未提交，它所访问的数据就不受未提交事务的影响。

比如现在有个交易是从A账户转100元至B账户，在这个交易还未完成的情况下，如果此时B查询自己的账户，是看不到新增加的100元的。

4、D (Durability) 持久性

持久性是指一旦事务提交后，它所做的修改将会永久的保存在数据库上，即使出现宕机也不会丢失。


分布式系统
分布式系统（distributed system）由多台计算机和通信的软件组件通过计算机网络连接（本地网络或广域网）组成。

分布式系统是建立在网络之上的软件系统。正是因为软件的特性，所以分布式系统具有高度的内聚性和透明性。

因此，网络和分布式系统之间的区别更多的在于高层软件（特别是操作系统），而不是硬件。

分布式系统可以应用在不同的平台上如：Pc、工作站、局域网和广域网上等。


分布式计算的优点
可靠性（容错） ：

分布式计算系统中的一个重要的优点是可靠性。一台服务器的系统崩溃并不影响到其余的服务器。

可扩展性：

在分布式计算系统可以根据需要增加更多的机器。

资源共享：

共享数据是必不可少的应用，如银行，预订系统。

灵活性：

由于该系统是非常灵活的，它很容易安装，实施和调试新的服务。

更快的速度：

分布式计算系统可以有多台计算机的计算能力，使得它比其他系统有更快的处理速度。

开放系统：

由于它是开放的系统，本地或者远程都可以访问到该服务。

更高的性能：

相较于集中式计算机网络集群可以提供更高的性能（及更好的性价比）。


分布式计算的缺点
故障排除：

故障排除和诊断问题。

软件：

更少的软件支持是分布式计算系统的主要缺点。

网络：

网络基础设施的问题，包括：传输问题，高负载，信息丢失等。

安全性：

开放系统的特性让分布式计算系统存在着数据的安全性和共享的风险等问题。


什么是NoSQL?
NoSQL，指的是非关系型的数据库。NoSQL有时也称作Not Only SQL的缩写，是对不同于传统的关系型数据库的数据库管理系统的统称。

NoSQL用于超大规模数据的存储。（例如谷歌或Facebook每天为他们的用户收集万亿比特的数据）。这些类型的数据存储不需要固定的模式，无需多余操作就可以横向扩展。

为什么使用NoSQL ?
今天我们可以通过第三方平台（如：Google,Facebook等）可以很容易的访问和抓取数据。用户的个人信息，社交网络，地理位置，用户生成的数据和用户操作日志已经成倍的增加。我们如果要对这些用户数据进行挖掘，那SQL数据库已经不适合这些应用了, NoSQL 数据库的发展却能很好的处理这些大的数据。




实例
社会化关系网:

Each record: UserID1, UserID2
Separate records: UserID, first_name,last_name, age, gender,...
Task: Find all friends of friends of friends of ... friends of a given user.
Wikipedia 页面 :

Large collection of documents
Combination of structured and unstructured data
Task: Retrieve all pages regarding athletics of Summer Olympic before 1950.

RDBMS vs NoSQL
RDBMS
- 高度组织化结构化数据
- 结构化查询语言（SQL） (SQL)
- 数据和关系都存储在单独的表中。
- 数据操纵语言，数据定义语言
- 严格的一致性
- 基础事务

NoSQL
- 代表着不仅仅是SQL
- 没有声明性查询语言
- 没有预定义的模式
-键 - 值对存储，列存储，文档存储，图形数据库
- 最终一致性，而非ACID属性
- 非结构化和不可预知的数据
- CAP定理
- 高性能，高可用性和可伸缩性




NoSQL 简史
NoSQL一词最早出现于1998年，是Carlo Strozzi开发的一个轻量、开源、不提供SQL功能的关系数据库。

2009年，Last.fm的Johan Oskarsson发起了一次关于分布式开源数据库的讨论[2]，来自Rackspace的Eric Evans再次提出了NoSQL的概念，这时的NoSQL主要指非关系型、分布式、不提供ACID的数据库设计模式。

2009年在亚特兰大举行的"no:sql(east)"讨论会是一个里程碑，其口号是"select fun, profit from real_world where relational=false;"。因此，对NoSQL最普遍的解释是"非关联型的"，强调Key-Value Stores和文档数据库的优点，而不是单纯的反对RDBMS。


CAP定理（CAP theorem）
在计算机科学中, CAP定理（CAP theorem）, 又被称作 布鲁尔定理（Brewer's theorem）, 它指出对于一个分布式计算系统来说，不可能同时满足以下三点:

一致性(Consistency) (所有节点在同一时间具有相同的数据)
可用性(Availability) (保证每个请求不管成功或者失败都有响应)
分隔容忍(Partition tolerance) (系统中任意信息的丢失或失败不会影响系统的继续运作)
CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，最多只能同时较好的满足两个。

因此，根据 CAP 原理将 NoSQL 数据库分成了满足 CA 原则、满足 CP 原则和满足 AP 原则三 大类：

CA - 单点集群，满足一致性，可用性的系统，通常在可扩展性上不太强大。
CP - 满足一致性，分区容忍性的系统，通常性能不是特别高。
AP - 满足可用性，分区容忍性的系统，通常可能对一致性要求低一些。



NoSQL的优点/缺点
优点:

- 高可扩展性
- 分布式计算
- 低成本
- 架构的灵活性，半结构化数据
- 没有复杂的关系
缺点:

- 没有标准化
- 有限的查询功能（到目前为止）
- 最终一致是不直观的程序

BASE
BASE：Basically Available, Soft-state, Eventually Consistent。 由 Eric Brewer 定义。

CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，最多只能同时较好的满足两个。

BASE是NoSQL数据库通常对可用性及一致性的弱要求原则:

Basically Availble --基本可用
Soft-state --软状态/柔性事务。 "Soft state" 可以理解为"无连接"的, 而 "Hard state" 是"面向连接"的
Eventual Consistency -- 最终一致性， 也是是 ACID 的最终目的。

ACID vs BASE
ACID	BASE
原子性(Atomicity)	基本可用(Basically Available)
一致性(Consistency)	软状态/柔性事务(Soft state)
隔离性(Isolation)	最终一致性 (Eventual consistency)
持久性 (Durable)	 

NoSQL 数据库分类
类型	部分代表
特点
列存储	
Hbase

Cassandra

Hypertable

顾名思义，是按列存储数据的。最大的特点是方便存储结构化和半结构化数据，方便做数据压缩，对针对某一列或者某几列的查询有非常大的IO优势。

文档存储

MongoDB

CouchDB

文档存储一般用类似json的格式存储，存储的内容是文档型的。这样也就有机会对某些字段建立索引，实现关系数据库的某些功能。

key-value存储

Tokyo Cabinet / Tyrant

Berkeley DB

MemcacheDB

Redis

可以通过key快速查询到其value。一般来说，存储不管value的格式，照单全收。（Redis包含了其他功能）

图存储

Neo4J

FlockDB

图形关系的最佳存储。使用传统关系数据库来解决的话性能低下，而且设计使用不方便。

对象存储

db4o

Versant

通过类似面向对象语言的语法操作数据库，通过对象的方式存取数据。

xml数据库

Berkeley DB XML

BaseX

高效的存储XML数据，并支持XML的内部查询语法，比如XQuery,Xpath。


谁在使用
现在已经有很多公司使用了 NoSQL：
Google
Facebook
Mozilla
Adobe
Foursquare
LinkedIn
Digg
McGraw-Hill Education
Vermont Public Radio
```

### Redis、Memcached、MongoDB
```
https://www.cnblogs.com/kuhaha/p/9827124.html
>>Memcached
Memcached的优点：
Memcached可以利用多核优势，单实例吞吐量极高，可以达到几十万QPS（取决于key、value的字节大小以及服务器硬件性能，日常环境中QPS高峰大约在4-6w左右）。适用于最大程度扛量。
支持直接配置为session handle。
Memcached的局限性：
只支持简单的key/value数据结构，不像Redis可以支持丰富的数据类型。
无法进行持久化，数据不能备份，只能用于缓存使用，且重启后数据全部丢失。
无法进行数据同步，不能将MC中的数据迁移到其他MC实例中。
Memcached内存分配采用Slab Allocation机制管理内存，value大小分布差异较大时会造成内存利用率降低，并引发低利用率时依然出现踢出等问题。需要用户注重value设计。

>>Redis
Redis的优点：
支持多种数据结构，如 string（字符串）、 list(双向链表)、dict(hash表)、set(集合）、zset(排序set)、hyperloglog（基数估算）
支持持久化操作，可以进行aof及rdb数据持久化到磁盘，从而进行数据备份或数据恢复等操作，较好的防止数据丢失的手段。
支持通过Replication进行数据复制，通过master-slave机制，可以实时进行数据的同步复制，支持多级复制和增量复制，master-slave机制是Redis进行HA的重要手段。
单线程请求，所有命令串行执行，并发情况下不需要考虑数据一致性问题。
支持pub/sub消息订阅机制，可以用来进行消息订阅与通知。
支持简单的事务需求，但业界使用场景很少，并不成熟。

Redis的局限性：
Redis只能使用单线程，性能受限于CPU性能，故单实例CPU最高才可能达到5-6wQPS每秒（取决于数据结构，数据大小以及服务器硬件性能，日常环境中QPS高峰大约在1-2w左右）。
支持简单的事务需求，但业界使用场景很少，并不成熟，既是优点也是缺点。
Redis在string类型上会消耗较多内存，可以使用dict（hash表）压缩存储以降低内存耗用。

Mc和Redis都是Key-Value类型，不适合在不同数据集之间建立关系，也不适合进行查询搜索。比如redis的keys pattern这种匹配操作，对redis的性能是灾难。

>>mongoDB 
mongoDB 是一种文档性的数据库。先解释一下文档的数据库，即可以存放xml、json、bson类型系那个的数据。

这些数据具备自述性（self-describing），呈现分层的树状数据结构。redis可以用hash存放简单关系型数据。

mongoDB 存放json格式数据。

适合场景：事件记录、内容管理或者博客平台，比如评论系统。

1.mongodb持久化原理

mongodb与mysql不同，mysql的每一次更新操作都会直接写入硬盘，但是mongo不会，做为内存型数据库，数据操作会先写入内存，然后再会持久化到硬盘中去，那么mongo是如何持久化的呢
mongodb在启动时，专门初始化一个线程不断循环（除非应用crash掉），用于在一定时间周期内来从defer队列中获取要持久化的数据并写入到磁盘的journal(日志)和mongofile(数据)处，当然因为它不是在用户添加记录时就写到磁盘上，所以按mongodb开发者说，它不会造成性能上的损耗，因为看过代码发现，当进行CUD操作时，记录(Record类型)都被放入到defer队列中以供延时批量（groupcommit）提交写入，但相信其中时间周期参数是个要认真考量的参数，系统为90毫秒，如果该值更低的话，可能会造成频繁磁盘操作，过高又会造成系统宕机时数据丢失过。

2.什么是NoSQL数据库？NoSQL和RDBMS有什么区别？在哪些情况下使用和不使用NoSQL数据库？
NoSQL是非关系型数据库，NoSQL = Not Only SQL。
关系型数据库采用的结构化的数据，NoSQL采用的是键值对的方式存储数据。
在处理非结构化/半结构化的大数据时；在水平方向上进行扩展时；随时应对动态增加的数据项时可以优先考虑使用NoSQL数据库。
在考虑数据库的成熟度；支持；分析和商业智能；管理及专业性等问题时，应优先考虑关系型数据库。

3.MySQL和MongoDB之间最基本的区别是什么？
关系型数据库与非关系型数据库的区别，即数据存储结构的不同。

4.MongoDB的特点是什么？
（1）面向文档（2）高性能（3）高可用（4）易扩展（5）丰富的查询语言

5.MongoDB支持存储过程吗？如果支持的话，怎么用？
MongoDB支持存储过程，它是javascript写的，保存在db.system.js表中。

6.如何理解MongoDB中的GridFS机制，MongoDB为何使用GridFS来存储文件？
GridFS是一种将大型文件存储在MongoDB中的文件规范。使用GridFS可以将大文件分隔成多个小文档存放，这样我们能够有效的保存大文档，而且解决了BSON对象有限制的问题。

7.为什么MongoDB的数据文件很大？
MongoDB采用的预分配空间的方式来防止文件碎片。

8.当更新一个正在被迁移的块（Chunk）上的文档时会发生什么？
更新操作会立即发生在旧的块（Chunk）上，然后更改才会在所有权转移前复制到新的分片上。

9.MongoDB在A:{B,C}上建立索引，查询A:{B,C}和A:{C,B}都会使用索引吗？
不会，只会在A:{B,C}上使用索引。

10.如果一个分片（Shard）停止或很慢的时候，发起一个查询会怎样？
如果一个分片停止了，除非查询设置了“Partial”选项，否则查询会返回一个错误。如果一个分片响应很慢，MongoDB会等待它的响应。

 

>>Redis、Memcache和MongoDB的区别
从以下几个维度，对redis、memcache、mongoDB 做了对比，

1、性能

都比较高，性能对我们来说应该都不是瓶颈

总体来讲，TPS方面redis和memcache差不多，要大于mongodb

2、操作的便利性

memcache数据结构单一

redis丰富一些，数据操作方面，redis更好一些，较少的网络IO次数

mongodb支持丰富的数据表达，索引，最类似关系型数据库，支持的查询语言非常丰富

3、内存空间的大小和数据量的大小

redis在2.0版本后增加了自己的VM特性，突破物理内存的限制；可以对key value设置过期时间（类似memcache）

memcache可以修改最大可用内存,采用LRU算法

mongoDB适合大数据量的存储，依赖操作系统VM做内存管理，吃内存也比较厉害，服务不要和别的服务在一起

4、可用性（单点问题）

对于单点问题，

redis，依赖客户端来实现分布式读写；主从复制时，每次从节点重新连接主节点都要依赖整个快照,无增量复制，因性能和效率问题，

所以单点问题比较复杂；不支持自动sharding,需要依赖程序设定一致hash 机制。

一种替代方案是，不用redis本身的复制机制，采用自己做主动复制（多份存储），或者改成增量复制的方式（需要自己实现），一致性问题和性能的权衡

Memcache本身没有数据冗余机制，也没必要；对于故障预防，采用依赖成熟的hash或者环状的算法，解决单点故障引起的抖动问题。

mongoDB支持master-slave,replicaset（内部采用paxos选举算法，自动故障恢复）,auto sharding机制，对客户端屏蔽了故障转移和切分机制。

5、可靠性（持久化）

对于数据持久化和数据恢复，

redis支持（快照、AOF）：依赖快照进行持久化，aof增强了可靠性的同时，对性能有所影响

memcache不支持，通常用在做缓存,提升性能；

MongoDB从1.8版本开始采用binlog方式支持持久化的可靠性

6、数据一致性（事务支持）

Memcache 在并发场景下，用cas保证一致性

redis事务支持比较弱，只能保证事务中的每个操作连续执行

mongoDB不支持事务

7、数据分析

mongoDB内置了数据分析的功能(mapreduce),其他不支持

8、应用场景

redis：数据量较小的更性能操作和运算上

memcache：用于在动态系统中减少数据库负载，提升性能;做缓存，提高性能（适合读多写少，对于数据量比较大，可以采用sharding）

MongoDB:主要解决海量数据的访问效率问题
```

### 持久化
```
持久化
ACID中的持久化和具体的硬件配置有很大的关系，因为持久化时的性能和具体的服务器CPU，网络，和硬盘有很大的关系，MySQL中跟持久化有关了的配置主要有:

doublewrite buffer的打开和关闭
innodb_flush_log_at_trx_commit的配置
sync_binlog的配置
innodb_file_per_table的配置
存储设备中的电池备份缓存
运行MySQL数据库的操作系统选择，必须要支持fsync() 系统调用
运行MySQL的服务器电源使用UPS，保护MySQL服务器和存储设备不会因为断电出现异常
对于分布式应用程序，需要考虑应用程序和MySQL数据中心之间的网络
一般MySQL运行的操作系统选择Linux，如Centos。如果是自建机房，那么运行MySQL的服务器尽量单独一台，且配置高一些，磁盘最好使用ssd。

下面着重讲解MySQL关于持久化的配置

1 doublewrite buffer的打开和关闭

1.1 建议打开doublewrite buffer

InnoDB doublewrite 机制提高了InnoDB的可靠性，解决了InnoDB部分数据写入失败(即: partial page write页断裂)的问题， 保证了数据的一致性。

double write虽然是一个buffer, 但是它是开在物理文件上的一个buffer, 其实也就是file, 所以它会导致系统有更多的fsync操作, 而硬盘的fsync性能是很慢的, 从而降低mysql的整体性能。

不过 doublewrite buffer写入磁盘共享表空间这个过程是连续存储，是顺序写，性能非常高，(约占写的%10)，牺牲一点写性能来保证数据页的完整还是很有必要的。所以一般建议打开MySQL的doublewrite buffer

1.2 监控double write工作负载

mysql> show global status like '%dblwr%';
+----------------------------+---------+
| Variable_name | Value |
+----------------------------+---------+
| Innodb_dblwr_pages_written | 2432011 |
| Innodb_dblwr_writes | 188819 |
+----------------------------+---------+
2 rows in set (0.00 sec)
mysql> select (2432011/188819);
+------------------+
| (2432011/188819) |
+------------------+
| 12.8801 |
+------------------+
1 row in set (0.00 sec)
关注点：Innodb_dblwr_pages_written / Innodb_dblwr_writes

开启doublewrite后，每次脏页刷新必须要先写doublewrite，而doublewrite存在于磁盘上的是两个连续的区，每个区由连续的页组成，一般情况下一个区最多有64个页，所以一次IO写入应该可以最多写64个页。

而根据以上系统Innodb_dblwr_pages_written与Innodb_dblwr_writes的比例来看，大概在12左右，远远还没到64(如果约等于64，那么说明系统的写压力非常大，有大量的脏页要往磁盘上写)，所以从这个角度也可以看出，系统写入压力并不高。

1.3 关闭double write的场景

海量DML
不惧怕数据损坏和丢失
系统写负载成为主要负载
作为InnoDB的一个关键特性，doublewrite功能默认是开启的

mysql> show variables like '%double%';
+----------------------------------+----------------+
| Variable_name | Value |
+----------------------------------+----------------+
| innodb_doublewrite | ON |
| innodb_parallel_doublewrite_path | xb_doublewrite |
+----------------------------------+----------------+
2 rows in set (0.01 sec)
2 innodb_flush_log_at_trx_commit的配置

innodb_flush_log_at_trx_commit：是 InnoDB 引擎特有的，ib_logfile的刷新方式（ ib_logfile：记录的是redo log和undo log的信息）， 其取值可以为0， 1， 2

innodb_flush_log_at_trx_commit=0: Innodb 中的Log Thread每隔1 秒钟会将log buffer中的数据写入到文件，同时还会通知文件系统进行文件同步的flush 操作，保证数据确实已经写入到磁盘上面的物理文件。但是，每次事务的结束（commit 或者是rollback）并不会触发Log Thread 将log buffer 中的数据写入文件。所以，当设置为0 的时候，当MySQL Crash 和OS Crash 或者主机断电之后，最极端的情况是丢失1 秒时间的数据变更。

innodb_flush_log_at_trx_commit=1: Innodb 的默认设置，表示在每次事务提交的时候，都把log buffer刷到文件系统中(os buffer)去，并且调用文件系统的“flush”操作将缓存刷新到磁盘上去。这样的话，数据库对IO的要求就非常高了，如果底层的硬件提供的IOPS比较差，那么MySQL数据库的并发很快就会由于硬件IO的问题而无法提升。

innodb_flush_log_at_trx_commit=2: 表示在每次事务提交的时候会把log buffer刷到文件系统中去，但并不会立即刷写到磁盘。如果只是MySQL数据库挂掉了，由于文件系统没有问题，那么对应的事务数据并没有丢失。只有在数据库所在的主机操作系统损坏或者突然掉电的情况下，数据库的事务数据可能丢失1秒之类的事务数据。这样的好处，减少了事务数据丢失的概率，而对底层硬件的IO要求也没有那么高(log buffer写到文件系统中，一般只是从log buffer的内存转移的文件系统的内存缓存中，对底层IO没有压力)。

[玩转MySQL之九]MySQL实现ACID机制之持久性
3 sync_binlog的配置

sync_binlog: 是MySQL 的二进制日志（binary log）同步到磁盘的频率。其可取的值是: 0 ~ N

sync_binlog=0: 当事务提交之后，MySQL不做fsync之类的磁盘同步指令刷新binlog_cache中的信息到磁盘，而让Filesystem自行决定什么时候来做同步，或者cache满了之后才同步到磁盘。这个是性能最好的。

sync_binlog=1: 当每进行1次事务提交之后，MySQL将进行一次fsync之类的磁盘同步指令来将binlog_cache中的数据强制写入磁盘。

sync_binlog=N: 当每进行n次事务提交之后，MySQL将进行一次fsync之类的磁盘同步指令来将binlog_cache中的数据强制写入磁盘。

大多数情况下，对数据的一致性并没有很严格的要求，所以并不会把 sync_binlog 配置成 1. 为了追求高并发，提升性能，可以设置为 100 或直接用 0.

而和 innodb_flush_log_at_trx_commit 一样，对于支付服务这样的应用，还是比较推荐 sync_binlog = 1.

4 innodb_file_per_table配置

Innodb存储引擎可将所有数据存放于ibdata*的共享表空间，也可将每张表存放于独立的.ibd文件的独立表空间。

共享表空间以及独立表空间都是针对数据的存储方式而言的。

4.1 共享表空间

某一个数据库的所有的表数据，索引文件全部放在一个文件中，默认这个共享表空间的文件路径在data目录下。 默认的文件名为:ibdata1 初始化为10M。

这个表空间可以由很多个文件组成，一个表可以跨多个文件存在，所以其大小限制不再是文件大小的限制，而是其自身的限制。从Innodb的官方文档中可以看到，其表空间的最大限制为64TB，也就是说，Innodb的单表限制基本上也在64TB左右了，当然这个大小是包括这个表的所有索引等其他相关数据。

共享表空间的优点
1) 可以将表空间分成多个文件存放到各个磁盘上（表空间文件大小不受表大小的限制，如一个表可以分布在不同的文件上）。

2) 数据和文件放在一起方便管理。

共享表空间的缺点
1) 所有的数据和索引存放到一个文件，虽然可以把一个大文件分成多个小文件，但是多个表及索引在表空间中混合存储，当数据量非常大的时候，表做了大量删除操作后表空间中将会有大量的空隙，特别是对于统计分析，对于经常删除操作的这类应用最不适合用共享表空间。

2) 共享表空间分配后不能回缩：当出现临时建索引或是创建一个临时表的操作表空间扩大后，就是删除相关的表也没办法回缩那部分空间了（可以理解为oracle的表空间10G，但是才使用10M，但是操作系统显示mysql的表空间为10G），进行数据库的冷备很慢；

4.2 独立表空间

每一个表都将会生成以独立的文件方式来进行存储，每一个表都有一个.frm表描述文件，还有一个.ibd文件。 其中这个文件包括了单独一个表的数据内容以及索引内容，默认情况下它的存储位置也是在表的位置之中。

独立表空间的优点
1) 每个表都有自已独立的表空间。

2) 每个表的数据和索引都会存在自已的表空间中。

3) 可以实现单表在不同的数据库中移动。

4) 空间可以回收（除drop table操作处，表空间不能自已回收）

i: Drop table操作自动回收表空间，如果对于统计分析或是日值表，删除大量数据后可以通过:alter table TableName engine=innodb;回缩不用的空间。

ii: 对于使innodb-plugin的Innodb使用turncate table也会使空间收缩

iii: 对于使用独立表空间的表，不管怎么删除，表空间的碎片不会太严重的影响性能，而且还有机会处理。

独立表空间的缺点
1) 单表增加过大，当单表占用空间过大时，存储空间不足，只能从操作系统层面思考解决方法；

2) 单表增加过大，如超过100个G，相比较之下，使用独占表空间的效率以及性能会更高一点。

4.3 结论

共享表空间在Insert操作上少有优势。其它都没独立表空间表现好。当启用独立表空间时，请合理调整一 下：innodb_open_files 。InnoDB Hot Backup（冷备）的表空间cp不会面对很多无用的copy了。而且利用innodb hot backup及表空间的管理命令可以实现单现移动。
```

### 支持多钟数据类型
```
MySQL 数据类型
MySQL中定义数据字段的类型对你数据库的优化是非常重要的。

MySQL支持多种类型，大致可以分为三类：数值、日期/时间和字符串(字符)类型。

数值类型
MySQL支持所有标准SQL数值数据类型。

这些类型包括严格数值数据类型(INTEGER、SMALLINT、DECIMAL和NUMERIC)，以及近似数值数据类型(FLOAT、REAL和DOUBLE PRECISION)。

关键字INT是INTEGER的同义词，关键字DEC是DECIMAL的同义词。

BIT数据类型保存位字段值，并且支持MyISAM、MEMORY、InnoDB和BDB表。

作为SQL标准的扩展，MySQL也支持整数类型TINYINT、MEDIUMINT和BIGINT。下面的表显示了需要的每个整数类型的存储和范围。

类型	大小	范围（有符号）	范围（无符号）	用途
TINYINT	1 byte	(-128，127)	(0，255)	小整数值
SMALLINT	2 bytes	(-32 768，32 767)	(0，65 535)	大整数值
MEDIUMINT	3 bytes	(-8 388 608，8 388 607)	(0，16 777 215)	大整数值
INT或INTEGER	4 bytes	(-2 147 483 648，2 147 483 647)	(0，4 294 967 295)	大整数值
BIGINT	8 bytes	(-9,223,372,036,854,775,808，9 223 372 036 854 775 807)	(0，18 446 744 073 709 551 615)	极大整数值
FLOAT	4 bytes	(-3.402 823 466 E+38，-1.175 494 351 E-38)，0，(1.175 494 351 E-38，3.402 823 466 351 E+38)	0，(1.175 494 351 E-38，3.402 823 466 E+38)	单精度
浮点数值
DOUBLE	8 bytes	(-1.797 693 134 862 315 7 E+308，-2.225 073 858 507 201 4 E-308)，0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308)	0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308)	双精度
浮点数值
DECIMAL	对DECIMAL(M,D) ，如果M>D，为M+2否则为D+2	依赖于M和D的值	依赖于M和D的值	小数值
日期和时间类型
表示时间值的日期和时间类型为DATETIME、DATE、TIMESTAMP、TIME和YEAR。

每个时间类型有一个有效值范围和一个"零"值，当指定不合法的MySQL不能表示的值时使用"零"值。

TIMESTAMP类型有专有的自动更新特性，将在后面描述。

类型	大小
( bytes)	范围	格式	用途
DATE	3	1000-01-01/9999-12-31	YYYY-MM-DD	日期值
TIME	3	'-838:59:59'/'838:59:59'	HH:MM:SS	时间值或持续时间
YEAR	1	1901/2155	YYYY	年份值
DATETIME	8	1000-01-01 00:00:00/9999-12-31 23:59:59	YYYY-MM-DD HH:MM:SS	混合日期和时间值
TIMESTAMP	4	
1970-01-01 00:00:00/2038

结束时间是第 2147483647 秒，北京时间 2038-1-19 11:14:07，格林尼治时间 2038年1月19日 凌晨 03:14:07

YYYYMMDD HHMMSS	混合日期和时间值，时间戳
字符串类型
字符串类型指CHAR、VARCHAR、BINARY、VARBINARY、BLOB、TEXT、ENUM和SET。该节描述了这些类型如何工作以及如何在查询中使用这些类型。

类型	大小	用途
CHAR	0-255 bytes	定长字符串
VARCHAR	0-65535 bytes	变长字符串
TINYBLOB	0-255 bytes	不超过 255 个字符的二进制字符串
TINYTEXT	0-255 bytes	短文本字符串
BLOB	0-65 535 bytes	二进制形式的长文本数据
TEXT	0-65 535 bytes	长文本数据
MEDIUMBLOB	0-16 777 215 bytes	二进制形式的中等长度文本数据
MEDIUMTEXT	0-16 777 215 bytes	中等长度文本数据
LONGBLOB	0-4 294 967 295 bytes	二进制形式的极大文本数据
LONGTEXT	0-4 294 967 295 bytes	极大文本数据
注意：char(n) 和 varchar(n) 中括号中 n 代表字符的个数，并不代表字节个数，比如 CHAR(30) 就可以存储 30 个字符。

CHAR 和 VARCHAR 类型类似，但它们保存和检索的方式不同。它们的最大长度和是否尾部空格被保留等方面也不同。在存储或检索过程中不进行大小写转换。

BINARY 和 VARBINARY 类似于 CHAR 和 VARCHAR，不同的是它们包含二进制字符串而不要非二进制字符串。也就是说，它们包含字节字符串而不是字符字符串。这说明它们没有字符集，并且排序和比较基于列值字节的数值值。

BLOB 是一个二进制大对象，可以容纳可变数量的数据。有 4 种 BLOB 类型：TINYBLOB、BLOB、MEDIUMBLOB 和 LONGBLOB。它们区别在于可容纳存储范围不同。

有 4 种 TEXT 类型：TINYTEXT、TEXT、MEDIUMTEXT 和 LONGTEXT。对应的这 4 种 BLOB 类型，可存储的最大长度不同，可根据实际情况选择。

```

### 可利用 CPU 多核心
```
在工作中可能遇到这样的情况，随着业务的增长，用户量也在逐渐增长，终究有一天，一到高峰期，数据库服务器CPU利用率直飚100%。

最简单的做法就是直接提升硬件性能，简单粗暴，直接有效。

假如我们最开始的服务器CPU核数是4，然后我们觉得4个有点扛不住，那就直接给他搞16个核。这下应该没啥问题了吧，用户访问站点貌似也比以前快了。但是不要高兴太早，当你打开任务管理器，你会发现利用率高的还是以前那几个核，或者说只有4个用的比较多，其它12位大爷感觉不接茬。

如果你去查一下innodb_read_io_threads和innodb_write_io_threads这两个变量的时候，你就回发现，它们的默认值是4。可以用以下语句查看这两个变量的值：

show variables like '%_io_threads'

那就往大放呗。如果你直接用set命令去修改，你就回发现，人家会告你这俩命令是只读变量。那好吧，只能在初始化文件中添加这两个变量了。找到mysql的安装目录，用管理员身份打开mysql.ini文件，在最后添加这么两句：

innodb_read_io_threads=16
innodb_read_write_threads=16

重启MySQL服务，再去观察一下任务管理器，你就会发现，这次任务分配到每个处理器的任务量就均匀了些。
```

### 内存淘汰机制
```
http://www.mamicode.com/info-detail-2679625.html
首先我们说一下大查询会不会把内存打爆？

比如说主机内存有5g，但是我们一个大查询的数据有10g，这样会不会把内存打爆呢？

答案：不会

为什么？

因为mysql读取数据是采取边读边发的策略

select * from t1

这条语句的流程是这样的

技术图片

 

1.读取数据放入net_buffer中，net_buffer大小是由net_buffer_length控制

2.net_buffer放满了以后，调用网络栈发送数据到客户端

3.如果发送成功就清空net_buffer，继续读取数据放入net_buffer中

4.如果发送函数返回EAGAIN或者WSAEWOULDBLOCK就表示本地网络栈满了，这时候就进入等待，知道网络栈重新可写，再继续发送。

根据这个流程来看，读取数据的时候占用的内存最多也就是net_buffer的大小。

 

InnoDB内存（buffer pool）管理

我们都知道mysql查询数据是先看内存中有没有数据，如果没有就从磁盘中读出来，然后在读入内存

所以说bufferpool对查询有加速效果，加速效果依赖于一个指标也就是内存命中率，如果命中率能达到100%那是最好的

通过

show engine innodb status

可以查看命中率

innodb buffer pool的大小是由参数innodb_buffer_pool_size控制的，一般设置为可用物理内存的60%-80%

 

内存淘汰

 

既然内存是一块固定大小的，那么存放在内存里的数据就肯定有的会被淘汰

下面是一个lur算法的基本模型

技术图片

 

innodb管理bufferpool的lru算法是基于链表实现的

state1:我们要查询p3的数据，由于p3是在内存中的，那么久直接把p3移动到链表头部，

也就是对应图中state2的状态

state3中由于我们查询的px数据不是在px中，那么就从磁盘中查询出px的数据放入链表头部，

但是由于内存满了，所以

就会把pm的数据从链表尾部淘汰掉，从现象上来看就是最久没有被访问都的数据会被淘汰

 

这种算法对于mysql来说有什么问题？？

如果我们对一个冷数据表进行全表扫描，比如说日志表，这些不是正常用户访问的表，

那么在bufferpool中就会大量存在这些数据的表，那么就会导致用户正常访问存放的业务数据会被淘汰掉，

就会导致大量数据需要重新读磁盘放入内存，这样性能就会大大降低

 

mysql肯定不会允许这种情况发生的，所以它基于上面的lru算法做了改进

下图就是改进后的模型

技术图片

 

 innodb把整个内存的前八分之五记为young区域，后八分之三记为old区域，

我们看上图state1中由于我们访问的p3是在young区域，那么就把p3移动到链表头部

但是如果我们访问的数据如果是在old区域，比如说我们访问了px，这个时候会做个判断

如果px在内存中存活时间超过1秒，就会把它移动到young区域的链表头部，否则位置不动

这个1秒是由参数

innodb_old_blocks_time控制的，默认值是1000，单位毫秒

 

这样我们在看扫描全表的步骤

扫描过程中被访问的数据页会被放在old区域

一个数据页有多条记录会被访问，所以这数据页会被多次访问到，但是由于是顺序扫描，

这个数据页第一次被访问和最后一次被访问的时间间隔不会超过一秒，所以就会一直在old区域

在继续扫描后面的数据页，之前的这个数据页也不会被访问到，因此就会一直在old区域，也就很快就会被淘汰掉了

 

可以看到这个策略的最大收益，就是在扫描的过程中，虽然也用到了bufferpool，

但是不会对young区域造成影响，也就保证了bufferpool响应业务的内存命中率
```

### 集群 Cluster
```
1. 参考文档
http://xuwensong.elastos.org/2014/01/13/ubuntu-%E4%B8%8Bmysql-cluster%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/

2. 简介
MySQL-Cluster是一种技术，该技术允许在无共享的系统中部署“内存中”数据库的簇。通过无共享体系结构，系统能够使用廉价的硬件，而且对软硬件无特殊要求。此外，由于每个组件都有自己的内存和磁盘，所以不存在单点故障。

MySQL簇将标准的MySQL服务器与名为NDB的“内存中”的簇式存储引擎集成了起来。术语NDB指的是与存储引擎相关的设置部分，而术语“MySQL簇”指的是MySQL和NDB存储引擎的组合。

MySQL簇由一组计算机构成，每台计算机上均运行着多种进程，包括MySQL服务器，NDB簇的数据节点，管理服务器（MGM），以及（可能）专门的数据访问程序。关于簇中组件的关系，如下图：

 

所有这些程序一起构成了MySQL簇。将数据保存到NBD簇引擎中时，表将保存在数据节点内。能够从簇中所有其他MySQL服务器直接访问这些表。因此，假如在将数据保存在簇内的工资应用程序中，如果某一应用程序更新了一位雇员的工资，所有查询该数据的其他MySQL 服务器能立刻发现这种变化。

对于MySQL簇，保存在数据节点的数据可被映射，簇能够处理单独数据节点的故障，除了少数事物将因事物状态丢失而被放弃外，不会产生其他影响。由于事物性应用程序能够处理失败事宜，因而它不是问题源。

3. MySQL簇的基本概念
NDB 是一种“内存中”存储引擎，它具有可用性高和数据一致性好的特点。

能够使用多种故障切换和负载平衡选项配置NDB 存储引擎，但以簇层面上的存储引擎开始最简单。MySQL簇的NDB存储引擎包含完整的数据集，仅取决于簇本身内的其他数据。

下面名，我们将介绍设置由NDB存储引擎和一些MySQL服务器构成的MySQL簇的设置方法。

目前，MySQL簇的部分可以独立于MySQL服务器进行配置。在MySQL簇中，簇的每个部分被视为一个节点。

注释：在很多情况下，术语“节点”用于指计算机，但在讨论MySQL簇时，它表示的是进程。在单台计算机上可以有任意数目的节点，所以我们才有可能将多个不同功能的节点配置在同一台计算机上，为此，我们采用术语簇主机。

有三类簇节点，在最低的MySQL簇配置中，至少有三个节点，这三类节点分别是：

管理节点（MGM）：这类节点的作用是管理MySQL簇内的其他节点，如提供配置数据，启动并停止节点，运行备份等。由于这类节点负责管理其他节点的配置，所以管理节点应在其他节点之前先启动。MGM节点是用命令ndb_mgm启动的。

数据节点（NDB）：这类节点用于保存簇的数据。数据集点的数目与副本的数目相关，是片段的倍数。例如，对于两个副本，每个副本又两个片段，那么就有4个数据节点。没有必要有一个以上的副本。。数据节点是用命令ndbd来启动的。

SQL节点：这类节点是用来访问簇数据的节点。对于MySQL簇，客户端节点是使用NDB 簇存储引擎的传统MySQL服务器。典型情况下，SQL节点是使用命令mysql -ndbcluster来启动的，或将ndbcluster添加到my.cnf后面使用mysqld启动。

簇配置包括对簇中单独节点的配置，以及设置节点之间的单独通信链路。对于目前设计的MySQL簇，其意图在于，从处理器的能力，内存空间和宽带来讲，存储节点是同质的，此外，为了提供单一的配置点，作为整体，簇的所有配置均位于一个文件中。

管理服务器（MGM节点）负责管理簇配置文件和簇日志。簇中的每个节点从管理服务器检索配置数据，并请求确定管理服务器所在的位置的方式。当数据节点内出现有趣的事件时，节点将关于这类事件的信息传输到管理服务器，然后，啊经这类信息写入簇日志。



4. 实现环境
现在，我们计划建立有5个节点的MySQL CLuster体系，因此需要用到3台机器(sql和数据节点共用)，分别做如下用途：

                 节点(用途)                 IP地址(主机名)
                 管理节点(MGM)              10.24.0.101(db1) nodeid = 1

                 数据节点1(NDBD1)           10.24.6.4 (db4) nodeid = 11
                 数据节点2(NDBD2)           10.24.6.6(db5) nodeid =12
                 SQL节点1(SQL1)            10.24.6.4(db2) nodeid = 21
                 SQL节点2(SQL2)            10.24.6.6(db3) nodeid = 22

5. MySQL-Cluster安装包下载
对于MySQL-Cluster的安装包下载，下载地址见http://dev.mysql.com/downloads/cluster/

mysql-cluster-gpl-7.4.7-debian7-x86_64.deb

6. 安装
6.1. 清除之前的mysql痕迹
此外 如果之前安装过mysql-server，在进行此次实验之前，需要将mysql-server卸载，执行以下指令卸载mysql

sudo apt-get autoremove --purge mysql-server

sudo apt-get remove mysql-server

sudo apt-get autoremove mysql-server

sudo apt-get remove mysql-common (非常重要)

6.2. 安装deb文件
sudo dpkg -i mysql-cluster-gpl-7.4.7-debian7-x86_64.deb

安装目录/opt/mysql/server-5.6

6.3. 存储节点/SQL节点安装
SQL节点和数据节点的安装步骤基本相同，所以在设计为存储节点或SQL节点的的每一台机器上，以系统根用户身份执行以下步骤：

mysql组和mysql用户

检查/etc/passwd和/etc/group/文件，查看在系统上是否已经存在mysql组和mysql用户，这时因为某些操作系统会将其作为安装进程的一部分创建。可以使用以下指令查看：

cat show /etc/passwd

cat show /etc/group

如果它们不存在，则需要创建新的mysql用户组，然后为该组添加一个mysql用户

groupadd mysql

useradd -g mysql mysql

6.4. 创建系统数据库的脚本
sudo /opt/mysql/server-5.6/scripts/mysql_install_db --user=mysql

如果此脚本不能运行，若报错为主机名不匹配的话，则很有可能是下载的版本不对，查看操作系统是32位还是64位，选择正确的版本。若报错找不到默认的文件，则很有可能是因为之前的mysql-server没有卸载，执行上文提供的指令，彻底卸载mysql-server

成功结果：

To start mysqld at boot time you have to copy

support-files/mysql.server to the right place for your system

PLEASE REMEMBER TO SET A PASSWORD FOR THE MySQL root USER !

To do so, start the server, then issue the following commands:

  /opt/mysql/server-5.6/bin/mysqladmin -u root password 'new-password'

  /opt/mysql/server-5.6/bin/mysqladmin -u root -h drbd01 password 'new-password'

Alternatively you can run:

  /opt/mysql/server-5.6/bin/mysql_secure_installation

which will also give you the option of removing the test

databases and anonymous user created by default.  This is

strongly recommended for production servers.

See the manual for more instructions.

You can start the MySQL daemon with:

  cd /opt/mysql/server-5.6 ; /opt/mysql/server-5.6/bin/mysqld_safe &

You can test the MySQL daemon with mysql-test-run.pl

  cd mysql-test ; perl mysql-test-run.pl

Support MySQL by buying support/licenses at http://shop.mysql.com

WARNING: Found existing config file /opt/mysql/server-5.6/my.cnf on the system.

Because this file might be in use, it was not replaced,

but was used in bootstrap (unless you used --defaults-file)

and when you later start the server.

The new default config file was created as /opt/mysql/server-5.6/my-new.cnf,

please compare it with your file and take the changes you need.

6.5.设置MySQL服务器和数据目录必要的权限
chown -R root .

chown -R mysql data

3

chgrp -R mysql .

6.6. 拷贝mysql.server
 	 
 	 
sudo cp /opt/mysql/server-5.6/support-files/mysql.server /etc/init.d/mysql

chmod +x /etc/init.d/mysql

6.7. 拷贝 my.cnf
sudo cp /opt/mysql/server-5.6/my-new.cnf /etc/my.cnf

sudo vim /etc/my.cnf

 

 

6.8. 设置root密码
sudo apt-get install mysql-client

mysqladmin -u root flush-privileges password "123456"

 

6.9. 管理节点安装
对于管理（MGM）节点，不需要安装mysqld可执行文件，仅需要安装用于MGM服务器和客户端的二进制文件，这类文件可在下载的档案文件中找到。假定将下载的档案文件放在了/var/tmp文件下，以系统管理员的身份执行以下步骤，在簇管理节点主机上安装ndb_mgmd和ndb_mgm

ndb_mgmd：ndb管理服务器

ndb_mgm: ndb管理客户端

6.10. 安装deb文件
sudo dpkg -i mysql-cluster-gpl-7.4.7-debian7-x86_64.deb

安装目录/opt/mysql/server-5.6

 

6.11. 创建管理目录
sudo mkdir /usr/local/mysql/

6.12. 拷贝ndb管理程序
sudo cp /opt/mysql/server-5.6/bin/ndb_mgm* /usr/local/mysql/

6.13. 端口
注释：簇管理节点的默认端口是1186,数据节点的默认端后是2202。

 



6.14.  配置ndb管理节点
cd /usr/local/mysql/

sudo vim config.ini：

 

# Options affecting ndbd processes on all data nodes:

[NDBD DEFAULT]

NoOfReplicas=2

DataMemory=80M

IndexMemory=18M

 

# TCP/IP options:

[TCP DEFAULT]

#portnumber=2202

 

# Management process options:

[NDB_MGMD]

nodeid=1

HostName=10.24.0.101

DataDir=/usr/local/mysql

 

# Options for data node :

[NDBD]

nodeid=11

HostName=10.24.6.4

DataDir=/opt/mysql/server-5.6/data/

 

# Options for data node :

[NDBD]

nodeid=12

HostName=10.24.6.6

DataDir=/opt/mysql/server-5.6/data/

 

# SQL node options:

[MYSQLD]

nodeid=21

HostName=10.24.6.4

[MYSQLD]

nodeid=22

HostName=10.24.6.6

 

 



7. 启动
完成配置后，启动簇并不困难。必须在数据节点所在的主机上分别启动每个簇节点进程。尽管能够按任何顺序启动节点，但还是建议，首先启动管理节点，然后启动存储节点，最后启动SQL节点。

7.1. 管理节点启动
可使用nbd_mgm指令登录到ndb_mgm客户端，登录后，可使用show指令来查看簇中个节点情况。

注意，启动MGM时，必须用-f或者–config-file选项，告诉ndb_mgmd到哪里找到配置文件。首次启动时必须选用–initial选项，或者更改了MGM节点的配置信息后，也需选用–initial选项。

 

sudo /usr/local/mysql/ndb_mgmd -f /usr/local/mysql/config.ini

 

Ndb客户端查看:

 

wiki@zoweewiki:/usr/local/mysql$ /usr/local/mysql/ndb_mgm

-- NDB Cluster -- Management Client --

ndb_mgm>

ndb_mgm>

ndb_mgm>

ndb_mgm> show

Connected to Management Server at: localhost:1186

Cluster Configuration

---------------------

[ndbd(NDB)]     2 node(s)

id=11   @10.24.6.4  (mysql-5.6.25 ndb-7.4.7, Nodegroup: 0, *)

id=12   @10.24.6.6  (mysql-5.6.25 ndb-7.4.7, Nodegroup: 0)

 

[ndb_mgmd(MGM)] 1 node(s)

id=1    @10.24.0.101  (mysql-5.6.25 ndb-7.4.7)

 

[mysqld(API)]   2 node(s)

id=21   @10.24.6.4  (mysql-5.6.25 ndb-7.4.7)

id=22   @10.24.6.6  (mysql-5.6.25 ndb-7.4.7)



7.2. 数据节点启动
在每台数据节点主机上，对于首次启动，运行下述命令启动NDBD进程：

sudo /opt/mysql/server-5.6/bin/ndbd --initial

注意，应仅在首次启动时ndbd时，或者在备份/恢复或者配置变化后重启ndbd时使用“–initial”参数，这很重要，因为该参数会使数据节点删除由早期ndbd实例创建的，用于恢复的任何文件，包括恢复用日志文件。

 

7.3. MYSQL节点启动
sudo /etc/init.d/mysql restart

 

启动日志：

/opt/mysql/server-5.6/data/drbd02.err

151015 14:33:19 mysqld_safe Starting mysqld daemon with databases from /opt/mysql/server-5.6/data

2015-10-15 14:33:22 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).

2015-10-15 14:33:22 0 [Note] /opt/mysql/server-5.6/bin/mysqld (mysqld 5.6.25-ndb-7.4.7-cluster-gpl-log) starting as process 15192 ...

2015-10-15 14:33:23 15192 [Note] Plugin 'FEDERATED' is disabled.

2015-10-15 14:33:23 15192 [Note] InnoDB: Using atomics to ref count buffer pool pages

2015-10-15 14:33:23 15192 [Note] InnoDB: The InnoDB memory heap is disabled

2015-10-15 14:33:23 15192 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins

2015-10-15 14:33:23 15192 [Note] InnoDB: Memory barrier is not used

2015-10-15 14:33:23 15192 [Note] InnoDB: Compressed tables use zlib 1.2.3

2015-10-15 14:33:23 15192 [Note] InnoDB: Using Linux native AIO

2015-10-15 14:33:23 15192 [Note] InnoDB: Not using CPU crc32 instructions

2015-10-15 14:33:23 15192 [Note] InnoDB: Initializing buffer pool, size = 128.0M

2015-10-15 14:33:23 15192 [Note] InnoDB: Completed initialization of buffer pool

2015-10-15 14:33:24 15192 [Note] InnoDB: Highest supported file format is Barracuda.

2015-10-15 14:33:24 15192 [Note] InnoDB: 128 rollback segment(s) are active.

2015-10-15 14:33:24 15192 [Note] InnoDB: Waiting for purge to start

2015-10-15 14:33:24 15192 [Note] InnoDB: 5.6.25 started; log sequence number 1626027

2015-10-15 14:33:24 15192 [Note] NDB: Changed global value of binlog_format from STATEMENT to MIXED

2015-10-15 14:33:24 15192 [Note] NDB: NodeID is 22, management server '10.24.0.101:1186'

2015-10-15 14:33:25 15192 [Note] NDB[0]: NodeID: 22, all storage nodes connected

2015-10-15 14:33:25 15192 [Warning] NDB: server id set to zero - changes logged to bin log with server id zero will be logged with another server id by slave mysqlds

2015-10-15 14:33:25 15192 [Note] NDB Binlog: Starting...

2015-10-15 14:33:25 15192 [Note] NDB Util: Starting...

2015-10-15 14:33:25 15192 [Note] NDB Index Stat: Starting...

2015-10-15 14:33:25 15192 [Note] NDB Index Stat: Wait for server start completed

2015-10-15 14:33:25 15192 [Note] NDB Util: Wait for server start completed

2015-10-15 14:33:25 15192 [Note] NDB Binlog: Started

2015-10-15 14:33:25 15192 [Note] NDB Binlog: Setting up

2015-10-15 14:33:25 15192 [Note] NDB Binlog: Created schema Ndb object, reference: 0x80040016, name: 'Ndb Binlog schema change monitoring'

2015-10-15 14:33:25 15192 [Note] NDB Binlog: Created injector Ndb object, reference: 0x80050016, name: 'Ndb Binlog data change monitoring'

2015-10-15 14:33:25 15192 [Note] NDB Binlog: Setup completed

2015-10-15 14:33:25 15192 [Note] NDB Binlog: Wait for server start completed

2015-10-15 14:33:25 15192 [Note] Server hostname (bind-address): '*'; port: 3306

2015-10-15 14:33:25 15192 [Note] IPv6 is available.

2015-10-15 14:33:25 15192 [Note]   - '::' resolves to '::';

2015-10-15 14:33:25 15192 [Note] Server socket created on IP: '::'.

2015-10-15 14:33:25 15192 [Note] Event Scheduler: Loaded 0 events

2015-10-15 14:33:25 15192 [Note] /opt/mysql/server-5.6/bin/mysqld: ready for connections.

Version: '5.6.25-ndb-7.4.7-cluster-gpl-log'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Cluster Community Server (GPL)

2015-10-15 14:33:25 15192 [Note] NDB Util: Wait for cluster to start

2015-10-15 14:33:25 15192 [Note] NDB Util: Started

2015-10-15 14:33:25 15192 [Note] NDB Binlog: Check for incidents

2015-10-15 14:33:25 15192 [Note] NDB Binlog: Wait for cluster to start

2015-10-15 14:33:25 15192 [Note] NDB Index Stat: Wait for cluster to start

2015-10-15 14:33:25 15192 [Note] ndb_index_stat_proc: Created Ndb object, reference: 0x80070016, name: 'Ndb Index Statistics monitoring'

2015-10-15 14:33:25 15192 [Note] NDB Index Stat: Started

2015-10-15 14:33:26 15192 [Note] NDB Binlog: DISCOVER TABLE Event: REPL$mysql/ndb_schema

2015-10-15 14:33:26 15192 [Note] NDB Binlog: logging ./mysql/ndb_schema (UPDATED,USE_WRITE)

2015-10-15 14:33:26 15192 [Note] NDB Binlog: DISCOVER TABLE Event: REPL$mysql/ndb_apply_status

2015-10-15 14:33:26 15192 [Note] NDB Binlog: logging ./mysql/ndb_apply_status (UPDATED,USE_WRITE)

2015-10-15 14:33:26 15192 [Note] NDB: Cleaning stray tables from database 'ndb_12_fs'

2015-10-15 14:33:26 15192 [Note] NDB: Cleaning stray tables from database 'ndbinfo'

2015-10-15 14:33:26 15192 [Note] NDB: Cleaning stray tables from database 'performance_schema'

2015-10-15 14:33:26 15192 [Note] NDB: Cleaning stray tables from database 'test'

2015-10-15 14:33:26 15192 [Note] NDB Binlog: Wait for first event

2015-10-15 14:33:26 [NdbApi] INFO     -- Flushing incomplete GCI:s < 4554/3

2015-10-15 14:33:26 [NdbApi] INFO     -- Flushing incomplete GCI:s < 4554/3

2015-10-15 14:33:26 15192 [Note] NDB Binlog: starting log at epoch 4554/3

2015-10-15 14:33:26 15192 [Note] NDB Binlog: Got first event

2015-10-15 14:33:26 15192 [Note] NDB Binlog: ndb tables writable

2015-10-15 14:33:26 15192 [Note] NDB Binlog: Startup and setup completed

2015-10-15 14:33:26 15192 [Note] NDB Schema dist: Data node: 11 reports subscribe from node 21, subscriber bitmask 0200000

2015-10-15 14:33:26 15192 [Note] NDB Schema dist: Data node: 12 reports subscribe from node 21, subscriber bitmask 0200000

2015-10-15 14:34:03 15192 [Warning] IP address '10.24.6.170' could not be resolved: Name or service not known

2015-10-15 14:34:56 15192 [Note] NDB Schema dist: Data node: 11 reports subscribe from node 21, subscriber bitmask 00

2015-10-15 14:34:56 15192 [Note] NDB Schema dist: Data node: 12 reports subscribe from node 21, subscriber bitmask 00

2015-10-15 14:35:03 15192 [Note] NDB Schema dist: Data node: 11 reports subscribe from node 21, subscriber bitmask 0200000

2015-10-15 14:35:03 15192 [Note] NDB Schema dist: Data node: 12 reports subscribe from node 21, subscriber bitmask 0200000

2015-10-15 14:41:04 15192 [Note] NDB Schema dist: Data node: 11 failed, subscriber bitmask 00

2015-10-15 14:42:36 15192 [Note] NDB Schema dist: Data node: 12 reports subscribe from node 21, subscriber bitmask 00



8. 测试
8.1. 常规测试
为了让数据表能够在cluster中正常复制，创建数据表时必须指定为ndbcluster引擎（engine=ndb或engine=ndbcluster）。

登录到10.24.64的mysql，然后创建一个新数据库songzi，并创建一个ndbcluster引擎的数据表test（id int，name char（10））,并向表中插入一条数据（0,songzi）。10.24.6.4上的操作及结果如下图：

 

然后登录10.24.6.6的mysql，可查看到数据已同步，并且新建的表及数据也存在。10.24.6.6上的操作及结果如下图：

 



8.2. 模拟NDB节点Crash
终止10.24.6.4上的NDB进程，执行以下指令查看NDB进程情况

ps -ef | grep ndbd

kill 24077

ps -ef | grep ndbd

具体操作及结果见下图：

 

也可以在ndb_mgm管理节点客户端查看到10.24.0.101上的NDB节点已停掉

 

然后分别登录到10.24.6.4和10.24.6.6的mysql，可发现依然能够查询到数据。结果如下图

10.24.6.4

 

10.24.6.6

 

此结果说明测试成功，即当有一个NDB节点Crash后，整个MySQL环境仍可以正常服务。

8.3. 模拟SQL节点Crash
在上10.24.6.4上终止mysqld进程，可执行以下指令：

killall mysqld

也可以在ndb_mgm管理节点客户端查看到10.24.6.4上的SQL节点已停掉

 

登录到10.24.6.6上的mysql，可查看到数据依然存在

 

此结果说明测试成功，及当有一个SQL节点Crash后，整个MySQL-Cluster环境仍可以工作。

至此，整个MySQL-Cluster安装及配置实验已完成。实验结束后，可使用指令shell> ndb_mgm -e shutdown或者ndb_mgm>shutdown来关闭簇中的所有节点。

9. 遇到问题

10. 总结
Mysql cluster是一个统一的共享集群
多mysql同时共享
一个值多份存储，不是像redis那样根据一致性hash分布存储
高并发、高可用、高伸缩性
share nothing架构
通过增加数据节点扩展：通过32个数据节点实现每秒2亿条NoSQL查询，以及通过16个数据节点每秒查询近250万SQL语句
推荐使用lvs + keepalived + mysql cluster 实现集群mysqlMySQL Cluster是一个实时可扩展且符合ACID的事务型内存数据库

支持 SQL
性能对比
支持事务
应用场景
你之前为了解决什么问题使用的什么，为什么选它？
https://www.cnblogs.com/guipeng/p/11876729.html
1. 为什么需要分布式数据库
    随着计算机和信息技术的迅猛发展和普及，行业应用系统的规模迅速扩大，行业应用所产生的数据量量呈爆炸式增长，动辄达到数百TB甚至数百PB规模，已远远超出现有的传统计算技术和信息系统的处理能力，而集中式数据库面对大规模数据处理逐渐表现出其局限性，因此，人们希望寻找一种能快速处理数据和及时响应用户访问的方法，也希望对数据进行集中分析、管理和维护。这已成为现实世界的迫切需求。

    分布式数据库是在集中式数据库的基础上发展起来的，是计算机技术和网络技术结合的产物。分布式数据库是指数据在物理上分布而逻辑上集中管理的数据库系统。物理上分布指的是分布式数据库的数据分布在物理位置不同并由网络连接的节点或站点上；逻辑上集中是指各数据库之间在逻辑上是一个整体，并由统一的数据库管理系统管理。不同的节点分布可以跨不同的机房、城市甚至国家。

    分布式数据库的主要特点如下：

透明性：用户不必关心数据的逻辑分区和物理位置的分布细节，也不必关心重复副本（冗余数据）的一致性问题，同时不必关心在局部场地上数据库支持哪种数据模型。对于系统开发工程师而言，当数据从一个场地移到另一个场地时不必改写应用程序，使用起来如同一个集中式数据库。
数据冗余性：分布式数据库通过冗余实现系统的可靠性、可用性，并改善其性能。多个节点存储数据副本，当某一节点的数据遭到破坏时，冗余的副本可保证数据的完整性；当工作的节点受损害时，可通过心跳等机制进行切换，系统整体不被破坏。还可以通过热点数据的就近分析原则减少网络通信的消耗，加快访问速度，改善性能。
易于扩展性：在分布式数据库中能够方便地通过水平扩展提高系统的整体性能，也能通过垂直扩展来提供性能，扩展并不需要修改系统程序。
自治性：各节点上的数据由本地的DBMS管理，具有自动处理能力，完成本场地的应用或局部应用。
2. MySQL Cluster原理
    MySQL 群集是 MySQL 适合于分布式计算环境的高可用、高冗余版本。它采用了 NDB Cluster 存储引擎，允许在 1 个群集中运行多个 MySQL 服务器。在 MySQL 5.0 及以上的二进制版本中，以及与最新的 Linux 版本兼容的 RPM 包中提供了该存储引擎。

    MySQL 群集是一种技术，该技术允许在无共享的系统中部署“内存中”和“磁盘中”数据库的 Cluster 。通过无共享体系结构，系统能够使用廉价的硬件，而且对软硬件无特殊要求。此外，由于每个组件有自己的内存和磁盘，不存在单点故障。MySQL Cluster 由一组计算机构成，每台计算机上均运行着多种进程，包括 MySQL 服务器，NDB Cluster 的数据节点，管理服务器，以及（可能存在的）专门的数据访问程序。

管理服务器(MGM节点)负责管理 Cluster 配置文件和 Cluster 日志。Cluster 中的每个节点从管理服务器检索配置数据。

当数据节点内出现新的事件时，节点将关于这类事件的信息传输到管理服务器，然后，将这类信息写入 Cluster 日志。

目前能够运行 MySQL Cluster 的操作系统有 Linux、Mac OS X 和 Solaris，最新的版本已经支持 Windows 操作系统。

MySQL 群集的数据节点之间的通信是不加密的，并且需要高速的带宽，所以建议把群集建立在一个高速局域网内，不建议跨网段、跨公网的部署这种系统体系。



 

MySQL 群集分为三种节点：管理节点，数据节点和SQL节点。

管理节点：主要用于管理各个节点，能够通过命令对某个节点进行重启、关闭、启动等操作。也能够监视全部节点的工作状态。

数据节点：主要是对数据的存储，不提供其他的服务。

SQL节点：主要是对外提供SQL功能，类似一台普通的 MySQL Server。

而SQL节点和数据节点可以是同一台机器，也就是说这台机器即是SQL节点也是数据节点。它们只是逻辑关系上的划分，实际部署时，甚至所有的阶段都可以位于同一台物理机器上，只是配置较复杂些。

 

3. MySQL Cluster的优缺点
优点：

99.999 ％的高可用性
快速的自动失效切换
灵活的分布式体系结构，没有单点故障
高吞吐量和低延迟
可扩展性强，支持在线扩容
缺点：

存在很多限制，比如：不支持外键，数据行不能超过8K（不包括BLOB和text中的数据）
部署、管理、配置很复杂
占用磁盘空间大，内存大
备份和恢复不方便
重启的时候，数据节点将数据load到内存需要很长时间
```

## 服务器篇

### 服务器
```
心中有概念，然后足够的实际操作。
Apache
[百度百科介绍](https://baike.baidu.com/item/apache/6265)
Nginx
[百度百科介绍](https://baike.baidu.com/item/nginx/3817705)
```

### 查看 CPU、内存、时间、系统版本等信息
```
CentOS查看系统信息

一：查看CPU

more /proc/cpuinfo | grep "model name"

grep "model name" /proc/cpuinfo

如果觉得需要看的更加舒服

grep "model name" /proc/cpuinfo | cut -f2 -d:



怎么样，linux的命令就要这样熟悉。

二：查看内存

grep MemTotal /proc/meminfo

grep MemTotal /proc/meminfo | cut -f2 -d:

free -m |grep "Mem" | awk '{print $2}'

三：查看cpu是32位还是64位

查看CPU位数(32 or 64)

#getconf LONG_BIT

#echo $HOSTTYPE

#uname -a

四：查看当前linux的版本

#more /etc/RedHat-release

      #cat /etc/redhat-release

五：查看内核版本

#uname -r

#uname -a

 

六：查看当前时间

date

七：查看硬盘和分区

df -h

 

 



 

fdisk -l

也可以查看分区

du -sh

可以看到全部占用的空间

du /etc -sh

可以看到这个目录的大小

八：查看安装的软件包

查看系统安装的时候装的软件包

cat -n /root/install.log

more /root/install.log | wc -l

查看现在已经安装了那些软件包

rpm -qa

rpm -qa | wc -l

yum list installed | wc -l

不过很奇怪，我通过rpm，和yum这两种方式查询的安装软件包，数量并不一样。没有找到原因。

九：查看键盘布局

cat /etc/sysconfig/keyboard

cat /etc/sysconfig/keyboard | grep KEYTABLE | cut -f2 -d=

十：查看selinux情况

sestatus

sestatus | cut -f2 -d:

cat /etc/sysconfig/selinux

十一：查看ip，mac地址

在ifcfg-eth0 文件里你可以看到mac，网关等信息。

ifconfig

cat /etc/sysconfig/network-scripts/ifcfg-eth0 | grep IPADDR

cat /etc/sysconfig/network-scripts/ifcfg-eth0 | grep IPADDR | cut -f2 -d=

ifconfig eth0 |grep "inet addr:" |awk '{print $2}'|cut -c 6-

ifconfig  | grep 'inet addr:'| grep -v '127.0.0.1' | cut -d: -f2 | awk '{ print $1}'

查看网关

cat /etc/sysconfig/network

查看dns

cat /etc/resolv.conf

十二：查看默认语言

echo $LANG $LANGUAGE

cat /etc/sysconfig/i18n

十三：查看所属时区和是否使用UTC时间

cat /etc/sysconfig/clock

十四：查看主机名

hostname

cat /etc/sysconfig/network

修改主机名就是修改这个文件，同时最好也把host文件也修改
```

### find 、grep 查找文件
```
查找命令：find & grep
区别：(1) find命令是根据文件的属性进行查找，如文件名，文件大小，所有者，所属组，是否为空，访问时间，修改时间等。 

           (2) grep是根据文件的内容进行查找，会对文件的每一行按照给定的模式(patter)进行匹配查找。

一.find命令

　　　　基本格式：find  path [options]

　　　　1.按照文件名查找

　　　　　　(1)find /etc -name httpd.conf　　#在/etc目录下文件httpd.conf
　　　　　　(2)find /etc -name '*srm*'　　#使用通配符*(0或者任意多个)。表示在/etc目录下查找文件名中含有字符串‘srm’的文件
　　　　　　(3)find . -name 'srm*' 　　#表示当前目录下查找文件名开头是字符串‘srm’的文件

　　　　2.按照文件特征查找 　　　　

　　　　　　(1)find / -amin -10 　　# 查找在系统中最后10分钟访问的文件(access time)
　　　　　　(2)find / -atime -2　　 # 查找在系统中最后48小时访问的文件
　　　　　　(3)find / -empty 　　# 查找在系统中为空的文件或者文件夹
　　　　　　(4)find / -group cat 　　# 查找在系统中属于group为cat的文件
　　　　　　(5)find / -mmin -5 　　# 查找在系统中最后5分钟里修改过的文件(modify time)
　　　　　　(6)find / -mtime -1 　　#查找在系统中最后24小时里修改过的文件
　　　　　　(7)find / -user fred 　　#查找在系统中属于fred这个用户的文件
　　　　　　(8)find / -size +10000c　　#查找出大于10000000字节的文件(c:字节，w:双字，k:KB，M:MB，G:GB)
　　　　　　(9)find / -size -1000k 　　#查找出小于1000KB的文件

　　　　3.使用混合查找方式查找文件

　　　　　参数有： ！，-and(-a)，-or(-o)。

　　　　　　(1)find /tmp -size +10000c -and -mtime +2 　　#在/tmp目录下查找大于10000字节并在最后2分钟内修改的文件
   　　   　　 (2)find / -user fred -or -user george 　　#在/目录下查找用户是fred或者george的文件文件
   　　   　　 (3)find /tmp ! -user panda　　#在/tmp目录中查找所有不属于panda用户的文件
    　　  

二、grep命令

　　　  基本格式：grep [options]

 　　　 1.主要参数

　　　　[options]主要参数：
　　　　－c：只输出匹配行的计数。
　　　　－i：不区分大小写
　　　　－h：查询多文件时不显示文件名。
　　　　－l：查询多文件时只输出包含匹配字符的文件名。
　　　　－n：显示匹配行及行号。
　　　　－s：不显示不存在或无匹配文本的错误信息。
　　　　－v：显示不包含匹配文本的所有行。

　　　　pattern正则表达式主要参数：
　　　　\： 忽略正则表达式中特殊字符的原有含义。
　　　　^：匹配正则表达式的开始行。
　　　　$: 匹配正则表达式的结束行。
　　　　\<：从匹配正则表达 式的行开始。
　　　　\>：到匹配正则表达式的行结束。
　　　　[ ]：单个字符，如[A]即A符合要求 。
　　　　[ - ]：范围，如[A-Z]，即A、B、C一直到Z都符合要求 。
　　　　.：所有的单个字符。
　　　　* ：有字符，长度可以为0。

　　　　2.实例　 　　　　　

　　             (1)grep 'test' d*　　#显示所有以d开头的文件中包含 test的行

　                (2)grep ‘test’ aa bb cc 　　 #显示在aa，bb，cc文件中包含test的行

　                (3)grep ‘[a-z]\{5\}’ aa 　　#显示所有包含每行字符串至少有5个连续小写字符的字符串的行

　                (4)grep magic /usr/src　　#显示/usr/src目录下的文件(不含子目录)包含magic的行

　                (5)grep -r magic /usr/src　　#显示/usr/src目录下的文件(包含子目录)包含magic的行

　                (6)grep -w pattern files ：只匹配整个单词，而不是字符串的一部分(如匹配’magic’，而不是’magical’)

　                (7)grep -n 8080 文件（确定8080字段在文件中的行数）

　                (8)grep -n -i "ggpush" 文件 （确定ggpush在文件中的行数并且忽略大小写）

　                (9)grep 'ggpush' *.cnf （匹配当前目录下所有后缀为cnf文件含有过滤字段的内容）

　　　　　详细使用参见：http://www.cnblogs.com/end/archive/2012/02/21/2360965.html

　　(1)grep 'test' d*　　#显示所有以d开头的文件中包含 test的行
　  (2)grep ‘test’ aa bb cc 　　 #显示在aa，bb，cc文件中包含test的行
　  (3)grep ‘[a-z]\{5\}’ aa 　　#显示所有包含每行字符串至少有5个连续小写字符的字符串的行
　  (4)grep magic /usr/src　　#显示/usr/src目录下的文件(不含子目录)包含magic的行
　  (5)grep -r magic /usr/src　　#显示/usr/src目录下的文件(包含子目录)包含magic的行
　  (6)grep -w pattern files ：只匹配整个单词，而不是字符串的一部分(如匹配’magic’，而不是’magical’)

　  (7)grep -n 8080 文件（确定8080字段在文件中的行数）

　  (8)grep -n -i "ggpush" 文件（确定ggpush在文件中的行数并且忽略大小写）

　  (9)grep 'ggpush' *.cnf （匹配当前目录下所有后缀为cnf文件含有过滤字段的内容）

```


 
### awk 处理文本
```
参考资料：
http://man.linuxde.net/awk

http://www.cnblogs.com/chengmo/archive/2013/01/17/2865479.html

http://bbs.chinaunix.net/thread-691456-1-1.html

 

       awk是一种编程语言，用于在linux/unix下对文本和数据进行处理。数据可以来自标准输入(stdin)、一个或多个文件，或其它命令的输出。它支持用户自定义函数和动态正则表达式等先进功能，是linux/unix下的一个强大编程工具。它在命令行中使用，但更多是作为脚本来使用。awk有很多内建的功能，比如数组、函数等，这是它和C语言的相同之处，灵活性是awk做大的优势。

       awk还提供了一系列内置的运算函数（如log、sqr、cos、sin等）和一些用于对字符串进行操作（运算）的函数（如length、substr等等）。这些函数的引用大大的提高了awk的运算功能。作为对条件转移指令的一部分，关系判断是每种程序设计语言都具备的功能，awk也不例外，awk中允许进行多种测试，作为样式匹配，还提供了模式匹配表达式~（匹配）和~!（不匹配）。作为对测试的一种扩充，awk也支持用逻辑运算符。


awk命令格式

awk [options] 'script' var=value file(s) 

awk [options] -f scriptfile var=value file(s)

option有以下选择：

-F fs：fs为分隔符，可以是字符串也可以是正则表达式，这个作用就是制定分隔符

-v：赋值一个用户定义变量，将外部变量传递给awk。VAR=10000 echo | awk -v VARIABLE=$VAR '{ print VARIABLE }'
或者：

var1="aaa" 

var2="bbb" 

echo | awk '{ print v1,v2 }' v1=$var1 v2=$var2
变量之间用空格分隔作为awk的命令行参数跟随在BEGIN、{}和END语句块之后。


script脚本的结构

awk 'BEGIN{ commands } pattern { commands } END{ commands }'

awk脚本通常由：BEGIN语句块、能够使用模式匹配的通用语句块、END语句块3部分组成，这三个部分是可选的。任意一个部分都可以不出现在脚本中，脚本通常是被单引号或双引号中，例如： awk 'BEGIN{ i=0 } { i++ } END{ print i }' filename

第一步：执行BEGIN{ commands }语句块中的语句； 

第二步：从文件或标准输入(stdin)读取一行，然后执行pattern{ commands }语句块，它逐行扫描文件，从第一行到最后一行重复这个过程，直到文件全部被读取完毕。 

第三步：当读至输入流末尾时，执行END{ commands }语句块。


        BEGIN语句块在awk开始从输入流中读取行之前被执行，这是一个可选的语句块，比如变量初始化、打印输出表格的表头等语句通常可以写在BEGIN语句块中。 END语句块在awk从输入流中读取完所有的行之后即被执行，比如打印所有行的分析结果这类信息汇总都是在END语句块中完成，它也是一个可选语句块。 pattern语句块中的通用命令是最重要的部分，它也是可选的。如果没有提供pattern语句块，则默认执行{ print }，即打印每一个读取到的行，awk读取的每一行都会执行该语句块。print中双引号里的内容是直接打印的。pattern语句块类似一个循环体，会对文件中的每一行进行迭代。


awk的内置变量

$n 当前记录的第n个字段，比如n为1表示第一个字段，n为2表示第二个字段。 

$0 这个变量包含执行过程中当前行的文本内容。

FILENAME 当前输入文件的名。

FS 字段分隔符（默认是任何空格）。

NF 表示字段数，在执行过程中对应于当前的字段数。

NR 表示记录数，在执行过程中对应于当前的行号。 

OFMT 数字的输出格式（默认值是%.6g）。

OFS 输出字段分隔符（默认值是一个空格）。 

ORS 输出记录分隔符（默认值是一个换行符）。

RS 记录分隔符（默认是一个换行符）。

使用print $NF可以打印出一行中的最后一个字段，使用$(NF-1)则是打印倒数第二个字段，其他以此类推。

统计文件中的行数：awk 'END{ print NR }' filename。这里只使用了END语句块，在读入每一行的时，awk会将NR更新为对应的行号，当到达最后一行NR的值就是最后一行的行号，所以END语句块中的NR就是文件的行数。

一个每一行中第一个字段值累加：awk 'BEGIN{ sum=0; print "总和：" } { print $1"+"; sum+=$1 } END{ print "等于"; print sum }' 


awk的运算

和普通语言的运算一样，||&&，只有~匹配正则表达式，~！不匹配正则表达式。

awk中next语句使用：在循环逐行匹配，如果遇到next，就会跳过当前行，直接忽略下面语句。而进行下一行匹配。next语句一般用于多行合并。当记录行号除以2余1，就跳过当前行。下面的print NR,$0也不会执行。下一行开始，程序有开始判断NR%2值。这个时候记录行号是：2 ，就会执行下面语句块：'print NR,$0'

打印偶数行：awk 'NR%2==1{next}{print NR,$0;}' awktext.txt

awktext.txt中的内容如下：



将包含有aa的行跳过：awk '/^aa/{next;}{print "\t"$0;}' awktest.txt

结果：



 

awk中允许用如下方式将结果输出到一个文件：

echo | awk '{printf("hello word!n") > "datafile"}' 或 echo | awk '{printf("hello word!n") >> "datafile"}'

      在linux awk的while、do-while和for语句中允许使用break,continue语句来控制流程走向，也允许使用exit这样的语句来退出。break中断当前正在执行的循环并跳到循环外执行下一条语句。if 是流程选择用法。awk中，流程控制语句，语法结构，与c语言类型。每条命令语句后面可以用;分号结尾。

awk 'BEGIN{ 

test=100; 

total=0; 

while(i<=test){ 

total+=i; 

i++; 

} 

print total; 

}'


awk数组

        数组是awk的灵魂，处理文本中最不能少的就是它的数组处理。因为数组索引（下标）可以是数字和字符串在awk中数组叫做关联数组(associative arrays)。awk 中的数组不必提前声明，也不必声明大小。数组元素用0或空字符串来初始化，这根据上下文而定。字符串和数字都可以做数组下标。

{ for(item in array) {print array[item]}; } #输出的顺序是随机的 

{ for(i=1;i<=len;i++) {print array[i]}; } #Len是数组的长度
得到数组长度：awk 'BEGIN{info="it is a test";lens=split(info,tA," ");print length(tA),lens;}'
删除数组：delete array[key]可以删除，对应数组key的，序列值。

awk中有一些字符串的处理函数，使用的话可以查询。

打开外部文件（close用法）：

awk 'BEGIN{while("cat /etc/passwd"|getline){print $0;};close("/etc/passwd");}'

逐行读取外部文件(getline使用方法）：

awk 'BEGIN{while(getline < "/etc/passwd"){print $0;};close("/etc/passwd");}'

awk 'BEGIN{print "Enter your name:";getline name;print name;}'

调用外部应用程序(system使用方法）：

awk 'BEGIN{b=system("ls -al");print b;}'


awk应用例子

单双引号的差别是:shell对单引号中的内容不解释,直接传给awk,而对双引号中的内容解释后再传给awk.

1、 awk '/101/'   file             显示文件file中包含101的匹配行。
      awk '/101/,/105/'  file       显示文件file中包含101到105的匹配行。

      awk '$1 == 5'   file            显示文件file中第一字段值为5的行。

      awk '$1 == "CT"'  file        显示文件file中第一字段值为CT的行。注意必须带双引号
      awk '$1 * $2 >100 '   file    显示文件file中第一字段值和第二字段值乘积大于100的行。

      awk '$2 >5 && $2<=15'   file    显示文件file中第2字段值大于5同时小于15的行。

2、 awk '{print NR,NF,$1,$NF,}' file   显示文件file的当前记录号、域数和每一行的第一个和最后一个域。
      awk '/aa/ {print $1,$2 + 10}' file    显示文件file的匹配行的第一、二个域加10。
      awk '/aa/ {print $1$2}'  file           找出匹配的行，在输出 等同于下面语句
      awk '/aa/ {print $1 $2}' file     显示文件file的匹配行的第一、二个域，但显示时域中间没有分隔符。
3、 df | awk '$4>1000000 '         通过管道符获得输入，如：显示第4个域满足条件的行。
4、 awk -F “|” '{print $1}'   file      按照新的分隔符“|”进行操作。
      awk  'BEGIN { FS="[: \t|]" }
     {print $1,$2,$3}'    file               通过设置输入分隔符（FS="[: \t|]"）修改输入分隔符。

     Sep="|"
     awk -F $Sep '{print $1}'  file          按照环境变量Sep的值做为分隔符。   
     awk -F '[ :\t|]' '{print $1}' file          按照正则表达式的值做为分隔符，这里代表空格、:、TAB、|同时做为分隔符。
     awk -F '[][]'    '{print $1}' file          按照正则表达式的值做为分隔符，这里代表[、]

5、awk -f awkfile              file 通过文件awkfile的内容依次进行控制。
     cat awkfile
     /101/{print "\047 Hello! \047"} --遇到匹配行以后打印 ' Hello! '.\047代表单引号。
     {print $1,$2}                               --因为没有模式控制，打印每一行的前两个域。
6、awk '$1 ~ /101/ {print $1}' file             显示文件中第一个域匹配101的行（记录）。
7、awk   'BEGIN { OFS="%"} {print $1,$2}'    file         通过设置输出分隔符（OFS="%"）修改输出格式。
8、取得文件第一个域的最大值

     awk   'BEGIN { max=100 ;print "max=" max}             BEGIN 表示在处理任意行之前进行的操作。
     {max=($1 >max ?$1:max); print $1,"Now max is "max}' file 
9、  awk '/tom/ {wage=$2+$3; printf wage}' file             找到匹配行后为变量wage赋值并打印该变量。
10、awk '/tom/ {count++;} 
         END {print "tom was found "count" times"}' file             END表示在所有输入行处理完后进行处理。

11、在awk中调用系统变量必须用单引号，如果是双引号，则表示字符串
      Flag=abcd
      awk '{print '$Flag'}'   结果为abcd
      awk '{print  "$Flag"}'   结果为$Flag

12、比如以下这个例子：
  　  a.sh脚本内容
　　CPU_MIN=90
　　cat aa|awk "{print $CPU_MIN,$1}"

　　执行时带个参数：a.sh  1234
　　如果按你的说法应该显示： 90  1234
　　但实际上只显示90，为什么$1的值没有取到，是否应该和awk本身的变量定义有冲突。如果不使用中间变量，这个1234如何传递到awk中？

　　$1是awk的特殊变量，不应该被shell解释。可以这样：
　　cat aa|awk "{print $CPU_MIN,\$1}"

```

### 自己编译过 PHP 吗？如何打开 readline 功能
```
手动编译PHP开发环境
这是一篇来自深夜加班的手稿

问题复盘
你有没有遇到过这样的情况，部署了集成环境，每次添加扩展的时候，总是需要找一堆的配置文件的位置（其实很多人都能熟练使用集成环境）

你有没有遇到过这样的情况，去面试，面试官问你:有没有自己手动编译过环境? 你却回答 我一般都使用集成环境

你有没有遇到过这样的问题，本来自己的服务器配置就很low（有的甚至使用的是vps）基本上使用lnmp或者bt或者其他等等集成环境 但是他们的优缺点十分明显

这可能就是我现在需要手动编译环境的理由吧

部署环境及配置
linux centos 7.3
阿里云香港轻量级应用服务器 34/月
购买推荐链接:https://promotion.aliyun.com/ntms/yunparter/invite.html?userCode=4jm8fecv

目标环境
php 7.2
mysql 5.7.2
nginx 1.1.8

安装部署环境开始
考虑到纯净安装. 所以首先我们需要一台干净的服务器（为了这个 我把博客的全部东西干掉了直接导致服务器停运24小时）

首先我们确定安装目录

我是在根目录部署了www目录直接使用命令

mkdir -p /www/{lnmp,web,source} `
这里创建的 www
/lnmp存放安装软件
/web 存放程序代码
/source 存放安装软件

首先安装PHP
PHP下载网址:
http://am1.php.net/distributions/php-7.2.1.tar.gz
所有的操作全部在 我们自己建立的www/lnmp 下

一、更换yum源
这对我们很重要，因为centos 内部的源一般都是国外的站点，我使用的是阿里云的服务器 所以这里我也是用阿里云的yum源

yum源的地址在

/etc/yum.repos.d/
将下面的默认Centos-Base.repo 进行备份

mv /etc/yum.repos.d/Centos-Base.repo.bak
进入阿里云的镜像地址:http://mirrors.aliyun.com/

在阿里云的镜像地址找到自己对应的版本然后使用wget 进行下载

没有wget的，提前使用yum 安装一个

然后依次执行以下命令

yum clean all
yum makecache
yum update
然后在裸机上面安装必要的扩展


yum -y install wget vim pcre pcre-devel openssl openssl-devel libicu-devel gcc gcc-c++ autoconf libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel zlib zlib-devel glibc glibc-devel glib2 glib2-devel ncurses ncurses-devel curl curl-devel krb5-devel libidn libidn-devel openldap openldap-devel nss_ldap jemalloc-devel cmake boost-devel bison automake libevent libevent-devel gd gd-devel libtool* libmcrypt libmcrypt-devel mcrypt mhash libxslt libxslt-devel readline readline-devel gmp gmp-devel libcurl libcurl-devel openjpeg-devel
上面安装的都是必要的一些扩展

我们还可以使用yum 对系统的内核进行更新

yum install epel-release -y // 此步骤可以省略
安装PHP
首先切换到我们的www/lnmp目录下

然后使用tar -zxvf php-7.2.1.tar.gz 进行解压

解压完毕之后 我们进入 解压过的目录

设置变量并开始源码编译：

cp -frp /usr/lib64/libldap* /usr/lib/
这里我们使用的是自定义的目录所以编译配置也需要改变

./configure --prefix=/www/source/php \
--with-config-file-path=/www/source/php/etc \
--enable-fpm \
--with-fpm-user=www \
--with-fpm-group=www \
--enable-mysqlnd \
--with-mysqli=mysqlnd \
--with-pdo-mysql=mysqlnd \
--enable-mysqlnd-compression-support \
--with-iconv-dir \
--with-freetype-dir \
--with-jpeg-dir \
--with-png-dir \
--with-zlib \
--with-openssl \
--with-libxml-dir \
--enable-xml \
--disable-rpath \
--enable-bcmath \
--enable-shmop \
--enable-sysvsem \
--enable-inline-optimization \
--with-curl \
--enable-mbregex \
--enable-mbstring \
--enable-intl \
--with-libmbfl \
--enable-ftp \
--with-gd \
--enable-gd-jis-conv \
--with-openssl \
--with-mhash \
--enable-pcntl \
--enable-sockets \
--with-xmlrpc \
--enable-zip \
--enable-soap \
--with-gettext \
--disable-fileinfo \
--enable-opcache \
--with-pear \
--enable-maintainer-zts \
--with-ldap=shared \
--without-gdbm
此处重点要考

php7.2 去掉了很多原先php5 - php7.0的扩展 如之前的加密扩展
mcrypt 还有很多 这里我就不一一举例了

编译完毕之后，我们进行下一步

make -j 4 && make install
接下来需要等待大约10-20分钟 这个根据我们的服务器而定 如果觉得慢我们可以使用

make -j 8 && make install
上面两条命令都可以使用

接下来配置php.ini 文件:

cp php.ini-development /www/source/php/ect/php.ini

cp /www/source/php/etc/php-fpm.conf.default /www/source/php/etc/php-fpm.conf

cp /www/source/php/etc/php-fpm.d/www.conf.default /www/source/php/etc/php-fpm.d/www.conf

上述是我的配置文件地址，如果你出现错误 或者找不到文件位置，请按照你自己配置的文件安装目录进行查询
也可以使用 find 查询

配置php.ini
我直接贴出我修改的一些配置 仅供参考 可以根据个人情况进行调整

expose_php = Off
short_open_tag = ON
max_execution_time = 30
date.timezone = Asia/Shanghai
extension= /www/source/php/lib/php/extensions/no-debug-zts-20170718/ldap.so
zend_extension = /www/source/php/lib/php/extensions/no-debug-zts-20170718/opcache.so
opcache.memory_consumption=128
opcache.interned_strings_buffer=8
opcache.max_accelerated_files=4000
opcache.revalidate_freq=60
opcache.fast_shutdown=1
opcache.enable_cli=1
disable_functions = passthru,exec,system,chroot,chgrp,chown,shell_exec,proc_open,proc_get_status,popen,ini_alter,ini_restore,dl,openlog,syslog,readlink,symlink,popepassthru
配置www.conf (此处为配置php-fpm 可略)
到了这一步的时候 我们需要建立对应的用户www

groupadd www
useradd -g www www
然后创建存放 php-cgi.sock 的目录

mkdir /var/run/www/
chown -R www:www /var/run/www
然后配置www.conf文件

 vim /www/source/php/etc/php-fpm.d/www.conf
配置详解

slowlog = var/log/slow.log
listen = /var/run/www/php-cgi.sock
其他的都不需要改变

然后配置php-fpm.conf

pid = /usr/local/php/var/run/php-fpm.pid
至此php7已经安装完成。说明：禁用php函数，如果程序需要这些函数，可以取消禁止，新手建议忽略此步骤。

创建system系统单元文件php-fpm启动脚本：
vim /usr/lib/systemd/system/php-fpm.service
直接 编写

[Unit]
Description=The PHP FastCGI Process Manager
After=syslog.target network.target

[Service]
Type=simple
PIDFile=/usr/local/php/var/run/php-fpm.pid
ExecStart=/www/source/php/bin/php-fpm --nodaemonize --fpm-config /www/source/php/etc/php-fpm.conf
ExecReload=/bin/kill -USR2 $MAINPID

[Install]
WantedBy=multi-user.target
将php-fpm服务启动并加入开机自启动

systemctl enable php-fpm.service
systemctl restart php-fpm.service
配置全局变量
接下来我们就已经配置完毕了试一试php-v

是不是突然报错 ， 没有php 不要慌

这是因为我们没有把PHP 加入全局变量的原因

vim /etc/profile
// 最后一行加入
PATH=$PAHT:/www/source/php/bin
// 保存退出
 source /etc/profile
再次执行 php -v
我们会显示

PHP 7.2.1 (cli) (built: Jul 26 2019 10:50:53) ( ZTS )
Copyright (c) 1997-2017 The PHP Group
Zend Engine v3.2.0, Copyright (c) 1998-2017 Zend Technologies
with Zend OPcache v7.2.1, Copyright (c) 1999-2017, by Zend Technologies
安装mysql
实现我们需要安装mysql 安装的一些依赖 这个大多都是使用yum安装 可以直接百度

// 我这边直接使用

yum install -y gcc gcc-c++ make sudo autoconf libtool-ltdl-devel gd-devel \
        freetype-devel libxml2-devel libjpeg-devel libpng-devel \
        openssl-devel curl-devel patch libmcrypt-devel \
        libmhash-devel ncurses-devel bzip2 \
        libcap-devel ntp sysklogd diffutils sendmail iptables unzip cmake
接下来创建mysql的用户组和用户

groupadd mysql
useradd -r mysql -g mysql
下载必要的包
安装mysql5.6的时候就已经使用cmake了 我们这里使用mysql 5.7
还需要安装一个boost的包


wget https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.20.tar.gz

wget --no-check-certificate http://sourceforge.net/projects/boost/files/boost/1.59.0/boost_1_59_0.tar.gz

然后解压mysql 5.7
进入mysql5.7目录

然后将 我们下载的 boots 包 移动到 解压完毕的mysql文件夹内部

然后我们新建一个 configure 文件夹 接下来我们将在该文件夹内 完成整套编译工作

然后贴出我的配置

cmake .. -DBUILD_CONFIG=mysql_release \
-DINSTALL_LAYOUT=STANDALONE \
-DCMAKE_BUILD_TYPE=RelWithDebInfo \
-DENABLE_DTRACE=OFF \
-DWITH_EMBEDDED_SERVER=OFF \
-DWITH_INNODB_MEMCACHED=ON \
-DWITH_SSL=bundled \
-DWITH_ZLIB=system \
-DCMAKE_INSTALL_PREFIX=/www/source/mysql/ \
-DINSTALL_PLUGINDIR="/www/source/mysql/lib/plugin" \
-DDEFAULT_CHARSET=utf8 \
-DDEFAULT_COLLATION=utf8_general_ci \
-DWITH_EDITLINE=bundled \
-DFEATURE_SET=community \
-DCOMPILATION_COMMENT="MySQL Server (GPL)" \
-DWITH_DEBUG=OFF \
-DWITH_BOOST=..
关于这些配置命令的名称 网络上都有介绍 我就不过多的讲述了

如果出现编译失败 那么 一定要删除当前目录下的
rm -rf CMakeCache.txt 文件

当我们编译完成之后 执行

make && make install
接下来就是耐心等待20分钟左右

初始化数据库
新建数据库文件夹以及日志文件夹，并更改用户为mysql:
mkdir /www/source/mysql/mysql_data
mkdir /www/source/mysql/log
chown -R mysql:mysql /www/source/mysql/mysql_data
chown -R mysql:mysql /www/source/mysql/log
// 在日志文件中建立 error.log 文件
// 赋权限整个目录及其文件夹
chmod -R 777 /www/source/mysql/log
修改配置文件
# vim /etc/my.cnf
[mysqld]
port=3306
datadir=/www/source/mysql/mysql_data
log_error=/www/source/mysql/log/error.log
basedir=/www/source/mysql/
// 注意：my.cnf文件有以下配置
socket=/www/source/mysql/mysql_data/mysql.sock
// 需要手动建立mysql.sock,并赋值读写执行权限
chmod -R 777 mysql.sock

初始化数据库
/var/mysql/bin/mysqld  --initialize --user=mysql
去查看刚刚创建的文件夹内 是否存在生成的文件

配置启动文件及环境变更
配置启动文件
cp /var/mysql/support-files/mysql.server /etc/init.d/mysqld
修改启动文件
vim /etc/init.d/mysqld

# 找到如下二行：
basedir=
datadir=
# 修改为：
basedir=/www/source/mysql/
datadir=/www/source/mysql/mysql_data/
启动mysql

/etc/init.d/mysqld start
可以看到提示，已经成功启动。当然你也可以使用systemctl来启动MySQL，但执行后，不会有任何提示。

systemctl start mysqld
然后我们将mysql 加入全局变量

这次我试了很多方式 但是没有成功 所以 我直接建立了一个软链

当我们执行某命令的时候 服务器首先去看/user/bin这个文件夹

ln -s /www/source/mysql/bin /usr/bin
修改mysql 密码
从mysql5.7开始 mysql不在支持空密码登录 所以我们直接查看密码

cat /www/source/mysql/log/error.log |grep 'A temporary password'
2019-07-13T06:28:23.096812Z 1 [Note] A temporary password is generated for root@localhost: wa&sk371_,US
后面的就是我们的mysql 密码了

然后我们直接使用mysql登录进去

mysql> alter user 'root'@'localhost' identified by 'your_password';
ok！！！！

安装nginx
准备工作
安装nginx 的时候 我们首先还需要安装3个依赖包

pcre：在使用 nginx 的 rewrite 模块的时候，需要有pcre库的支持
openssl：在使用ssl功能时，需要有 openssl库的支持
zlib：在使用gzip模块时，需要有zlib库的支持。
而这三个模块都是我们常用的，所以这3个依赖包还是要安装的。

1、安装pcre
首先下载这个包

wget https://jaist.dl.sourceforge.net/project/pcre/pcre/8.36/pcre-8.36.tar.gz
tar -zxvf pcre-8.36.tar.gz
cd pcre-8.36
./configure
make && make install
2、安装 openssl
wget https://www.openssl.org/source/openssl-1.1.0k.tar.gz
tar -zxvf openssl-1.1.0k.tar.gz
cd openssl-1.1.0k
./config
make && make install
3、安装zlib
wget https://zlib.net/zlib-1.2.11.tar.gz
tar -zxvf zlib-1.2.11.tar.gz
cd zlib-1.2.11
CFLAGS="-O3 -fPIC" ./configure
make && make install
编译nginx
因为nginx的命令有很多 我们可以使用

./configure --help
这里我推荐使用

./configure --user=www --group=www --prefix=/www/source/nginx --with-pcre=/www/lnmp/pcre-8.36 --with-zlib=/www/lnmp/zlib-1.2.11 --with-openssl=/www/lnmp/openssl-1.1.0k
上面的命令是和我们下载的pcre以及openssl and zlib要关联上的 具体看你的配置

执行完毕之后 我们执行

make && make install
// 然后
ll /www/source/
下面存在nginx 说明我们安装成功

然后我们使用

cd /www/source/nginx/conf

到这里的时候我们基本上已经安装完毕

本次手动编译nginx+php+mysql 就到这里

关于nginx的配置 和 php的问题

请继续关注我的博客

学习不是一件幸福的事情 只是面对这繁华人生 我们更希望能拥有一技之长
```

### 如何查看 PHP 进程的内存、CPU 占用
```
1. 查看内存使用情况
观察你程序的内存使用能够让你更好的优化你的代码。
PHP 是有垃圾回收机制的，而且有一套很复杂的内存管理机制。你可以知道你的脚本所使用的内存情况。要知道当前内存使用情况，你可以使用?memory_get_usage() 函数，如果你想知道使用内存的峰值，你可以调用memory_get_peak_usage() 函数。

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

echo "Initial: ".memory_get_usage()." bytes \n";

/* 输出

Initial: 361400 bytes

*/

// 使用内存

for ($i = 0; $i < 100000; $i++) {

$array []= md5($i);

}

// 删除一半的内存

for ($i = 0; $i < 100000; $i++) {

unset($array[$i]);

}

echo "Final: ".memory_get_usage()." bytes \n";

/* prints

Final: 885912 bytes

*/

echo "Peak: ".memory_get_peak_usage()." bytes \n";

/* 输出峰值

Peak: 13687072 bytes

*/

2. 查看CPU使用情况
使用?getrusage() 函数可以让你知道CPU的使用情况。注意，这个功能在Windows下不可用。

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

print_r(getrusage());

/* 输出

Array

(

[ru_oublock] => 0

[ru_inblock] => 0

[ru_msgsnd] => 2

[ru_msgrcv] => 3

[ru_maxrss] => 12692

[ru_ixrss] => 764

[ru_idrss] => 3864

[ru_minflt] => 94

[ru_majflt] => 0

[ru_nsignals] => 1

[ru_nvcsw] => 67

[ru_nivcsw] => 4

[ru_nswap] => 0

[ru_utime.tv_usec] => 0

[ru_utime.tv_sec] => 0

[ru_stime.tv_usec] => 6269

[ru_stime.tv_sec] => 0

)

*/

这个结构看上出很晦涩，除非你对CPU很了解。下面一些解释：

ru_oublock: 块输出操作
ru_inblock: 块输入操作
ru_msgsnd: 发送的message
ru_msgrcv: 收到的message
ru_maxrss: 最大驻留集大小
ru_ixrss: 全部共享内存大小
ru_idrss:全部非共享内存大小
ru_minflt: 页回收
ru_majflt: 页失效
ru_nsignals: 收到的信号
ru_nvcsw: 主动上下文切换
ru_nivcsw: 被动上下文切换
ru_nswap: 交换区
ru_utime.tv_usec: 用户态时间 (microseconds)
ru_utime.tv_sec: 用户态时间(seconds)
ru_stime.tv_usec: 系统内核时间 (microseconds)
ru_stime.tv_sec: 系统内核时间?(seconds)

```

### 如何给 PHP 增加一个扩展
```
PHP取得成功的一个主要原因之一是她拥有大量的可用扩展。web开发者无论有何种需求，这种需求最有可能在PHP发行包里找到。PHP发行包包括支持各种数据库，图形文件格式，压缩，XML技术扩展在内的许多扩展。
扩展API的引入使PHP3取得了巨大的进展，扩展API机制使PHP开发社区很容易的开发出几十种扩展。现在，两个版本过去了，API仍然和PHP3时的非常相似。扩展主要的思想是：尽可能的从扩展编写者那里隐藏PHP的内部机制和脚本引擎本身，仅仅需要开发者熟悉API。

有两个理由需要自己编写PHP扩展。第一个理由是：PHP需要支持一项她还未支持的技术。这通常包括包裹一些现成的C函数库，以便提供PHP接口。例如，如果一个叫FooBase的数据库已推出市场，你需要建立一个PHP扩展帮助你从PHP里调用FooBase的C函数库。这个工作可能仅由一个人完成，然后被整个PHP社区共享（如果你愿意的话）。第二个不是很普遍的理由是：你需要从性能或功能的原因考虑来编写一些商业逻辑。

如果以上的两个理由都和你没什么关系，同时你感觉自己没有冒险精神，那么你可以跳过本章。

本章教你如何编写相对简单的PHP扩展，使用一部分扩展API函数。对于大多数打算开发自定义PHP扩展开发者而言，它含概了足够的资料。学习一门编程课程的最好方法之一就是动手做一些极其简单的例子，这些例子正是本章的线索。一旦你明白了基础的东西，你就可以在互联网上通过阅读文挡、原代码或参加邮件列表新闻组讨论来丰富自己。因此，本章集中在让你如何开始的话题。在UNIX下一个叫ext_skel的脚本被用于建立扩展的骨架，骨架信息从一个描述扩展接口的定义文件中取得。因此你需要利用UNIX来建立一个骨架。Windows开发者可以使用Windows ext_skel_win32.php代替ext_skel。

然而，本章关于用你开发的扩展编译PHP的指导仅涉及UNIX编译系统。本章中所有的对API的解释与UNIX和Windows下开发的扩展都有联系。

当你阅读完这章，你能学会如何

•建立一个简单的商业逻辑扩展。
•建议个C函数库的包裹扩展，尤其是有些标准C文件操作函数比如fopen()
快速开始
本节没有介绍关于脚本引擎基本构造的一些知识，而是直接进入扩展的编码讲解中，因此不要担心你无法立刻获得对扩展整体把握的感觉。假设你正在开发一个网站，需要一个把字符串重复n次的函数。下面是用PHP写的例子：

复制代码代码如下:

function self_concat($string, $n){
$result = "";
for($i = 0; $i < $n; $i++){
$result .= $string;
}
return $result;
}
self_concat("One", 3) returns "OneOneOne".
self_concat("One", 1) returns "One".


假设由于一些奇怪的原因，你需要时常调用这个函数，而且还要传给函数很长的字符串和大值n。这意味着在脚本里有相当巨大的字符串连接量和内存重新分配过程，以至显著地降低脚本执行速度。如果有一个函数能够更快地分配大量且足够的内存来存放结果字符串，然后把$string重复n次，就不需要在每次循环迭代中分配内存。

为扩展建立函数的第一步是写一个函数定义文件，该函数定义文件定义了扩展对外提供的函数原形。该例中，定义函数只有一行函数原形self_concat() :

复制代码代码如下:

string self_concat(string str, int n)


函数定义文件的一般格式是一个函数一行。你可以定义可选参数和使用大量的PHP类型，包括: bool, float, int, array等。

保存为myfunctions.def文件至PHP原代码目录树下。

该是通过扩展骨架(skeleton)构造器运行函数定义文件的时机了。该构造器脚本叫ext_skel，放在PHP原代码目录树的ext/目录下（PHP原码主目录下的README.EXT_SKEL提供了更多的信息）。假设你把函数定义保存在一个叫做myfunctions.def的文件里，而且你希望把扩展取名为myfunctions，运行下面的命令来建立扩展骨架

复制代码代码如下:

./ext_skel --extname=myfunctions --proto=myfunctions.de


这个命令在ext/目录下建立了一个myfunctions/目录。你要做的第一件事情也许就是编译该骨架，以便编写和测试实际的C代码。编译扩展有两种方法：

•作为一个可装载模块或者DSO（动态共享对象）
•静态编译到PHP

PHP扩展开发导图

因为第二种方法比较容易上手，所以本章采用静态编译。如果你对编译可装载扩展模块感兴趣，可以阅读PHP原代码根目录下的README.SELF-CONTAINED_EXTENSIONS文件。为了使扩展能够被编译，需要修改扩展目录ext/myfunctions/下的config.m4文件。扩展没有包裹任何外部的C库，你需要添加支持–enable-myfunctions配置开关到PHP编译系统里（–with-extension 开关用于那些需要用户指定相关C库路径的扩展）。可以去掉自动生成的下面两行的注释来开启这个配置。

复制代码代码如下:

./ext_skel --extname=myfunctions --proto=myfunctions.def
PHP_ARG_ENABLE(myfunctions, whether to enable myfunctions support,
[ --enable-myfunctions Include myfunctions support]


现在剩下的事情就是在PHP原代码树根目录下运行./buildconf，该命令会生成一个新的配置脚本。通过查看./configure –help输出信息，可以检查新的配置选项是否被包含到配置文件中。现在，打开你喜好的配置选项开关和–enable-myfunctions重新配置一下PHP。最后的但不是最次要的是，用make来重新编译PHP。

ext_skel应该把两个PHP函数添加到你的扩展骨架了：打算实现的self_concat()函数和用于检测myfunctions 是否编译到PHP的confirm_myfunctions_compiled()函数。完成PHP的扩展开发后，可以把后者去掉。

复制代码代码如下:

<?php
print confirm_myfunctions_compiled("myextension");
?>

运行这个脚本会出现类似下面的输出：
复制代码代码如下:

"Congratulations! You have successfully modified ext/myfunctions
config.m4. Module myfunctions is now compiled into PHP.

另外，ext_skel脚本生成一个叫myfunctions.php的脚本，你也可以利用它来验证扩展是否被成功地编译到PHP。它会列出该扩展所支持的所有函数。
现在你学会如何编译扩展了，该是真正地研究self_concat()函数的时候了。
下面就是ext_skel脚本生成的骨架结构：
复制代码代码如下:

/* {{{ proto string self_concat(string str, int n)
*/
PHP_FUNCTION(self_concat)
{
char *str = NULL;
int argc = ZEND_NUM_ARGS();
int str_len;
long n;
if (zend_parse_parameters(argc TSRMLS_CC, "sl", &str, &str_len, &n) == FAILURE)
return;
php_error(E_WARNING, "self_concat: not yet implemented");
}
/* }}} */


自动生成的PHP函数周围包含了一些注释，这些注释用于自动生成代码文档和vi、Emacs等编辑器的代码折叠。函数自身的定义使用了宏PHP_FUNCTION()，该宏可以生成一个适合于Zend引擎的函数原型。逻辑本身分成语义各部分，取得调用函数的参数和逻辑本身。

为了获得函数传递的参数，可以使用zend_parse_parameters()API函数。下面是该函数的原型：
复制代码代码如下:

zend_parse_parameters(int num_args TSRMLS_DC, char *type_spec, …);

第一个参数是传递给函数的参数个数。通常的做法是传给它ZEND_NUM_ARGS()。这是一个表示传递给函数参数总个数的宏。第二个参数是为了线程安全，总是传递TSRMLS_CC宏，后面会讲到。第三个参数是一个字符串，指定了函数期望的参数类型，后面紧跟着需要随参数值更新的变量列表。因为PHP采用松散的变量定义和动态的类型判断，这样做就使得把不同类型的参数转化为期望的类型成为可能。例如，如果用户传递一个整数变量，可函数需要一个浮点数，那么zend_parse_parameters()就会自动地把整数转换为相应的浮点数。如果实际值无法转换成期望类型（比如整形到数组形），会触发一个警告。

下表列出了可能指定的类型。我们从完整性考虑也列出了一些没有讨论到的类型。

类型指定符	对应的C类型	描述
l	long	符号整数
d	double	浮点数
s	char *, int	二进制字符串，长度
b	zend_bool	逻辑型（1或0）
r	zval *	资源（文件指针，数据库连接等）
a	zval *	联合数组
o	zval *	任何类型的对象
O	zval *	指定类型的对象。需要提供目标对象的类类型
z	zval *	无任何操作的zval

为了容易地理解最后几个选项的含义，你需要知道zval是Zend引擎的值容器[1]。无论这个变量是布尔型，字符串型或者其他任何类型，其信息总会包含在一个zval联合体中。本章中我们不直接存取zval，而是通过一些附加的宏来操作。下面的是或多或少在C中的zval, 以便我们能更好地理解接下来的代码。

复制代码代码如下:

typedef union _zval{
long lval;
double dval;
struct {
char *val;
int len;
}str;
HashTable *ht;
zend_object_value obj;
}zval;


在我们的例子中，我们用基本类型调用zend_parse_parameters()，以本地C类型的方式取得函数参数的值，而不是用zval容器。

为了让zend_parse_parameters()能够改变传递给它的参数的值，并返回这个改变值，需要传递一个引用。仔细查看一下self_concat()：

复制代码代码如下:

if (zend_parse_parameters(argc TSRMLS_CC, "sl", &str, &str_len, &n) == FAILURE)return;


注意到自动生成的代码会检测函数的返回值FAILUER(成功即SUCCESS)来判断是否成功。如果没有成功则立即返回，并且由zend_parse_parameters()负责触发警告信息。因为函数打算接收一个字符串l和一个整数n，所以指定 ”sl” 作为其类型指示符。s需要两个参数，所以我们传递参考char * 和 int (str 和 str_len)给zend_parse_parameters()函数。无论什么时候，记得总是在代码中使用字符串长度str_len来确保函数工作在二进制安全的环境中。不要使用strlen()和strcpy()，除非你不介意函数在二进制字符串下不能工作。二进制字符串是包含有nulls的字符串。二进制格式包括图象文件，压缩文件，可执行文件和更多的其他文件。”l” 只需要一个参数，所以我们传递给它n的引用。尽管为了清晰起见，骨架脚本生成的C变量名与在函数原型定义文件中的参数名一样；这样做不是必须的，尽管在实践中鼓励这样做。

回到转换规则中来。下面三个对self_concat()函数的调用使str, str_len和n得到同样的值：

复制代码代码如下:

self_concat("321", 5);
self_concat(321, "5");
self_concat("321", "5");
str points to the string "321", str_len equals 3, and n equals 5.
str 指向字符串"321"，str_len等于3，n等于5


在我们编写代码来实现连接字符串返回给PHP的函数前，还得谈谈两个重要的话题：内存管理、从PHP内部返回函数值所使用的API。

内存管理

用于从堆中分配内存的PHP API几乎和标准C API一样。在编写扩展的时候，使用下面与C对应（因此不必再解释）的API函数：
复制代码代码如下:

emalloc(size_t size);
efree(void *ptr);
ecalloc(size_t nmemb, size_t size);
erealloc(void *ptr, size_t size);
estrdup(const char *s);
estrndup(const char *s, unsigned int length);

在这一点上，任何一位有经验的C程序员应该象这样思考一下：“什么？标准C没有strndup()？”是的，这是正确的，因为GNU扩展通常在Linux下可用。estrndup()只是PHP下的一个特殊函数。它的行为与estrdup()相似，但是可以指定字符串重复的次数（不需要结束空字符），同时是二进制安全的。这是推荐使用estrndup()而不是estrdup()的原因。

在几乎所有的情况下，你应该使用这些内存分配函数。有一些情况，即扩展需要分配在请求中永久存在的内存，从而不得不使用malloc()，但是除非你知道你在做什么，你应该始终使用以上的函数。如果没有使用这些内存函数，而相反使用标准C函数分配的内存返回给脚本引擎，那么PHP会崩溃。

这些函数的优点是：任何分配的内存在偶然情况下如果没有被释放，则会在页面请求的最后被释放。因此，真正的内存泄漏不会产生。然而，不要依赖这一机制，从调试和性能两个原因来考虑，应当确保释放应该释放的内存。剩下的优点是在多线程环境下性能的提高，调试模式下检测内存错误等。

还有一个重要的原因，你不需要检查这些内存分配函数的返回值是否为null。当内存分配失败，它们会发出E_ERROR错误，从而决不会返回到扩展。

从PHP函数中返回值

扩展API包含丰富的用于从函数中返回值的宏。这些宏有两种主要风格：第一种是RETVAL_type()形式，它设置了返回值但C代码继续执行。这通常使用在把控制交给脚本引擎前还希望做的一些清理工作的时候使用，然后再使用C的返回声明 ”return” 返回到PHP；后一个宏更加普遍，其形式是RETURN_type()，他设置了返回类型，同时返回控制到PHP。下表解释了大多数存在的宏。

设置返回值并且结束函数	设置返回值	宏返回类型和参数
RETURN_LONG(l)	RETVAL_LONG(l)	整数
RETURN_BOOL(b)	RETVAL_BOOL(b)	布尔数(1或0)
RETURN_NULL()	RETVAL_NULL()	NULL
RETURN_DOUBLE(d)	RETVAL_DOUBLE(d)	浮点数
RETURN_STRING(s, dup)	RETVAL_STRING(s, dup)	字符串。如果dup为1，引擎会调用estrdup()重复s，使用拷贝。如果dup为0，就使用s
RETURN_STRINGL(s, l, dup)	RETVAL_STRINGL(s, l, dup)	长度为l的字符串值。与上一个宏一样，但因为s的长度被指定，所以速度更快。
RETURN_TRUE	RETVAL_TRUE	返回布尔值true。注意到这个宏没有括号。
RETURN_FALSE	RETVAL_FALSE	返回布尔值false。注意到这个宏没有括号。
RETURN_RESOURCE(r)	RETVAL_RESOURCE(r)	资源句柄。

完成self_concat()

现在你已经学会了如何分配内存和从PHP扩展函数里返回函数值，那么我们就能够完成self_concat()的编码：
复制代码代码如下:

/* {{{ proto string self_concat(string str, int n)
*/
PHP_FUNCTION(self_concat)
}
char *str = NULL;
int argc = ZEND_NUM_ARGS();
int str_len;
long n;
char *result; /* Points to resulting string */
char *ptr; /* Points at the next location we want to copy to */
int result_length; /* Length of resulting string */
if (zend_parse_parameters(argc TSRMLS_CC, "sl", &str, &str_len, &n) == FAILURE)
return;
/* Calculate length of result */
result_length = (str_len * n);
/* Allocate memory for result */
result = (char *) emalloc(result_length + 1);
/* Point at the beginning of the result */
ptr = result;
while (n--) {
/* Copy str to the result */
memcpy(ptr, str, str_len);
/* Increment ptr to point at the next position we want to write to */
ptr += str_len;
}
/* Null terminate the result. Always null-terminate your strings
even if they are binary strings */
*ptr = '\0';
/* Return result to the scripting engine without duplicating it*/
RETURN_STRINGL(result, result_length, 0);
}
/* }}} */

现在要做的就是重新编译一下PHP，这样就完成了第一个PHP函数。

让我门检查函数是否真的工作。在最新编译过的PHP树下执行[2]下面的脚本：
复制代码代码如下:

<?php
for ($i = 1; $i <= 3; $i++){
print self_concat("ThisIsUseless", $i);
print "\n";
}
?>

你应该得到下面的结果：
复制代码代码如下:

ThisIsUseless
ThisIsUselessThisIsUseless
ThisIsUselessThisIsUselessThisIsUseles

实例小结
你已经学会如何编写一个简单的PHP函数。回到本章的开头，我们提到用C编写PHP功能函数的两个主要的动机。第一个动机是用C实现一些算法来提高性能和扩展功能。前一个例子应该能够指导你快速上手这种类型扩展的开发。第二个动机是包裹三方函数库。我们将在下一步讨论。

包裹第三方的扩展
本节中你将学到如何编写更有用和更完善的扩展。该节的扩展包裹了一个C库，展示了如何编写一个含有多个互相依赖的PHP函数扩展。

动机
也许最常见的PHP扩展是那些包裹第三方C库的扩展。这些扩展包括MySQL或Oracle的数据库服务库，libxml2的 XML技术库，ImageMagick 或GD的图形操纵库。

在本节中，我们编写一个扩展，同样使用脚本来生成骨架扩展，因为这能节省许多工作量。这个扩展包裹了标准C函数fopen(), fclose(), fread(), fwrite()和 feof().

扩展使用一个被叫做资源的抽象数据类型，用于代表已打开的文件FILE*。你会注意到大多数处理比如数据库连接、文件句柄等的PHP扩展使用了资源类型，这是因为引擎自己无法直接“理解”它们。我们计划在PHP扩展中实现的C API列表如下：

复制代码代码如下:

FILE *fopen(const char *path, const char *mode);
int fclose(FILE *stream);
size_t fread(void *ptr, size_t size, size_t nmemb, FILE *stream);
size_t fwrite(const void *ptr, size_t size, size_t nmemb, FILE *stream);
int feof(FILE *stream);


我们实现这些函数，使它们在命名习惯和简单性上符合PHP脚本。如果你曾经向PHP社区贡献过代码，你被期望遵循一些公共习俗，而不是跟随C库里的API。并不是所有的习俗都写在PHP代码树的CODING_STANDARDS文件里。这即是说，此功能已经从PHP发展的很早阶段即被包含在PHP中，并且与C库API类似。PHP安装已经支持fopen(), fclose()和更多的PHP函数。
以下是PHP风格的API：

复制代码代码如下:

resource file_open(string filename, string mode)
file_open() //接收两个字符串（文件名和模式），返回一个文件的资源句柄。
bool file_close(resource filehandle)
file_close() //接收一个资源句柄，返回真/假指示是否操作成功。
string file_read(resource filehandle, int size)
file_read() //接收一个资源句柄和读入的总字节数，返回读入的字符串。
bool file_write(resource filehandle, string buffer)
file_write() //接收一个资源句柄和被写入的字符串，返回真/假指示是否操作成功。
bool file_eof(resource filehandle)
file_eof() //接收一个资源句柄，返回真/假指示是否到达文件的尾部。


因此，我们的函数定义文件——保存为ext/目录下的myfile.def——内容如下：

复制代码代码如下:

resource file_open(string filename, string mode)

bool file_close(resource filehandle)

string file_read(resource filehandle, int size)

bool file_write(resource filehandle, string buffer)

bool file_eof(resource filehandle)


下一步，利用ext_skel脚本在ext./ 原代码目录执行下面的命令：
复制代码代码如下:

./ext_skel --extname=myfile --proto=myfile.de

然后，按照前一个例子的关于编译新建立脚本的步骤操作。你会得到一些包含FETCH_RESOURCE()宏行的编译错误，这样骨架脚本就无法顺利完成编译。为了让骨架扩展顺利通过编译，把那些出错行[3]注释掉即可。

资源
资源是一个能容纳任何信息的抽象数据结构。正如前面提到的，这个信息通常包括例如文件句柄、数据库连接结构和其他一些复杂类型的数据。

使用资源的主要原因是因为：资源被一个集中的队列所管理，该队列可以在PHP开发人员没有在脚本里面显式地释放时可以自动地被释放。

举个例子，考虑到编写一个脚本，在脚本里调用mysql_connect()打开一个MySQL连接，可是当该数据库连接资源不再使用时却没有调用mysql_close()。在PHP里，资源机制能够检测什么时候这个资源应当被释放，然后在当前请求的结尾或通常情况下更早地释放资源。这就为减少内存泄漏赋予了一个“防弹”机制。如果没有这样一个机制，经过几次web请求后，web服务器也许会潜在地泄漏许多内存资源，从而导致服务器当机或出错。

注册资源类型
如何使用资源？Zend引擎让使用资源变地非常容易。你要做的第一件事就是把资源注册到引擎中去。使用这个API函数：

int zend_register_list_destructors_ex(rsrc_dtor_func_t ld, rsrc_dtor_func_t pld, char *type_name, int module_number)

这个函数返回一个资源类型id，该id应当被作为全局变量保存在扩展里，以便在必要的时候传递给其他资源API。ld：该资源释放时调用的函数。pld用于在不同请求中始终存在的永久资源，本章不会涉及。type_name是一个具有描述性类型名称的字符串，module_number为引擎内部使用，当我们调用这个函数时，我们只需要传递一个已经定义好的module_number变量。

回到我们的例子中来：我们会添加下面的代码到myfile.c原文件中。该文件包括了资源释放函数的定义，此资源函数被传递给zend_register_list_destructors_ex()注册函数（资源释放函数应该提早添加到文件中，以便在调用zend_register_list_destructors_ex()时该函数已被定义）：

复制代码代码如下:

static void myfile_dtor(zend_rsrc_list_entry *rsrc TSRMLS_DC){
FILE *fp = (FILE *) rsrc->ptr;
fclose(fp);
}

把注册行添加到PHP_MINIT_FUNCTION()后，看起来应该如下面的代码：

复制代码代码如下:

PHP_MINIT_FUNCTION(myfile){
/* If you have INI entries, uncomment these lines
ZEND_INIT_MODULE_GLOBALS(myfile, php_myfile_init_globals,NULL);

REGISTER_INI_ENTRIES();
*/

le_myfile = zend_register_list_destructors_ex(myfile_dtor,NULL,"standard-c-file", module_number);

return SUCCESS;
}

l 注意到le_myfile是一个已经被ext_skel脚本定义好的全局变量。

PHP_MINIT_FUNCTION()是一个先于模块（扩展）的启动函数，是暴露给扩展的一部分API。下表提供可用函数简要的说明。

函数声明宏	语义
PHP_MINIT_FUNCTION()	当PHP被装载时，模块启动函数即被引擎调用。这使得引擎做一些例如资源类型，注册INI变量等的一次初始化。
PHP_MSHUTDOWN_FUNCTION()	当PHP完全关闭时，模块关闭函数即被引擎调用。通常用于注销INI条目
PHP_RINIT_FUNCTION()	在每次PHP请求开始，请求前启动函数被调用。通常用于管理请求前逻辑。
PHP_RSHUTDOWN_FUNCTION()	在每次PHP请求结束后，请求前关闭函数被调用。经常应用在清理请求前启动函数的逻辑。
PHP_MINFO_FUNCTION()	调用phpinfo()时模块信息函数被呼叫，从而打印出模块信息。
新建和注册新资源 我们准备实现file_open()函数。当我们打开文件得到一个FILE *，我们需要利用资源机制注册它。下面的主要宏实现注册功能：

复制代码代码如下:

ZEND_REGISTER_RESOURCE(rsrc_result, rsrc_pointer, rsrc_type);

参考表格对宏参数的解释

ZEND_REGISTER_RESOURCE 宏参数

宏参数	参数类型
rsrc_result	zval *, which should be set with the registered resource information. zval * 设置为已注册资源信息
rsrc_pointer	Pointer to our resource data. 资源数据指针
rsrc_type	The resource id obtained when registering the resource type. 注册资源类型时获得的资源id

文件函数
现在你知道了如何使用ZEND_REGISTER_RESOURCE()宏，并且准备好了开始编写file_open()函数。还有一个主题我们需要讲述。

当PHP运行在多线程服务器上，不能使用标准的C文件存取函数。这是因为在一个线程里正在运行的PHP脚本会改变当前工作目录，因此另外一个线程里的脚本使用相对路径则无法打开目标文件。为了阻止这种错误发生，PHP框架提供了称作VCWD （virtual current working directory 虚拟当前工作目录）宏，用来代替任何依赖当前工作目录的存取函数。这些宏与被替代的函数具备同样的功能，同时是被透明地处理。在某些没有标准C函数库平台的情况下，VCWD框架则不会得到支持。例如，Win32下不存在chown()，就不会有相应的VCWD_CHOWN()宏被定义。

VCWD列表
标准C库	VCWD宏
getcwd()	VCWD_GETCWD()
fopen()	VCWD_FOPEN
open()	VCWD_OPEN() //用于两个参数的版本
open()	VCWD_OPEN_MODE() //用于三个参数的open()版本
creat()	VCWD_CREAT()
chdir()	VCWD_CHDIR()
getwd()	VCWD_GETWD()
realpath()	VCWD_REALPATH()
rename()	VCWD_RENAME()
stat()	VCWD_STAT()
lstat()	VCWD_LSTAT()
unlink()	VCWD_UNLINK()
mkdir()	VCWD_MKDIR()
rmdir()	VCWD_RMDIR()
opendir()	VCWD_OPENDIR()
popen()	VCWD_POPEN()
access()	VCWD_ACCESS()
utime()	VCWD_UTIME()
chmod()	VCWD_CHMOD()
chown()	VCWD_CHOWN()

编写利用资源的第一个PHP函数
实现file_open()应该非常简单，看起来像下面的样子：

复制代码代码如下:

PHP_FUNCTION(file_open){
char *filename = NULL;
char *mode = NULL;
int argc = ZEND_NUM_ARGS();
int filename_len;
int mode_len;
FILE *fp;
if (zend_parse_parameters(argc TSRMLS_CC, "ss", &filename,&filename_len, &mode, &mode_len) == FAILURE) {
return;
}
fp = VCWD_FOPEN(filename, mode);
if (fp == NULL) {
RETURN_FALSE;
}
ZEND_REGISTER_RESOURCE(return_value, fp, le_myfile);
}

你可能会注意到资源注册宏的第一个参数return_value，可此地找不到它的定义。这个变量自动的被扩展框架定义为zval * 类型的函数返回值。先前讨论的、能够影响返回值的RETURN_LONG() 和RETVAL_BOOL()宏确实改变了return_value的值。因此很容易猜到程序注册了我们取得的文件指针fp，同时设置return_value为该注册资源。

访问资源 需要使用下面的宏访问资源（参看表对宏参数的解释）
复制代码代码如下:

ZEND_FETCH_RESOURCE(rsrc, rsrc_type, passed_id, default_id, resource_type_name, resource_type);

ZEND_FETCH_RESOURCE 宏参数
参数	含义
rsrc	资源值保存到的变量名。它应该和资源有相同类型。
rsrc_type	rsrc的类型，用于在内部把资源转换成正确的类型
passed_id	寻找的资源值(例如zval **)
default_id	如果该值不为-1，就使用这个id。用于实现资源的默认值。
resource_type_name	资源的一个简短名称，用于错误信息。
resource_type	注册资源的资源类型id

使用这个宏，我们现在能够实现file_eof()：
复制代码代码如下:

PHP_FUNCTION(file_eof){
int argc = ZEND_NUM_ARGS();
zval *filehandle = NULL;
FILE *fp;
if (zend_parse_parameters(argc TSRMLS_CC, "r", &filehandle) ==FAILURE) {
return;
}
ZEND_FETCH_RESOURCE(fp, FILE *, &filehandle, -1, "standard-c-file",le_myfile);
if (fp == NULL){
RETURN_FALSE;
}
if (feof(fp) <= 0) {
/* Return eof also if there was an error */
RETURN_TRUE;
}
RETURN_FALSE;
}

删除一个资源通常使用下面这个宏删除一个资源：
复制代码代码如下:

int zend_list_delete(int id)

传递给宏一个资源id，返回SUCCESS或者FAILURE。如果资源存在，优先从Zend资源列队中删除，该过程中会调用该资源类型的已注册资源清理函数。因此，在我们的例子中，不必取得文件指针，调用fclose()关闭文件，然后再删除资源。直接把资源删除掉即可。
使用这个宏，我们能够实现file_close()：
复制代码代码如下:

PHP_FUNCTION(file_close){
int argc = ZEND_NUM_ARGS();
zval *filehandle = NULL;
if (zend_parse_parameters(argc TSRMLS_CC, "r", &filehandle) == FAILURE) {
return;
}
if (zend_list_delete(Z_RESVAL_P(filehandle)) == FAILURE) {
RETURN_FALSE;
}
RETURN_TRUE;
}

你肯定会问自己Z_RESVAL_P()是做什么的。当我们使用zend_parse_parameters()从参数列表中取得资源的时候，得到的是zval的形式。为了获得资源id，我们使用Z_RESVAL_P()宏得到id，然后把id传递给zend_list_delete()。
有一系列宏用于访问存储于zval值（参考表的宏列表）。尽管在大多数情况下zend_parse_parameters()返回与c类型相应的值，我们仍希望直接处理zval，包括资源这一情况。

Zval访问宏
宏	访问对象	C 类型
Z_LVAL, Z_LVAL_P, Z_LVAL_PP	整型值	long
Z_BVAL, Z_BVAL_P, Z_BVAL_PP	布尔值	zend_bool
Z_DVAL, Z_DVAL_P, Z_DVAL_PP	浮点值	double
Z_STRVAL, Z_STRVAL_P, Z_STRVAL_PP	字符串值	char *
Z_STRLEN, Z_STRLEN_P, Z_STRLEN_PP	字符串长度值	int
Z_RESVAL, Z_RESVAL_P,Z_RESVAL_PP	资源值	long
Z_ARRVAL, Z_ARRVAL_P, Z_ARRVAL_PP	联合数组	HashTable *
Z_TYPE, Z_TYPE_P, Z_TYPE_PP	Zval类型	Enumeration (IS_NULL, IS_LONG, IS_DOUBLE, IS_STRING, IS_ARRAY, IS_OBJECT, IS_BOOL, IS_RESOURCE)
Z_OBJPROP, Z_OBJPROP_P, Z_OBJPROP_PP	对象属性hash（本章不会谈到）	HashTable *
Z_OBJCE, Z_OBJCE_P, Z_OBJCE_PP	对象的类信息	zend_class_entry
用于访问zval值的宏
所有的宏都有三种形式：一个是接受zval s，另外一个接受zval *s，最后一个接受zval **s。它们的区别是在命名上，第一个没有后缀，zval *有后缀_P（代表一个指针），最后一个 zval **有后缀_PP（代表两个指针）。
现在，你有足够的信息来独立完成 file_read()和 file_write()函数。这里是一个可能的实现：

复制代码代码如下:

PHP_FUNCTION(file_read){
int argc = ZEND_NUM_ARGS();
long size;
zval *filehandle = NULL;
FILE *fp;
char *result;
size_t bytes_read;
if (zend_parse_parameters(argc TSRMLS_CC, "rl", &filehandle,&size) == FAILURE) {
return;
}
ZEND_FETCH_RESOURCE(fp, FILE *, &filehandle, -1, "standard-cfile", le_myfile);
result = (char *) emalloc(size+1);
bytes_read = fread(result, 1, size, fp);
result[bytes_read] = '\0';
RETURN_STRING(result, 0);
}
PHP_FUNCTION(file_write){
char *buffer = NULL;
int argc = ZEND_NUM_ARGS();
int buffer_len;
zval *filehandle = NULL;
FILE *fp;
if (zend_parse_parameters(argc TSRMLS_CC, "rs", &filehandle,&buffer, &buffer_len) == FAILURE) {
return;
}
ZEND_FETCH_RESOURCE(fp, FILE *, &filehandle, -1, "standard-cfile", le_myfile);
if (fwrite(buffer, 1, buffer_len, fp) != buffer_len) {
RETURN_FALSE;
}
RETURN_TRUE;
}

测试扩展
你现在可以编写一个测试脚本来检测扩展是否工作正常。下面是一个示例脚本，该脚本打开文件test.txt，输出文件类容到标准输出，建立一个拷贝test.txt.new。
复制代码代码如下:

<?php
$fp_in = file_open("test.txt", "r") or die("Unable to open input file\n");
$fp_out = file_open("test.txt.new", "w") or die("Unable to open output file\n");
while (!file_eof($fp_in)) {
$str = file_read($fp_in, 1024);
print($str);
file_write($fp_out, $str);
}
file_close($fp_in);
file_close($fp_out);
?>

全局变量
你可能希望在扩展里使用全局C变量，无论是独自在内部使用或访问php.ini文件中的INI扩展注册标记（INI在下一节中讨论）。因为PHP是为多线程环境而设计，所以不必定义全局变量。PHP提供了一个创建全局变量的机制，可以同时应用在线程和非线程环境中。我们应当始终利用这个机制，而不要自主地定义全局变量。用一个宏访问这些全局变量，使用起来就像普通全局变量一样。

用于生成myfile工程骨架文件的ext_skel脚本创建了必要的代码来支持全局变量。通过检查php_myfile.h文件，你应当发现类似下面的被注释掉的一节，
复制代码代码如下:

ZEND_BEGIN_MODULE_GLOBALS(myfile)
int global_value;
char *global_string;
ZEND_END_MODULE_GLOBALS(myfile)

你可以把这一节的注释去掉，同时添加任何其他全局变量于这两个宏之间。文件后部的几行，骨架脚本自动地定义一个MYFILE_G(v)宏。这个宏应当被用于所有的代码，以便访问这些全局变量。这就确保在多线程环境中，访问的全局变量仅是一个线程的拷贝，而不需要互斥的操作。

为了使全局变量有效，最后需要做的是把myfile.c：
复制代码代码如下:

ZEND_DECLARE_MODULE_GLOBALS(myfile)

注释去掉。

你也许希望在每次PHP请求的开始初始化全局变量。另外，做为一个例子，全局变量已指向了一个已分配的内存，在每次PHP请求结束时需要释放内存。为了达到这些目的，全局变量机制提供了一个特殊的宏，用于注册全局变量的构造和析构函数（参考表对宏参数的说明）：
复制代码代码如下:

ZEND_INIT_MODULE_GLOBALS(module_name, globals_ctor, globals_dtor)

表 ZEND_INIT_MODULE_GLOBALS 宏参数
参数	含义
module_name	与传递给ZEND_BEGIN_MODULE_GLOBALS()宏相同的扩展名称。
globals_ctor	构造函数指针。在myfile扩展里，函数原形与void php_myfile_init_globals(zend_myfile_globals *myfile_globals)类似
globals_dtor	析构函数指针。例如，php_myfile_init_globals(zend_myfile_globals *myfile_globals)

你可以在myfile.c里看到如何使用构造函数和ZEND_INIT_MODULE_GLOBALS()宏的示例。

添加自定义INI指令
INI文件(php.ini)的实现使得PHP扩展注册和监听各自的INI条目。如果这些INI条目由php.ini、Apache的htaccess或其他配置方法来赋值，注册的INI变量总是更新到正确的值。整个INI框架有许多不同的选项以实现其灵活性。我们涉及一些基本的（也是个好的开端），借助本章的其他材料，我们就能够应付日常开发工作的需要。

通过在PHP_INI_BEGIN()/PHP_INI_END()宏之间的STD_PHP_INI_ENTRY()宏注册PHP INI指令。例如在我们的例子里，myfile.c中的注册过程应当如下：

复制代码代码如下:

PHP_INI_BEGIN()
STD_PHP_INI_ENTRY("myfile.global_value", "42", PHP_INI_ALL, OnUpdateInt, global_value, zend_myfile_globals, myfile_globals)
STD_PHP_INI_ENTRY("myfile.global_string", "foobar", PHP_INI_ALL, OnUpdateString, global_string, zend_myfile_globals, myfile_globals)
PHP_INI_END()


除了STD_PHP_INI_ENTRY()其他宏也能够使用，但这个宏是最常用的，可以满足大多数需要（参看表对宏参数的说明）：
复制代码代码如下:

STD_PHP_INI_ENTRY(name, default_value, modifiable, on_modify, property_name, struct_type, struct_ptr)

STD_PHP_INI_ENTRY 宏参数表
参数	含义
name	INI条目名
default_value	如果没有在INI文件中指定，条目的默认值。默认值始终是一个字符串。
modifiable	设定在何种环境下INI条目可以被更改的位域。可以的值是：
• PHP_INI_SYSTEM. 能够在php.ini或http.conf等系统文件更改
• PHP_INI_PERDIR. 能够在 .htaccess中更改
• PHP_INI_USER. 能够被用户脚本更改
• PHP_INI_ALL. 能够在所有地方更改
on_modify	处理INI条目更改的回调函数。你不需自己编写处理程序，使用下面提供的函数。包括：
• OnUpdateInt
• OnUpdateString
• OnUpdateBool
• OnUpdateStringUnempty
• OnUpdateReal
property_name	应当被更新的变量名
struct_type	变量驻留的结构类型。因为通常使用全局变量机制，所以这个类型自动被定义，类似于zend_myfile_globals。
struct_ptr	全局结构名。如果使用全局变量机制，该名为myfile_globals。

最后，为了使自定义INI条目机制正常工作，你需要分别去掉PHP_MINIT_FUNCTION(myfile)中的REGISTER_INI_ENTRIES()调用和PHP_MSHUTDOWN_FUNCTION(myfile)中的UNREGISTER_INI_ENTRIES()的注释。

访问两个示例全局变量中的一个与在扩展里编写MYFILE_G(global_value) 和MYFILE_G(global_string)一样简单。

如果你把下面的两行放在php.ini中，MYFILE_G(global_value)的值会变为99。
复制代码代码如下:

; php.ini – The following line sets the INI entry myfile.global_value to 99.myfile.global_value = 9

线程安全资源管理宏
现在，你肯定注意到以TSRM（线程安全资源管理器）开头的宏随处使用。这些宏提供给扩展拥有独自的全局变量的可能，正如前面提到的。

当编写PHP扩展时，无论是在多进程或多线程环境中，都是依靠这一机制访问扩展自己的全局变量。如果使用全局变量访问宏（例如MYFILE_G()宏），需要确保TSRM上下文信息出现在当前函数中。基于性能的原因，Zend引擎试图把这个上下文信息作为参数传递到更多的地方，包括PHP_FUNCTION()的定义。正因为这样，在PHP_FUNCTION()内当编写的代码使用访问宏（例如MYFILE_G()宏）时，不需要做任何特殊的声明。然而，如果PHP函数调用其他需要访问全局变量的C函数，要么把上下文作为一个额外的参数传递给C函数，要么提取上下文（要慢点）。

在需要访问全局变量的代码块开头使用TSRMLS_FETCH()来提取上下文。例如：
复制代码代码如下:

void myfunc(){
TSRMLS_FETCH();

MYFILE_G(myglobal) = 2;
}

如果希望让代码更加优化，更好的办法是直接传递上下文给函数（正如前面叙述的，PHP_FUNCTION()范围内自动可用）。可以使用TSRMLS_C（C表示调用Call）和TSRMLS_CC（CC边式调用Call和逗号Comma）宏。前者应当用于仅当上下文作为一个单独的参数，后者应用于接受多个参数的函数。在后一种情况中，因为根据取名，逗号在上下文的前面，所以TSRMLS_CC不能是第一个函数参。

在函数原形中，可以分别使用TSRMLS_D和TSRMLS_DC宏声名正在接收上下文。

下面是前一例子的重写，利用了参数传递上下文。
复制代码代码如下:

void myfunc(TSRMLS_D){
MYFILE_G(myglobal) = 2;
}
PHP_FUNCTION(my_php_function)
{
…
myfunc(TSRMLS_C);
…
}
~

总 结
现在，你已经学到了足够的东西来创建自己的扩展。本章讲述了一些重要的基础来编写和理解PHP扩展。Zend引擎提供的扩展API相当丰富，使你能够开发面向对象的扩展。几乎没有文档谈几许多高级特性。当然，依靠本章所学的基础知识，你可以通过浏览现有的原码学到很多。

更多关于信息可以在PHP手册的扩展PHP章节http://www.php.net/manual/en/zend.php中找到。另外，你也可以考虑加入PHP开发者邮件列表internals@ lists.php.net，该邮件列表围绕开发PHP 本身。你还可以查看一下新的扩展生成工具——PECL_Gen(http://pear.php.net/package/PECL_Gen)，这个工具正在开发之中，比起本章使用的ext_skel有更多的特性。
```

### 修改 PHP Session 存储位置、修改 INI 配置参数
```
n文件保存在c:/tmp目录下，默认tmp目录并没有创建，你可以在c盘下创建tmp目录，或者创建一个其他目录，比如leapsoulcn，再修改session.save_path的值，并去掉;，即

　　session.save_path = ‘/leapsoulcn’;

　　注意事项：

　　1、一般为了保证服务器的安全，session.save_path值最好设置为外网无法访问的目录，另外如果你是在linux服务器下进行session配置，请务必同时配置此目录为可读写权限，否则在执行session操作时会报错。

　　2、在使用session变量时，为了保证服务器的安全性，最好将register_globals设置为off，以保证全局变量不混淆，在使用session_register()注册session变量时，你可以通过系统全局变量$_SESSION来访问，比如你注册了leapsoulcn变量，你可以通过$_SESSION['leapsoulcn']来访问此变量。

　　session.save_path配置其他说明事项，从php.ini配置文件翻译而来

　　你可以使用”N;[MODE;]/path”这样模式定义该路径，N是一个整数，表示使用N层深度的子目录，而不是将所有数据文件都保存在一个目录下。

　　[MODE;]可选，必须使用8进制数，默认600(=384)，表示每个目录下最多保存的会话文件数量。[MODE;]并不会改写进程的umask。php不会自动创建这些文件夹结构。可使用ext/session目录下的mod_files.sh脚本创建。如果该文件夹可以被不安全的用户访问(比如默认的”/tmp”)，那么将会带来安全漏洞。当N>0时自动垃圾回收将会失效，具体参见下面有关垃圾搜集的部分。

　　如果你服务器上有多个虚拟主机，建议针对每个不同的虚拟主机分别设置各自不同的目录。

　　至此最基本的session配置就完成了，你只要保存php.ini，并重启apache，即可使用session功能。

　　其他session配置说明

　　session.save_handler = ”files”

　　默认以文件方式存取session数据，如果想要使用自定义的处理器来存取session数据，比如数据库，用”user”。

　　session.use_cookies = 1

　　是否使用cookies在客户端保存会话sessionid，默认为采用cookies

　　session.use_only_cookies = 0

　　是否仅仅使用cookie在客户端保存会话sessionid，这个选项可以使管理员禁止用户通过URL来传递id，默认为0，如果禁用的话，客户端如果禁用Cookie将使session无法工作。

　　session.name = “PHPSESSID”

　　当做cookie name来使用的session标识名

　　session.auto_start = 0

　　是否自动启动session，默认不启动，我们知道在使用session功能时，我们基本上在每个php脚本头部都会通过session_start()函数来启动session，如果你启动这个选项，则在每个脚本头部都会自动启动session，不需要每个脚本头部都以session_start()函数启动session，推荐关闭这个选项，采用默认值。

　　session.cookie_lifetime = 0

　　传递sessionid的Cookie有效期(秒)，0表示仅在浏览器打开期间有效。

　　session.gc_probability = 1

　　session.gc_divisor = 100

　　定义在每次初始化会话时，启动垃圾回收程序的概率。计算公式如下：session.gc_probability/session.gc_divisor，比如1/100，表示有1%的概率启动启动垃圾回收程序，对会话页面访问越频繁，概率就应当越小。建议值为1/1000~5000。

　　session.gc_maxlifetime = 1440

　　设定保存的session文件生存期，超过此参数设定秒数后，保存的数据将被视为’垃圾’并由垃圾回收程序清理。判断标准是最后访问数据的时间(对于FAT文件系统是最后刷新数据的时间)。如果多个脚本共享同一个session.save_path目录但session.gc_maxlifetime不同，将以所有session.gc_maxlifetime指令中的最小值为准。

　　如果你在session.save_path选项中设定使用子目录来存储session数据文件，垃圾回收程序不会自动启动，你必须使用自己编写的shell脚本、cron项或者其他办法来执行垃圾搜集。

　　比如设置”session.gc_maxlifetime=1440″ (24分钟)：

　　cd /path/to/sessions; find -cmin +24 | xargs rm

　　以上是一些常用的session配置选项说明，更多的session配置选项说明你可以参考php.ini文件中的说明。

　　至此，在php.ini配置文件中对session进行配置的PHP教程就介绍完毕了，通过上面的步骤实践与学习，基本的session功能都可以使用，至于session性能等其他方面则需要根据服务器环境和需求进行微调了，这个得自己体会。
```

### 负载均衡有哪几种，挑一种你熟悉的说明其原理
```
在说明常用负载均衡原理之前我们需要先知道何为“均衡”？

均衡，不能狭义地理解为分配给所有实际服务器一样多的工作量，因为多台服务器的承载能力各不相同，这可能体现在硬件配置、网络带宽的差异，也可能因为某台服务器身兼多职，我们所说的“均衡”，也就是希望所有服务器都不要过载，并且能够最大程序地发挥作用。


一、IP负载均衡(LVS-NAT)

因为反向代理服务器工作在HTTP层，其本身的开销就已经严重制约了可扩展性，从而也限制了它的性能极限。那能否在HTTP层面以下实现负载均衡呢？

当用户请求的数据包到达负载均衡服务器后，服务器会在操作系统内核进程获取网络数据包，然后根据负载均衡算法计算得到一台真实的Web服务器，然后将数据目的地址修改为新的地址，真实的Web服务器处理完成后，响应数据包会回到负载均衡服务器，负载均衡服务器再将数据包原地址修改为自身的IP发送给用户浏览器。

相较于反向代理负载均衡，IP负载均衡由于是在内核进程完成的数据分发，因此具有更好的处理性能。但是仍然没有解决的一个问题是：所有请求响应都要经过负载均衡服务器，所以这个时候负载均衡服务器的网卡带宽就成为集群吞吐量的瓶颈。

二、DNS负载均衡


DNS负责提供域名解析服务，当访问某个站点时，实际上首先需要通过该站点域名的DNS服务器来获取域名指向的IP地址，在这一过程中，DNS服务器完成了域名到IP地址的映射，同样，这样映射也可以是一对多的，这时候，DNS服务器便充当了负载均衡调度器，它就像http重定向转换策略一样，将用户的请求分散到多台服务器上，但是它的实现机制完全不同。

相比http重定向，基于DNS的负载均衡完全节省了所谓的主站点，或者说DNS服务器已经充当了主站点的职能。但不同的是，作为调度器，DNS服务器本身的性能几乎不用担心。因为DNS记录可以被用户浏览器或者互联网接入服务商的各级DNS服务器缓存，只有当缓存过期后才会重新向域名的DNS服务器请求解析。也说是DNS不存在http的吞吐率限制，理论上可以无限增加实际服务器的数量。

特性:

1、可以根据用户IP来进行智能解析。DNS服务器可以在所有可用的A记录中寻找离用记最近的一台服务器。

2、动态DNS：在每次IP地址变更时，及时更新DNS服务器。当然，因为缓存，一定的延迟不可避免。

不足：

1、没有用户能直接看到DNS解析到了哪一台实际服务器，加服务器运维人员的调试带来了不便。

2、策略的局限性。例如你无法将HTTP请求的上下文引入到调度策略中，而在前面介绍的基于HTTP重定向的负载均衡系统中，调度器工作在HTTP层面，它可以充分理解HTTP请求后根据站点的应用逻辑来设计调度策略，比如根据请求不同的URL来进行合理的过滤和转移。

3、如果要根据实际服务器的实时负载差异来调整调度策略，这需要DNS服务器在每次解析操作时分析各服务器的健康状态，对于DNS服务器来说，这种自定义开发存在较高的门槛，更何况大多数站点只是使用第三方DNS服务。

4、DNS记录缓存，各级节点的DNS服务器不同程序的缓存会让你晕头转向。

5、基于以上几点，DNS服务器并不能很好地完成工作量均衡分配，最后，是否选择基于DNS的负载均衡方式完全取决于你的需要。

三、反向代理负载均衡


这个肯定大家都有所接触，因为几乎所有主流的Web服务器都热衷于支持基于反向代理的负载均衡。它的核心工作就是转发HTTP请求。

相比前面的HTTP重定向和DNS解析，反向代理的调度器扮演的是用户和实际服务器中间人的角色：

1、任何对于实际服务器的HTTP请求都必须经过调度器

2、调度器必须等待实际服务器的HTTP响应，并将它反馈给用户（前两种方式不需要经过调度反馈，是实际服务器直接发送给用户）

特性：

1、调度策略丰富。例如可以为不同的实际服务器设置不同的权重，以达到能者多劳的效果。

2、对反向代理服务器的并发处理能力要求高，因为它工作在HTTP层面。

3、反向代理服务器进行转发操作本身是需要一定开销的，比如创建线程、与后端服务器建立TCP连接、接收后端服务器返回的处理结果、分析HTTP头部信息、用户空间和内核空间的频繁切换等，虽然这部分时间并不长，但是当后端服务器处理请求的时间非常短时，转发的开销就显得尤为突出。例如请求静态文件，更适合使用前面介绍的基于DNS的负载均衡方式。

4、反向代理服务器可以监控后端服务器，比如系统负载、响应时间、是否可用、TCP连接数、流量等，从而根据这些数据调整负载均衡的策略。

5、反射代理服务器可以让用户在一次会话周期内的所有请求始终转发到一台特定的后端服务器上（粘滞会话），这样的好处一是保持session的本地访问，二是防止后端服务器的动态内存缓存的资源浪费。

四、http重定向

当http代理（比如浏览器）向web服务器请求某个URL后，web服务器可以通过http响应头信息中的Location标记来返回一个新的URL。这意味着HTTP代理需要继续请求这个新的URL，完成自动跳转。

性能缺陷：

1、吞吐率限制

主站点服务器的吞吐率平均分配到了被转移的服务器。现假设使用RR（Round Robin）调度策略，子服务器的最大吞吐率为1000reqs/s，那么主服务器的吞吐率要达到3000reqs/s才能完全发挥三台子服务器的作用，那么如果有100台子服务器，那么主服务器的吞吐率可想而知得有大？相反，如果主服务的最大吞吐率为6000reqs/s，那么平均分配到子服务器的吞吐率为2000reqs/s，而现子服务器的最大吞吐率为1000reqs/s，因此就得增加子服务器的数量，增加到6个才能满足。

2、重定向访问深度不同

有的重定向一个静态页面，有的重定向相比复杂的动态页面，那么实际服务器的负载差异是不可预料的，而主站服务器却一无所知。因此整站使用重定向方法做负载均衡不太好。

我们需要权衡转移请求的开销和处理实际请求的开销，前者相对于后者越小，那么重定向的意义就越大，例如下载。你可以去很多镜像下载网站试下，会发现基本下载都使用了Location做了重定向。

最后要感谢这个优秀的平台，可以让我们相互交流，如果想进一步学习交流，可以加群460570824，希望大家可以一起学习进步！

```

### 数据库主从复制 M-S 是怎么同步的？是推还是拉？会不会不同步？怎么办
```
0、为什么需要主从复制？
1、在业务复杂的系统中，有这么一个情景，有一句sql语句需要锁表，导致暂时不能使用读的服务，那么就很影响运行中的业务，使用主从复制，让主库负责写，从库负责读，这样，即使主库出现了锁表的情景，通过读从库也可以保证业务的正常运作。

2、做数据的热备

3、架构的扩展。业务量越来越大，I/O访问频率过高，单机无法满足，此时做多库的存储，降低磁盘I/O访问的频率，提高单个机器的I/O性能。

1、什么是mysql的主从复制？
MySQL 主从复制是指数据可以从一个MySQL数据库服务器主节点复制到一个或多个从节点。MySQL 默认采用异步复制方式，这样从节点不用一直访问主服务器来更新自己的数据，数据的更新可以在远程连接上进行，从节点可以复制主数据库中的所有数据库或者特定的数据库，或者特定的表。

2、mysql复制原理
原理：
（1）master服务器将数据的改变记录二进制binlog日志，当master上的数据发生改变时，则将其改变写入二进制日志中；

（2）slave服务器会在一定时间间隔内对master二进制日志进行探测其是否发生改变，如果发生改变，则开始一个I/OThread请求master二进制事件

（3）同时主节点为每个I/O线程启动一个dump线程，用于向其发送二进制事件，并保存至从节点本地的中继日志中，从节点将启动SQL线程从中继日志中读取二进制日志，在本地重放，使得其数据和主节点的保持一致，最后I/OThread和SQLThread将进入睡眠状态，等待下一次被唤醒。

也就是说：
从库会生成两个线程,一个I/O线程,一个SQL线程;
I/O线程会去请求主库的binlog,并将得到的binlog写到本地的relay-log(中继日志)文件中;
主库会生成一个log dump线程,用来给从库I/O线程传binlog;
SQL线程,会读取relay log文件中的日志,并解析成sql语句逐一执行;
注意：
1--master将操作语句记录到binlog日志中，然后授予slave远程连接的权限（master一定要开启binlog二进制日志功能；通常为了数据安全考虑，slave也开启binlog功能）。 2--slave开启两个线程：IO线程和SQL线程。其中：IO线程负责读取master的binlog内容到中继日志relay log里；SQL线程负责从relay log日志里读出binlog内容，并更新到slave的数据库里，这样就能保证slave数据和master数据保持一致了。 3--Mysql复制至少需要两个Mysql的服务，当然Mysql服务可以分布在不同的服务器上，也可以在一台服务器上启动多个服务。 4--Mysql复制最好确保master和slave服务器上的Mysql版本相同（如果不能满足版本一致，那么要保证master主节点的版本低于slave从节点的版本） 5--master和slave两节点间时间需同步


具体步骤：
1、从库通过手工执行change master to 语句连接主库，提供了连接的用户一切条件（user 、password、port、ip），并且让从库知道，二进制日志的起点位置（file名 position 号）； start slave

2、从库的IO线程和主库的dump线程建立连接。

3、从库根据change master to 语句提供的file名和position号，IO线程向主库发起binlog的请求。

4、主库dump线程根据从库的请求，将本地binlog以events的方式发给从库IO线程。

5、从库IO线程接收binlog events，并存放到本地relay-log中，传送过来的信息，会记录到http://master.info中

6、从库SQL线程应用relay-log，并且把应用过的记录到http://relay-log.info中，默认情况下，已经应用过的relay 会自动被清理purge

3、mysql主从形式
（一）一主一从

（二）主主复制


（三）一主多从


（四）多主一从


（五）联级复制


4、mysql主从同步延时分析
mysql的主从复制都是单线程的操作，主库对所有DDL和DML产生的日志写进binlog，由于binlog是顺序写，所以效率很高，slave的sql thread线程将主库的DDL和DML操作事件在slave中重放。DML和DDL的IO操作是随机的，不是顺序，所以成本要高很多，另一方面，由于sql thread也是单线程的，当主库的并发较高时，产生的DML数量超过slave的SQL thread所能处理的速度，或者当slave中有大型query语句产生了锁等待，那么延时就产生了。

解决方案：

1.业务的持久化层的实现采用分库架构，mysql服务可平行扩展，分散压力。

2.单个库读写分离，一主多从，主写从读，分散压力。这样从库压力比主库高，保护主库。

3.服务的基础架构在业务和mysql之间加入memcache或者redis的cache层。降低mysql的读压力。

4.不同业务的mysql物理上放在不同机器，分散压力。

5.使用比主库更好的硬件设备作为slave，mysql压力小，延迟自然会变小。

6.使用更加强劲的硬件设备

主从复制中可能遇到主从数据不一致的情况，好了篇帖子，介绍比较详细
————————————如下———————————————-
先上Master库：
mysql>show processlist; 查看下进程是否Sleep太多。发现很正常。
show master status; 也正常。
mysql> show master status;
+——————-+———-+————–+——————————-+
| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |
+——————-+———-+————–+——————————-+
| mysqld-bin.000001 | 3260 | | mysql,test,information_schema |
+——————-+———-+————–+——————————-+
1 row in set (0.00 sec)
再到Slave上查看
mysql> show slave status\G
Slave_IO_Running: Yes
Slave_SQL_Running: No
可见是Slave不同步

下面介绍两种解决方法：
方法一：忽略错误后，继续同步
该方法适用于主从库数据相差不大，或者要求数据可以不完全统一的情况，数据要求不严格的情况
解决：
stop slave;

表示跳过一步错误，后面的数字可变
set global sql_slave_skip_counter =1;
start slave;
之后再用mysql> show slave status\G 查看：
Slave_IO_Running: Yes
Slave_SQL_Running: Yes
ok，现在主从同步状态正常了。。。

方式二：重新做主从，完全同步
该方法适用于主从库数据相差较大，或者要求数据完全统一的情况
解决步骤如下：
1.先进入主库，进行锁表，防止数据写入
使用命令：
mysql> flush tables with read lock;
注意：该处是锁定为只读状态，语句不区分大小写
2.进行数据备份

把数据备份到mysql.bak.sql文件
[root@server01 mysql]#mysqldump -uroot -p -hlocalhost > mysql.bak.sql
这里注意一点：数据库备份一定要定期进行，可以用shell脚本或者python脚本，都比较方便，确保数据万无一失
3.查看master 状态
mysql> show master status;
+——————-+———-+————–+——————————-+
| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |
+——————-+———-+————–+——————————-+
| mysqld-bin.000001 | 3260 | | mysql,test,information_schema |
+——————-+———-+————–+——————————-+
1 row in set (0.00 sec)
4.把mysql备份文件传到从库机器，进行数据恢复

使用scp命令
[root@server01 mysql]# scp mysql.bak.sql root@192.168.128.101:/tmp/
5.停止从库的状态
mysql> stop slave;
6.然后到从库执行mysql命令，导入数据备份
mysql> source /tmp/mysql.bak.sql
7.设置从库同步，注意该处的同步点，就是主库show master status信息里的| File| Position两项
change master to master_host = ‘192.168.128.100’, master_user = ‘rsync’, master_port=3306, master_password=”, master_log_file = ‘mysqld-bin.000001’, master_log_pos=3260;
8.重新开启从同步
mysql> stop slave;
9.查看同步状态
mysql> show slave status\G 查看：
Slave_IO_Running: Yes
Slave_SQL_Running: Yes
好了，同步完成啦。
```

### 如何保障数据的可用性，即使被删库了也能恢复到分钟级别。你会怎么做。
```
高可用数据库架构】

一般来说数据库集群会是主从架构：


或者主主架构：


 

如果此时主库宕机，可以：

（1）一个从库顶上，重建集群

（2）流量迁移到另一个主库

来保证数据的安全性与服务的可用性。

 

但是，如果人为不小心执行了“删全库”操作，命令会同步给其他从（主）库，导致所有库上的数据全部丢失，这下怎么办呢？

可以问问自己，当这种情况发生的时候：

（1）能不能恢复数据？（应该没有公司不能）

（2）多久能够恢复数据？

保证数据的安全性是DBA第一要务。

 

【全量备份+增量备份】

常见的数据库安全性策略是：全量备份+增量备份。


全量备份：定期（例如一个月）将库文件全量备份

 


增量备份：定期（例如每天）将binlog增量备份

 

如果不小心误删了全库，可以这么恢复：

（1）将最近一次全量备份的全库找到，拷贝回来（文件一般比较大），解压，应用

（2）将最近一次全量备份后，每一天的增量binlog找到，拷贝回来（文件较多），依次重放

（3）将最近一次增量备份后，到执行“删全库”之前的binlog找到，重放

恢复完毕。

为了保证方案的可靠性，建议定期进行恢复演练。

 

方案优点：能够找回数据

方案缺点：恢复时间非常长

有没有更优，更快恢复的方案呢？

 

【1小时延时从】

使用1小时延时从库，可大大加速“删全库”恢复时间。


什么是1小时延时从？

如图所示，增加一个从库，这个从库不是实时与主库保持同步的，而是每隔1个小时同步一次主库，同步完之后立马断开1小时，这个从库会与主库保持1个小时的数据差距。

当“删全库”事故发生时，只需要：

（1）应用1小时延时从

（2）将1小时延时从最近一次同步时间到，将执行“删全库”之前的binlog找到，重放

快速恢复完毕。

 

方案优点：能够快速找回数据

潜在不足：万一，万一，万一，1小时延时从正在连上主库进行同步的一小段时间内，发生了“删全库”事故，那怎么办咧？

 

【双份1小时延时从】

使用双份1小时延时从库，可以避免上述“万一，万一，万一”的事故发生。


什么是双份1小时延时从？

如图所示，两个1小时延时从，他们连主库同步数据的时间“岔开半小时”。

这样，即使一个延时从连上主库进行同步的一小段时间内，发生了“删全库”事故，依然有另一个延时从保有半小时之前的数据，可以实施快速恢复。

 

方案优点：没有万一，都能快速恢复数据

潜在不足：资源利用率有点低，为了保证数据的安全性，多了2台延时从，降低了从库利用率

 

【提高从库效率】


1小时延时从也不是完全没有用，对于一些“允许延时”的业务，可以使用1小时延时从，例如：

（1）运营后台，产品后台

（2）BI进行数据同步

（3）研发进行数据抽样，调研

但需要注意的是，毕竟这是从库，只能够提供“只读”服务哟。

 

【总结】

保证数据的安全性是DBA第一要务，需要进行：

（1）全量备份+增量备份，并定期进行恢复演练，但该方案恢复时间较久，对系统可用性影响大

（2）1小时延时从，双份1小时延时从能极大加速数据库恢复时间

（3）个人建议1小时延时从足够，后台只读服务可以连1小时延时从，提高资源利用率
```

### 数据库连接过多，超过最大值，如何优化架构。从哪些方便处理？
```
最近网站出现 User 数据库名称 has already more than 'max_user_connections' active connections 的报错，网站瘫痪。有必要研究下这个问题。

max_user_connections 是 MySQL 用户连接数的最大值设置，整段语句的意思是：服务器的 MySQL 的最大连接数参数设置不足。解决方法：修改 MySQL 安装目录下 my.ini 或者 my.cnf 文件内的 max_user_connections 参数的数值，重启 MySQL 服务器。

但是正常来说，MySQL默认的100个连接数是足够的。我们需要从程序上去考虑。MySQL的默认最大连接数为100（N），实际给普通用户使用只有N-1个，保留一个连接是留给超级管理员使用的，防止连接占满了不会把管理员也踢出来。很多网站在运行的时候都会出现连接数受限现象，我认为十之八九并非是网站的真实访问量太大导致连接数超标，更多是因为我们在设计网站程序的时候采用了不合理的设计架构或数据结构引起的。非正常连接超限可能原因如下（天缘即时归纳未必完整或无错讹仅供参考）：

类似人数、在线时间、浏览数等统计功能与主程序数据库同属一个数据空间时就很容易出现。
复杂的动态页尤其是用户每次浏览都涉及到多数据库或多表操作时候也很容易出现。
还有就是程序设计的不合理（比如复杂运算、等待等操作放置在数据库交互行为中间进行），或者程序存在释放BUG。
计算机硬件配置太低却安装太高版、太高配置的MySQL。
未采用缓存技术。
数据库未经过优化或表格设计及其复杂。
等等一些原因，都会延长数据库的数据交互时间或增加交互次数。所以，如果大家遇到这类问题，首先要考虑程序是否存在BUG导致连接释放失败，再次就是考虑优化软硬件。当然修改MySQL连接数也是软件优化的操作方法之一，希望大家都能够本着学习的态度通过研究一下自身的原因从而解决这一问题。如果实在是找不到原因，那就只好先修改连接数，暂缓定位真实原因了。

关于PHP的数据库持久连接 mysql_pconnect
PHP程序员应该都知道连接MySQL数据库可以使用mysql_pconnect（永久连接）函数，使用数据库永久连接可以提高效率，但是实际应用中数据库永久连接往往会导致出现一些问题，通常的表现就是在大访问量的网站上时常发生断断续续的无法连接数据库的情况，出现类似＂Too many connections in ...＂的错误提示信息，重新启动服务器又正常了，但过不了一会儿又出现同样的故障。对于这些问题的成因，恐怕就不是每个人都能说清楚的了，虽然PHP文档里有一些相关资料，但是解释的并不浅显易懂，这里我厚着脸皮试图做一个简单的讨论，所述观点不见得全都正确，欢迎大家反馈意见。

首先看看数据库永久连接的定义：永久的数据库连接是指在脚本结束运行时不关闭的连接。当收到一个永久连接的请求时。PHP 将检查是否已经存在一个（前面已经开启的）相同的永久连接。如果存在，将直接使用这个连接；如果不存在，则建立一个新的连接。所谓"相同"的连接是指用相同的用户名和密码到相同主机的连接。

PHP使用永久连接方式操作MySQL是有前提的：就是PHP必须安装为多线程或多进程Web服务器的插件或模块。最常见的形式是把PHP用作多进程Apache服务器的一个模块。对于一个多进程的服务器，其典型特征是有一个父进程和一组子进程协调运行，其中实际生成Web页面的是子进程。每当客户端向父进程提出请求时，该请求会被传递给还没有被其它的客户端请求占用的子进程。这也就是说当相同的客户端第二次向服务端提出请求时，它将有可能被一个不同的子进程来处理。在开启了一个永久连接后，所有不同子进程请求SQL服务的后继页面都能够重新使用这个已经建立的 SQL服务器连接。它使得每个子进程在其生命周期中只做一次连接操作，而非每次在处理一个页面时都要向 SQL 服务器提出连接请求。每个子进程将对服务器建立各自独立的永久连接。PHP本身并没有数据库连接池的概念，但是Apache有进程池的概念, 一个Apache子进程结束后会被放回进程池, 这也就使得用mysql_pconnect打开的的那个mysql连接资源可以不被释放，而是依附在相应的Apache子进程上保存到了进程池中。于是在下一个连接请求时它就可以被复用。一切看起来似乎都很正常，但是在Apache并发访问量大的时候，如果使用mysql_pconnect，会由于之前的Apache子进程占用的MySQL连接没有close, 很快使MySQL达到最大连接数，使得之后的请求可能得不到响应。

上面的部分文字是摘抄自PHP文档，看起来可能还是有些文绉绉的不好理解，那么我就用大白话再举一个例子来说明问题：

假设Apache配置最大连接数为1000，MySQL配置最大连接数为100，当Apache服务器接到200个并发访问的时候，其中100个涉及到数据库访问，剩下的100个不涉及数据库访问，因为这个时候还不存在可用的数据库连接，所以这里面涉及到数据库访问的100个并发会同时产生100个数据库永久连接，达到了数据库最大连接数，当这些操作没有结束的时候，任何其他的连接都无法再获得数据库连接，当这些操作结束了，相应的连接会被放入进程池，此时Apache的进程池里就有了200个空闲的子进程，其中100个是带有数据库连接的，由于Apache会为访问请求随机的挑选空闲子进程，所以你得到的子进程很可能是不包含数据库连接的那100个中的一个，而数据库连接已经达到了最大值，你也不可能成功的建立新的数据库连接，唉，你便只好不停的刷新页面，哪个时候运气好，碰巧分配到了带有数据库连接的子进程，才能正常浏览页面。如果是大访问量的网站来说，任何时候都可能存在大量的并发，所以浏览者可能就会不停的发现无法连接数据库的现象了。

或许你会说，我们把Apache和MySQL的最大连接数调成一样大不就可以了么？是的，合理的调整这个最大连接数某种程度上会避免这个问题的发生，但是Apache和MySQL的负载能力是不同的，如果按照Apache的负载能力来设置，对于MySQL来说，这个最大连接数就偏大，会产生大量的MySQL数据库永久连接，打个比方，就好像和平时代还要养活一个几百万的军队一样，其开销得不偿失；而如果按照Mysql的负载能力设置，对于Apache来说，这个最大连接数就偏小，有点杀鸡牛刀的感觉，无法发挥Apache的最大效率。

所以按照PHP手册上的介绍，只适合在并发访问不大的网站上使用数据库永久连接，但对于一个并发访问不大的网站来说，使用数据库永久连接带来的效率提高似乎没有太大的意义，从这个角度上来看，我觉得PHP中的数据库永久连接基本上是一个鸡肋的角色，如果你一定要使用数据库连接池的概念，可以尝试一下sqlrelay或者Apache本身提供的mod_dbd，说不定会有惊喜。

关于mysql_free_result和mysql_close
之前用mysql的时候一直是在用短链接，调用mysql_store_result获取一次数据之后就直接调用：

1
mysql_free_result(m_result);
2
mysql_close(m_Database);
但是有两个问题：

当使用长连接时（即connect之后一直不close），如果最后会调用mysql_close，需不需要每次都调用mysql_free_result呢?
当mysql_close调用之后，m_result的数据是否还可以用。
先说一下结论：

必须每次调用。因为经过测试，每次mysql_store_result的指针都是不同的，可见并不是共享了同一块buf。
还是可以使用。经过valgrind扫描，只调用mysql_close的扫描结果是：
1
==9397== 16,468 (88 direct, 16,380 indirect) bytes in 1 blocks are definitely lost in loss record 4 of 5
2
==9397==    at 0x40219B3: malloc (vg_replace_malloc.c:195)
3
==9397==    by 0x8053EA2: my_malloc (in /data/home/dantezhu/appbase/application/platform/openqqcom/share/db_openright/test/test)
4
==9397==    by 0x806D314: mysql_store_result (in /data/home/dantezhu/appbase/application/platform/openqqcom/share/db_openright/test/test)
5
==9397==    by 0x804BB04: CMySQLCppClient::Result(st_mysql_res*&) (mysql_cpp_client.cpp:127)
6
==9397==    by 0x804AB58: CDBOpenRight::GetUinsByApp(unsigned int, std::set<unsigned int, std::less<unsigned int>, std::allocator<unsigned int> >&) (db_openright.cpp:58)
7
==9397==    by 0x8049F10: main (test.cpp:27)
以后再慢慢研究。。
```

### 502 大概什么什么原因？ 如何排查  504呢？
```

状态代码解释

502 Bad Gateway：作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。
504 Gateway Time-out：作为网关或者代理工作的服务器尝试执行请求时，未能及时从上游服务器（URI标识出的服务器，例如HTTP、FTP、LDAP）或者辅助服务器（例如DNS）收到响应。

502 Bad Gateway原因分析

将请求提交给网关如php-fpm执行，但是由于某些原因没有执行完毕导致php-fpm进程终止执行。说到此，这个问题就很明了了，与网关服务如php-fpm的配置有关了。
php-fpm.conf配置文件中有两个参数就需要你考虑到，分别是max_children和request_terminate_timeout。
max_children最大子进程数，在高并发请求下，达到php-fpm最大响应数，后续的请求就会出现502错误的。可以通过netstat命令来查看当前连接数。
request_terminate_timeout设置单个请求的超时终止时间。还应该注意到php.ini中的max_execution_time参数。当请求终止时，也会出现502错误的。
当积累了大量的php请求，你重启php-fpm释放资源，但一两分钟不到，502又再次呈现，这是什么原因导致的呢？ 这时还应该考虑到数据库，查看下数据库进程是否有大量的locked进程，数据库死锁导致超时，前端终止了继续请求，但是SQL语句还在等待释放锁，这时就要重启数据库服务了或kill掉死锁SQL进程了。
对于长时间的请求可以考虑使用异步方式，可以参阅《关于PHP实现异步操作的研究》。

504 Gateway Time-out原因分析

     504错误一般是与nginx.conf配置有关了。主要与以下几个参数有关：fastcgi_connect_timeout、fastcgi_send_timeout、fastcgi_read_timeout、fastcgi_buffer_size、fastcgi_buffers、fastcgi_busy_buffers_size、fastcgi_temp_file_write_size、fastcgi_intercept_errors。特别是前三个超时时间。如果fastcgi缓冲区太小会导致fastcgi进程被挂起从而演变为504错误。

小结

1. max_children

2. request_terminate_timeout、max_execution_time
3. 数据库
4. 网关服务是否启动如php-fpm

504错误主要查看nginx.conf关于网关如fastcgi的配置。

解决方法之一



一、fastcgi缓冲区设置过小  

出现错误，首先要查找nginx的日志文件，目录为/var/log/nginx，在日志中发现了如下错误。


2013/01/17 13:33:47 [error] 15421#0: *16 upstream sent too big header while reading response header from upstream

查阅了一下资料，大意是nginx缓冲区有一个bug造成的,我们网站的页面消耗占用缓冲区可能过大。

网上查找了一下解决方法，在国外网站看到了一个增加缓冲区的方法，彻底解决了Nginx 502 Bad Gateway的问题。方法如下：



http {
    ...
    fastcgi_buffers 8 16k;
    fastcgi_buffer_size 32k;
    ...
}
二、代理缓冲区设置过小请根据服务器已经网站的情况自行增大上述两个配置项。

如果你使用的是nginx反向代理，如果header过大，超出了默认的1k，就会引发上述的upstream sent too big header （说白了就是nginx把外部请求给后端处理，后端返回的header太大，nginx处理不过来就会导致502。

server {

        listen       80;
        server_name  www.example.com;
  
        location / {
  
###############添加这3行
            proxy_buffer_size 64k;
            proxy_buffers   32 32k;
            proxy_busy_buffers_size 128k;
###############添加这3行
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP       $remote_addr;
            proxy_set_header X-Forwarded-For  $proxy_add_x_forwarded_for;
............
}
三、默认php-cgi的进程数设置过少
在安装好使用过程中出现502问题，一般是因为默认php-cgi进程是5个，可能因为phpcgi进程不够用而造成502，需要修改/usr/local/php/etc/php-fpm.conf 将其中的max_children值适当增加。也有可能是max_requests值不够用。需要说明的是这连个配置项占用内存很大，请根据服务器配置进行设置。否则可能起到反效果。

四、php执行超时

php执行超时，修改/usr/local/php/etc/php.ini 将max_execution_time 改为300

五、nginx等待时间超时
部分PHP程序的执行时间超过了Nginx的等待时间，可以适当增加nginx.conf配置文件中FastCGI的timeout时间


http  {
  fastcgi_connect_timeout 300;
  fastcgi_send_timeout 300;
  fastcgi_read_timeout 300;
  ......
  }
nginx 502 bad gateway
         一些运行在Nginx上的网站有时候会出现“502 Bad Gateway”错误，有些时候甚至频繁的出现。以下是小编搜集整理的一些Nginx 502错误的排查方法，供参考：

　　Nginx 502错误的原因比较多，是因为在代理模式下后端服务器出现问题引起的。这些错误一般都不是nginx本身的问题，一定要从后端找原因！但nginx把这 些出错都揽在自己身上了，着实让nginx的推广者备受置疑，毕竟从字眼上理解，bad gateway？不就是bad nginx吗？让不了解的人看到，会直接把责任推在nginx身上，希望nginx下一个版本会把出错提示写稍微友好一些，至少不会是现在简单的一句 502 Bad Gateway，另外还不忘附上自己的大名。

Nginx 502的触发条件

　　502错误最通常的出现情况就是后端主机当机。在upstream配置里有这么一项配置：proxy_next_upstream，这个配置指定了 nginx在从一个后端主机取数据遇到何种错误时会转到下一个后端主机，里头写上的就是会出现502的所有情况拉，默认是error timeout。error就是当机、断线之类的，timeout就是读取堵塞超时，比较容易理解。我一般是全写上的：



proxy_next_upstream error timeout invalid_header http_500 http_503;

     不过现在可能我要去掉http_500这一项了，http_500指定后端返回500错误时会转一个主机，后端的jsp出错的话，本来会打印一堆 stacktrace的错误信息，现在被502取代了。但公司的程序员可不这么认为，他们认定是nginx出现了错误，我实在没空跟他们解释502的原理 了……

503错误就可以保留，因为后端通常是apache resin，如果apache死机就是error，但resin死机，仅仅是503，所以还是有必要保留的。

解决办法

遇到502问题，可以优先考虑按照以下两个步骤去解决。

1、查看当前的PHP FastCGI进程数是否够用：

netstat -anpo | grep "php-cgi" | wc -l


如果实际使用的“FastCGI进程数”接近预设的“FastCGI进程数”，那么，说明“FastCGI进程数”不够用，需要增大。

2、部分PHP程序的执行时间超过了Nginx的等待时间，可以适当增加nginx.conf配置文件中FastCGI的timeout时间，例如：



http  {
  fastcgi_connect_timeout 300;
  fastcgi_send_timeout 300;
  fastcgi_read_timeout 300;
  ......
  }
php.ini中memory_limit设低了会出错，修改了php.ini的memory_limit为64M，重启nginx，发现好了，原来是PHP的内存不足了。

　　如果这样修改了还解决不了问题，可以参考下面这些方案：

一、max-children和max-requests

　　一台服务器上运行着nginx php(fpm) xcache，访问量日均 300W pv左右。

　　最近经常会出现这样的情况：php页面打开很慢，cpu使用率突然降至很低，系统负载突然升至很高，查看网卡的流量，也会发现突然降到了很低。这种情况只持续数秒钟就恢复了。

　　检查php-fpm的日志文件发现了一些线索。

Sep 30 08:32:23.289973 [NOTICE] fpm_unix_init_main(), line 271: getrlimit(nofile): max:51200, cur:51200  Sep 30 08:32:23.290212 [NOTICE] fpm_sockets_init_main(), line 371: using inherited socket fd=10, “127.0.0.1:9000″  Sep 30 08:32:23.290342 [NOTICE] fpm_event_init_main(), line 109: libevent: using epoll  Sep 30 08:32:23.296426 [NOTICE] fpm_init(), line 47: fpm is running, pid 30587
      在这几句的前面，是1000多行的关闭children和开启children的日志。

　　原来，php-fpm有一个参数 max_requests，该参数指明了，每个children最多处理多少个请求后便会被关闭，默认的设置是500。因为php是把请求轮询给每个 children，在大流量下，每个childre到达max_requests所用的时间都差不多，这样就造成所有的children基本上在同一时间 被关闭。

　　在这期间，nginx无法将php文件转交给php-fpm处理，所以cpu会降至很低(不用处理php，更不用执行sql)，而负载会升至很高(关 闭和开启children、nginx等待php-fpm)，网卡流量也降至很低(nginx无法生成数据传输给客户端)

　　解决问题很简单，增加children的数量，并且将 max_requests 设置为 0 或者一个比较大的值：

　　打开 /usr/local/php/etc/php-fpm.conf调大以下两个参数(根据服务器实际情况，过大也不行）


<value name="max_children">5120</value>  <value name="max_requests">600</value>
然后重启php-fpm。


二、增加缓冲区容量大小

　　将nginx的error log打开，发现“pstream sent too big header while reading response header from upstream”这样的错误提示。查阅了一下资料，大意是nginx缓冲区有一个bug造成的,我们网站的页面消耗占用缓冲区可能过大。参考老外写的修 改办法增加了缓冲区容量大小设置，502问题彻底解决。后来系统管理员又对参数做了调整只保留了2个设置参数：client head buffer，fastcgi buffer size。

三、request_terminate_timeout

　　如果主要是在一些post或者数据库操作的时候出现502这种情况，而不是在静态页面操作中常见，那么可以查看一下php-fpm.conf设置中的一项：

request_terminate_timeout

这个值是max_execution_time，就是fast-cgi的执行脚本时间。

0s

0s为关闭，就是无限执行下去。（当时装的时候没仔细看就改了一个数字）问题解决了，执行很长时间也不会出错了。优化fastcgi中，还可以改改这个值5s 看看效果。

php-cgi进程数不够用、php执行时间长、或者是php-cgi进程死掉，都会出现502错误。


深入分析Nginx 502 Bad Gateway和Nginx 504 Gateway Time-out及其解决

      Nginx 502 Bad Gateway的含义是请求的PHP-CGI已经执行，但是由于某种原因（一般是读取资源的问题）没有执行完毕而导致PHP-CGI进程终止。 
　　 Nginx 504 Gateway Time-out的含义是所请求的网关没有请求到，简单来说就是没有请求到可以执行的PHP-CGI。 
　　解决这两个问题其实是需要综合思考的，一般来说Nginx 502 Bad Gateway和php-fpm.conf的设置有关，而Nginx 504 Gateway Time-out则是与nginx.conf的设置有关。 
　　 而正确的设置需要考虑服务器自身的性能和访客的数量等多重因素。 
　　 以我目前的服务器为例子CPU是奔四1.5G的，内存1GB，CENTOS的系统，访客大概是50人左右同时在线。 
　　 但是在线的人大都需要请求PHP-CGI进行大量的信息处理，因此我将nginx.conf设置为： 
　　 fastcgi_connect_timeout 300s; 
　　 fastcgi_send_timeout 300s; 
　　 fastcgi_read_timeout 300s; 
　　 fastcgi_buffer_size 128k; 
　　 fastcgi_buffers 8 128k;#8 128 
　　 fastcgi_busy_buffers_size 256k; 
　　 fastcgi_temp_file_write_size 256k; 
　　 fastcgi_intercept_errors on; 
　　 这里最主要的设置是前三条，即 
　　 fastcgi_connect_timeout 300s; 
　　 fastcgi_send_timeout 300s; 
　　 fastcgi_read_timeout 300s; 
　　 这里规定了PHP-CGI的连接、发送和读取的时间，300秒足够用了，因此我的服务器很少出现504 Gateway Time-out这个错误。最关键的是php-fpm.conf的设置，这个会直接导致502 Bad Gateway和504 Gateway Time-out。 
　　 下面我们来仔细分析一下php-fpm.conf几个重要的参数： 
　　 php-fpm.conf有两个至关重要的参数，一个是"max_children",另一个是"request_terminate_timeout" 
　　 我的两个设置的值一个是"40 ，一个是"900 ，但是这个值不是通用的，而是需要自己计算的。 
 计算的方式如下： 
　　 如果你的服务器性能足够好，且宽带资源足够充足，PHP脚本没有系循环或BUG的话你可以直接将"request_terminate_timeout"设置成0s。0s的含义是让PHP-CGI一直执行下去而没有时间限制。而如果你做不到这一点，也就是说你的PHP-CGI可能出现某个BUG，或者你的宽带不够充足或者其他的原因导致你的PHP-CGI能够假死那么就建议你给"request_terminate_timeout"赋一个值，这个值可以根据你服务器的性能进行设定。一般来说性能越好你可以设置越高，20分钟-30分钟都可以。由于我的服务器PHP脚本需要长时间运行，有的可能会超过10分钟因此我设置了900秒，这样不会导致PHP-CGI死掉而出现502 Bad gateway这个错误。 
　　而"max_children"这个值又是怎么计算出来的呢？这个值原则上是越大越好，php-cgi的进程多了就会处理的很快，排队的请求就会很少。设置"max_children"也需要根据服务器的性能进行设定，一般来说一台服务器正常情况下每一个php-cgi所耗费的内存在20M左右，因此我的"max_children"我设置成40个，20M*40=800M也就是说在峰值的时候所有PHP-CGI所耗内存在800M以内，低于我的有效内存1Gb。而如果我的"max_children"设置的较小，比如5-10个，那么php-cgi就会"很累"，处理速度也很慢，等待的时间也较长。如果长时间没有得到处理的请求就会出现504 Gateway Time-out这个错误，而正在处理的很累的那几个php-cgi如果遇到了问题就会出现502 Bad gateway这个错误。




Nginx 502 bad gateway错误解决方法


使用Nginx作为Web服务器的时候，你或多或少都会遇到Nginx 502 bad gateway的错误，造成这种错误的原因有很多。下面我们来一一解析。  
 

一、查看php-cgi是否在运行  
有时候由于网站流量过大或者其它原因，导致php-cgi直接down掉，所以我们得看php-cgi是否在运行。执行如下命令：

ps -A | grep php5-cgi  
如果没有运行，手动启动

/etc/init.d/php_cgi start  
如果你发现php-cgi不明原因有时候down掉，可以使用下面的脚本临时解决这个问题，添加到cronjob。

if ps aux | grep ‘php5-cgi' | grep -v grep  > /dev/null ; then          echo "PHP-cgi is runnning !"      else          echo "PHP-cgi is down. Starting over…"          /etc/init.d/php-fcgi start  fi 二、fastcgi进程数不够用、php执行时间长的原因  
　　fastcgi进程数可以修改php-fpm.conf中的max_children的数值，高峰时php-cgi耗掉的最大内存为20M，请根据自己的内存情况计算了。 
　　 限制php执行时间可以在php-fpm.conf中的request_terminate_timeout设置，这是为了防止php程序的bug导致php-cgi假死。

三、FastCGI执行时间过长  
根据实际情况调高以下参数值

fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300;  
除了上面列出的三种情况，当然还会有其它原因，但上面三种情况是最常见的

　　最近几天发现网通线路的服务器出现流量不稳定的情况，具体的表现是，流量时而高，时而低，在流量低的时候发现系统的负载很小，几乎为0，但是过一会，负载又高上去，流量也上去，很是奇怪，查找了2天没有找到原因，后来看到一边文章，介绍了解决nginx出现502的错误现象，按照这个方法进行尝试，最终还是找到了问题的原因。

　　解决步骤如下：

　　1、查看当前的PHP FastCGI进程数是否够用

　　netstat -anpo | grep "php-cgi" | wc -l

　　如果实际使用的"FastCGI进程数"接近预设的"FastCGI进程数"，那么，说明"FastCGI进程数"不够用，需要增大。

　　2、部分PHP程序的执行时间超过了Nginx的等待时间，可以适当增加nginx.conf配置文件中FastCGI的timeout时间，例如：

　　在做第一步的时候，系统当前的PHP FastCGI进程数明显超过了预设值的64这个数值，在电信的服务器上查看当前的PHP FastCGI进程数没有高于64这个数值，而且网通线路的活动连接明显高于电信的活动连接，准备到晚上的时候看看情况，结果到晚上22：30的时候，查看系统当前的PHP FastCGI进程数明显小于64预设值，当前的活动连接也比原来低很多，由此可以说明出现nginx不稳定的情况是由于服务器访问负载过大引起的，就是加上第二步的错误也不顶作用。

　　总结，php-cgi进程数不够用、php执行时间长、或者是php-cgi进程死掉，都会出现502错误

意思就是叫你去 Nginx 的 upstream 去找原因
至于怎么找原因，难道还能不看 log?
502 504 直接去看 PHP-FPM/Spawn-fcgi 这个没的说
首先看 nginx 的 error log （默认为 /var/log/nginx/error.log ），通常会有些线索。
然后看应用服务器（ PHP-FPM 、 Ruby 的 Unicorn, Puma 等）的日志。
最后看应用的的日志。一般靠框架记录日志，自己写的代码很少写日志。
经验多了之后可以调整下顺序。
```


## 架构篇

### 负载均衡（Nginx、HAProxy、DNS）
```

```

### 主从复制（MySQL、Redis）
```

```

### 数据冗余、备份（MySQL增量、全量 原理）
```

```

### 监控检查（分存活、服务可用两个维度）
```

```

### MySQL、Redis、Memcached Proxy 、Cluster 目的、原理
```

```

### 分片
```

```

### 高可用集群
```

```

### RAID
```

```

### 源代码编译、内存调优
```

```


### php性能优化
```
php语言级的性能优化
优化点：少写代码，多用php自身能力
- 性能问题：自身代码冗余较多，可读性不佳，并且性能低。
- 为什么性能低：php代码需要编译解析为底层语言，这一过程每次请求都会处理一遍，开销大。
- 解决方案：多用php内置变量、常量、函数
- 测试方法：直接使用ab对比

优化点：php内置函数的性能优劣
- 性能问题：php内置函数，之间依然存在快慢差异
- 解决方案：多去了解php内置函数的时间复杂度
- 测试方法：对比isset()和array_key_exists()的性能差异
<?php
    $start = current_time();
    $i = 0;
    $arr = range(1, 200000);
    while($i<200000){
        ++$i;
        //isset($arr[$i]);
        array_key_exists($i,$arr);
    }
    $end = current_time();
    echo "Lost Time:". number_format($end-$start,3)###1000;
    echo "\n";
    function current_time(){
        list($usec, $sec) = explode(" ".microtime());
        return ((float)$usec + (float)$sec);
    }
?>
优化点：尽可能少用魔法函数
- 情况描述：php提供的魔法函数，性能不佳
- 为什么性能低：为了给php程序员省事，php语言为你做了很多
- 解决方案：尽可能规避使用魔法函数
- 测试方法：time php test.php
    - time  liunx命令
    - php 指定程序
    - test.php 指定文件 
    > 注意：php主要在返回值中看user耗时

优化点：产生额外开销在错误抑制符
- 情况描述：php提供的错误抑制符只是为了方便懒人
- @的实际逻辑：在代码开始前，结束后，增加Opcode,忽略报错
    vld php Opcode查看扩展:用于将Opcode显示出来
- 解决方案：尽量不要使用@错误抑制符
- 测试方法：php -dvld.active=1 -dvld.execute=0 at.php
    
- php 执行php的vld显示Opcode
    
优化点：避免在循环内做运算
- 情况描述：循环内在函数或运算会被重复执行
- 解决方案：在循环外获取需要在值，再给循环操作

优化点：减少计算密集型业务
- 情况描述：php不适合密集型的场景
- 为什么：php语言特性决定php不适合做大数据业务
- php适合场景：适合衔接webserver与sql

优化点：务必使用带引号字符串做键值
- 情况描述：php会将没有引号的键值当作常量，产生查找常量在开销。
- 解决方案：严格使用带引号作为键值

php周边问题的性能优化
- php周边有什么:
    - linux运行环境
    - 文件存储  硬盘
    - 数据库    mysql
    - 缓存      redis
    - 网络  

优化点：减少文件类操作
- 常见php场景在开销次序
读写内存 << 读写数据库 << 读写磁盘 << 读写网络数据

优化点：优化网络请求
- 网络请求的坑：
    1. 对方接口的不确定因素
    2. 网络稳定性
- 如何优化网络请求：
    - 设置超时时间
    1. 连接超时 <200ms
    2. 读超时   <800ms
    3. 写超时   <500ms
    - 将串行请求并行化
    1. 使用curl_multi_###()
    2. 使用swoole扩展

优化点：压缩php接口输出
- 如何压缩：使用Gzip即可
- 压缩的利于弊：利于我们的数据输出，Client段能更快获取数据;弊端为会有额外的CPU开销

优化点：缓存重复计算内容
- 什么情况下坐输出内容缓存：多次请求，内容不变情况

重叠时间窗口思想===并行
旁路方案===并行

php语言自身分析、优化
 php性能分析

工具:XHPorf（源自FackBook的php性能分析工具）
实践：通过分析Wordpress程序，做优化。
使用: php --ri xhprof   查看版本
在入口文件index.php添加

xhprof_enable();

// ...

$data = xhporf_disable();
include_once "/var/www/html/xhprof_lib/utils/xhprof_lib.php";
include_once "/var/www/html/xhprof_lib/utils/xhprof_runs.php";
$objXhprofRun = new XHProfRuns_Default();
$run_id = $objXhprofRun->save_run($data,"test");
var_dump($run_id);
查看xhp目录查看相关信息
参数：
    runction_name   函数名
    calls   被调用在次数
    InclWallTime    当流程走到该函数，之前和现在这个函数处理在总耗时
    ExclWallTime    这个函数执行了多少微秒

其他工具推荐：
    ab  压力测试
    vld opcode代码分析

php性能瓶颈解决方案：
    Opcode Cache:php扩展APC等
    peci.php.net    php扩展网站
    使用php扩展解决复杂的业务
    Runtime优化:HHVM

 Apache Benchmark(ab)

> ab是由Apache提供的压力测试软件。安装apache服务器时会自带该压测软件
- 使用方法: ./ab -n1000 -c100 http://www.baidu.com/
    - -n 请求数
    - -c 并发数
    - http 压测目标地址
    - -h 帮助 

###*返回参数说明**
- Requests per second（每秒接受请求数尽可能多）
- Time per request（每秒请求在耗时尽可能少）
```

### 缓存
```

```

### 工作中遇到哪里需要缓存，分别简述为什么
```

```

### 搜索解决方案
```
```
### 性能调优
```
```
### 各维度监控方案
```
```
### 日志收集集中处理方案
```
```
### 国际化
```
```
### 数据库设计
```
```
### [静态化如何实现的](https://blog.csdn.net/qq_39618306/article/details/79014438)
```
这里要说的静态化指的是页面静态化，也即生成实实在在的静态文件，
也即不需要查询数据库就可以直接从文件中获取数据，
指的是真静态。它的实现方式主要有两种：
一种是我们在添加信息入库的时候就生成的静态文件，也称为模板替换技术，
这种主要用在后台，用于一些基本上很少变化的信息上，
在添加信息的时候使用添加的信息来替换制定好的模板中的内容，
达到生成静态文件的目的，这样在前台访问该信息时，
可以直接从生成好的静态文件中获取信息，如一些CMS系统。
另外一种是用户在访问我们的页面时先判断是否有对应的缓存文件存在，
如果存在就读缓存，不存在就读数据库，同时生成缓存文件。
这种实现的主要原理是基于PHP中的ob缓冲技术来实现的，
当没有静态文件时，从数据库中读取，读取的数据使用OB缓存，
使用相关的函数从OB缓冲中读取数据，写入到文件中，形成静态文件。
当然这个过程中要考虑静态文件的缓存周期问题，
我们可以根据文件的最后修改时间和当前时间及设定的缓存时间来定时更新缓存文件。
```
### 静态化方案
```
```
### 画出常见 PHP 应用架构图
```
```

## 框架篇
### ThinkPHP（TP）、CodeIgniter（CI）、Zend（非 OOP 系列）
```

```

### Yaf、Phalcon（C 扩展系）
```

```

### Yii、Laravel、Symfony（纯 OOP 系列）
```

```

### Swoole、Workerman （网络编程框架）
```

```

### 对比框架区别几个方向点
```

```

### 是否纯 OOP
```

```

### 类库加载方式（自己写 autoload 对比 composer 标准）
```

```

### 易用性方向（CI 基础框架，Laravel 这种就是高开发效率框架以及基础组件多少） 
```

```

### 黑盒（相比 C 扩展系）
```

```

### 运行速度（如：Laravel 加载一大堆东西）
```

```

### 内存占用
```

```


## 设计模式
### php的设计模式
```
1. ###*单例模式**
一个类在整个应用中，只有一个对象实例的设计模式
类必须自行创建这个实例
必须自行向整个系统提供这个实例
###*三私**：私有静态成员变量、构造函数、克隆函数
###*一公**：公共的静态方法

2. ###*工厂模式**
可以根据输入的参数或者应用程序配置的不同,创建一种专门用来实例化并返回其它类的实例的类

3. 观察者模式
观察者模式提供了组件之间紧密耦合的另一种方法。
该模式：一个对象通过添加一个方法（该方法允许另一个对象，即观察者注册自己）全本身变得可观察。
当可观察的对象更改时，它会将消息发送到已注册的观察者。这些观察者使用该信息执行的操作与可观察的对象无关。

4. 命令链模式：
以松散耦合主题为基础，发送消息、命令和请求，或通过一组处理程序发送任意内容。
每个处理程序都会自行判断自己能否处理请求，如果可以，该请求被处理，进程停止。

5. 策略模式：
此算法是从复杂类提取的，因而可以方便地替换。
```

### 单例模式（重点）
```
```
### 工厂模式（重点）
```
```
### 观察者模式（重点）
```
```
### 依赖注入（重点）
```
```
### 装饰器模式
```
```
### 代理模式
```
```
### 组合模式
```
```

## 安全篇
### SQL 注入
### sql注入获取后台管理员账号密码
```
在完全拿下服务器主机之前，存在sql注入漏洞的网站，可能会因此提供给黑客后台管理员的账号密码，黑客登录后台后，上传木马，拿下整个主机。这是sql注入的一种应用场景。

以下讲解sql注入获取后台管理员账号密码的过程，本文以尽力对新手友好的展现过程来讲解基本原理，高手与百事通请避免观看，以免徒耗时间。

为避免读者惹上麻烦，已对原本目标网站信息进行涂抹，并提供笔者自己搭建的测试网站供有兴趣的朋友操作，换句话说，黑我的网站是合法的。

正式开始，在网站中，点击人才招聘，跳转到如下页面，同时地址栏url变为图中所示。

可以看到链接中传给服务器参数，id=2，此处逻辑一般为：服务器获取到id值为2，再通过id值返回给浏览器对应页面或内容，这个过程有可能会经过数据库。

首先惯常试试是否存在sql注入漏洞，将地址栏中的id=2更改为id=2'，即在值2后面添加单引号，回车看结果。

sweet，塔斯丁狗，服务器返回了数据库错误，并详细的打印了错误信息。由错误信息可以得知，网站使用了mysql数据库，并展示了具体的sql语句：

select ### from sy_page where 1 and id=2
这条语句表示：查询sy_page表里id为2的全部数据，而发生错误的原因则是因为多出的单引号使sql语句语法发生错误。

同时可以看出，我们只加过一个单引号'，上图错误信息却显示，实际sql语句中变为了\'反斜杠加单引号，这说明，服务器是做了特殊符号过滤的，以此来防sql注入，这确实起作用，我们便无法注入带有单引号双引号等特殊符号的语句，但其他的注入却是没问题的。

在确定存在sql注入漏洞后，对于查询语句，可以先确定查询的数据有多少列，通过如下图链接所示增加排序条件order by来探测，将地址栏中id=2改为id=2 order by 15，这将使原本的查询语句变成

select ### from sy_page where 1 and id=2 order by 15
这表示，在原来查询结果的基础上进行排序，排序的依据是第15列数据的值。

但实际结果却报错，显示不存在第15列，这正是我们追求的结果，说明查询的出数据不超过15列。

递减列数尝试，直到列数降为10時，才不再报错，如下图所示，说明数据有10列。

在得知列数后，便可以开启真正的掠夺了。那就是在原本的查询结果中加入自己的查询数据。如下图所示，将地址栏中

id=2
改为

id=2 union select 1,2,3,4,5,6,7,8,9,10

这样变化的意图是什么？举个例子，假如下图所示查询数据为网站本来正常的查询结果，id=852，从左至右刚好10列数据。

而加入union select 1,2,3,4,5,6,7,8,9,10后，查询结果便如下图所示。多出了一行结果为1 2 3 4 5 6 7 8 9 10的数据。

而网站中这行多出来的数据没有显示在页面上的原因是什么？可以想像，网站在获取到两行数据后只取了第一行的数据，因为网站根本没预料到会有两行以上的数据。那么我们只需要将自己的数据排到第一行，就可以替代网站本身的数据了。于是再做一次排序，继续追加一句order by 1，根据第一列的值进行排序，便产生如下结果

可以看到，我们自己的数据排到了最前面，这是因为我们自己构建的数据中，第一列的值为1，小于上图示例数据852，于是排到了前面。

随后拿到网站中测试，将

id=2
改为

id=2 union select 1,2,3,4,5,6,7,8,9,10 order by 1
可以看到，我们的内容替换掉了网站内容，第二列数据值2与第6列数据值6显示在了网站页面。

这两列显示数据便成了数据输出窗口。换言之，只需要将2和6的值替换为数据库实际内容，就可以输出到页面显示，被我们看见。

得知第2列和第6列会显示后，便可以开始尝试将2和6替换为系统表数据，将前面的

id=2 union select 1,2,3,4,5,6,7,8,9,10 order by 1
更改为

id=2 union select 1,table_schema,3,4,5,table_name,7,8,9,10 from information_schema.columns order by 1

这句表示，我们自己追加的数据不再是单纯的10个数字，而是从mysql的系统表columns中查询的数据，第二列与第六列分别显示columns表中的table_schema列与table_name列数据。table_schema列存储的是数据库名字信息，table_name列存储着表名信息。下图查询到的数据库名为information_schema，表名为character_sets

上面查询的columns表是mysql的系统表，里面存储着mysql中所有的表名及列信息，如下图的本机展示可以看到所有的表名，列名，列的数据类型，通过这个表，可以爆出所有的表名，列名，及所在数据库，是获取数据的突破口。

下图是一个本机测试，用的是前面爆网站表名的语句：

select ### from table1 where id=852 union select 1,table_schema,3,4,5,table_name,7,8,9,10 from information_schema.columns order by 1
可以看到，所有的表列数据被追加到我们自己的数据里，做个对比的话，下图最后一列数据就像网站正常显示的数据，其他的则是我们注入的数据。

那么如何把这么多行数据逐个爆出来呢？很明显依靠排序是不足以实现的，这时可以再在之前的查询语句后追加一个limit条件，如下图所示

limit m,n
表示筛选出从第m+1行开始的n行数据

limit 0,1
便表示筛选出第一行开始的一行数据，这样如下图就得到了第一行数据，换成limit 1,1则是筛选出第二行数据，以此类推，可以分别得出每一行的数据。

应用到网站中，发现从第41行开始不再是系统表，如下图所示，第41行爆出了数据库名sq_sydata，表名sy_admin

一行一行爆，如图展示，更改limit限定值会爆出其他的表名。

最后爆出了所有的表，如下图所示。

按照命名推断，sy_admin表必定是后台管理员账号表，既然columns系统表中存放着所有列信息，自然也可以用来将sy_admin表的列名爆出来，将

id=2 union select 1,table_schema,3,4,5,table_name,7,8,9,10 from information_schema.columns order by 1
中的table_schema（数据库名）替换为table_name（表名），table_name（表名）替换为column_name（列名），再尝试更改limit的限定值，直到更改为480時开始出现sy_admin表的内容，下图可以看到第481列（limit 480,1）爆出了sy_admin表的一个列名为id。

继续递增列爆481，得到列名loginname。

继续递增，最终爆出sy_admin表的所有列名，如下图所示。

可以推断，上图中的lpginname列应该是用户名，password则是登录密码。

ok，已经得到了sy_admin表的所有列名，接下来就可以开始获取sy_admin表中的实际数据了。

然后我们不再查询系统表，转向sy_admin表，如下图所示，将第二列替换为loginname，第六列替换为password，表名从系统表information_schema.columns（information_schema数据库中的columns表）更换为sq_sydata.sy_admin（sq_sydata数据库中的sy_admin表），再稍微更换limit限定条件，最终得到了后台管理员admin的密码，如下图所示

目标达成。

上图中加密后的密码可以通过工具或百度在线md5解密工具解密，不作赘述。

同样也可以更换列名查出其他的信息，比如下图所示登录次数27次，上次登录于2月1号。以及其他的东西，你懂的。
```
### xss攻击怎么防止
```
XSS又称CSS，全称Cross SiteScript(跨站脚本攻击)， XSS攻击类似于SQL注入攻击，
是Web程序中常见的漏洞，XSS属于被动式且用于客户端的攻击方式，所以容易被忽略其危害性。
其原理是攻击者向有XSS漏洞的网站中输入(传入)恶意的HTML代码，当用户浏览该网站时，
这段HTML代码会自动执行，从而达到攻击的目的。如，盗取用户Cookie信息、破坏页面结构
常见的恶意字符XSS输入：
1. XSS 输入通常包含 JavaScript 脚本，如弹出恶意警告框：
`<script>alert("XSS");</script>`
2. XSS 输入也可能是 HTML 代码段，譬如：
    (1) 网页不停地刷新 `<meta http-equiv="refresh" content="0;">`
    (2) 嵌入其它网站的链接，重定向到其它网站等。
方法：利用php htmlentities()函数
php防止XSS跨站脚本攻击的方法：是针对非法的HTML代码包括单双引号等，使用htmlspecialchars()函数。
```

### XSS 与 CSRF
```
```
### 输入过滤
```
```
### Cookie 安全
```
```
### 禁用 `mysql_` 系函数
```
```
### 数据库存储用户密码时，应该是怎么做才安全
```
```
### 验证码 Session 问题
```
```
### 安全的 Session ID （让即使拦截后，也无法模拟使用）
```
```
### 目录权限安全
```
```
### 包含本地与远程文件
```
```
### 文件上传 PHP 脚本
```
```
### `eval` 函数执行脚本
```
```
### `disable_functions` 关闭高危函数
```
```
### FPM 独立用户与组，给每个目录特定权限
```
```
### 了解 Hash 与 Encrypt 区别
```
```
### php在储存session以什么形式存在
```
PHP为session的存储提供了三种方式: 文件/ 内存/ 自定义存储,默认是使用文件存储.
在访问量大的网站上采用这种方式就不大合适,因为这样会导致大量的输入输出的冗余.
我们可以在php.ini更改配置文件或者php脚本中通过相应的函数来设置session文件的存储类型
来改变session文件的存储形式
```
### Session 共享、存活时间
```
https://blog.csdn.net/m_nanle_xiaobudiu/article/details/81177698
一、Session的原理
以下以默认情况举例：

session_start();之后，会生成一个唯一的session_id，每一个用户对应唯一一个session_id，每一个session_id对应服务器端的一个session文件。这个session文件存储着当前session_id的信息，比如下面，就存储了name和age的键值。

 

1、设置Session存储的引擎（本地服务器的文件还是redis等），【php.ini 文件】

[Session]
 
session.save_handler = files
 
session.save_path = /data/SessionLogs
 

2、默认情况下的Session的使用 

<?php
/**
 * session的使用
 * 默认情况（不更改session.save_handle参数时），是存储在文件file中的
 * 默认情况下使用session的情况（用户24分钟内没有刷新操作会过期）
 * 每个用户对应唯一session_id,每一个session_id对应服务器中存储的一个session文件，这个文件中存储了当前session_id的信息，比如下面，就存储了name和age的键值
 */
session_start();
echo session_id();
echo "<br>";
$_SESSION['age'] = 26;
$_SESSION['name'] = 'xiaobudiu';
var_dump($_SESSION);
 

3、在服务器中存储的形式是这样的



 

 

二、使用Redis存储Session
在网站访问量较大时，我们通常会做集群（比如nginx负载均衡等），这时，如何解决session会话的共享问题。

（1）使用ip_hash或者自定义key做负载均衡轮询策略是一个办法，但由于有时候用户可能走代理，所以这个方法其实并不是那么完美。

（2）另一个解决session共享问题的方法就是使用redis或者memcache缓存数据库去存储session，进而实现session共享问题。

 

1、设置php.ini 文件中的session.save_handle 和session.save_path

session.save_handler = Redis
 
session.save_path = "tcp://localhost:6379"
注1：如果连接的是远程redis，需要将localhost换成对应的远程ip地址。像这样，

session.save_handler = Redis
 
session.save_path =  "tcp://47.94.203.119:6379"
注2：如果为redis已经添加了auth权限（requirpass），session.save_path项则应该这样写

session.save_handler = Redis
 
session.save_path =  "tcp://47.94.203.119:6379?persistent=1&database=10&auth=myredisG506"
 

2、使用redis存储session信息

<?php
/**
 * 将session存储在redis中
 */
session_start();
echo session_id();
echo "<br>";
$_SESSION['age'] = 26;
$_SESSION['name'] = 'xiaobudiu';
$_SESSION['sex'] = 'man';
var_dump($_SESSION);
在redis上是以这样的形式进行存储的



 

注：

搭建nginx集群： https://blog.csdn.net/m_nanle_xiaobudiu/article/details/80862272

搭建mysql主从复制架构：https://blog.csdn.net/m_nanle_xiaobudiu/article/details/81086243

搭建redis集群： https://blog.csdn.net/m_nanle_xiaobudiu/article/details/81004557

 
 
三、使用Redis存储Session，并设置Session会话存活时间以及Session中某一元素存活时间
 

封装session类 b.php

<?php
 
 
/**
 * session控制类
 *
 */
 
class Session
{
 
    function __construct($lifetime = 3600)
    {
        //初始化设置session会话存活时间
        ini_set('session.gc_maxlifetime',$lifetime);
    }
 
    /**
     * 设置当前会话session的key-value
     * @param String $name   session name
     * @param Mixed  $data   session data
     * @param Int    $expire 有效时间(秒)
     */
    function set($name, $data, $expire = 600)
    {
        $session_data = array();
        $session_data['data'] = $data;
        $session_data['expire'] = time()+$expire;
        $_SESSION[$name] = $session_data;
    }
 
    /**
     * 读取当前会话session中的key-value
     * @param  String $name  session name
     * @return Mixed
     */
    function get($name)
    {
        if(isset($_SESSION[$name])) {
            if($_SESSION[$name]['expire'] > time()) {
                return $_SESSION[$name]['data'];
            }else{
                self::clear($name);
            }
        }
        return false;
    }
 
    /**
     * 清除当前session会话中的某一key-value
     * @param  String  $name  session name
     */
    function clear($name)
    {
        unset($_SESSION[$name]);
    }
 
    /**
     * 删除当前session_id对应的session文件（清空当前session会话存储）
     */
    function destroy()
    {
        session_destroy();
    }
 
}
 
session类的使用：d.php

<?php
 
require_once 'b.php';
 
session_start();
 
$session = new Session();
$session->set('wan','kkkk',1966);
 
$session->set('name','xiaobudiu');
$session->set('age',26);
$session->set('sex','man');
 
 
//输出当前会话的session存储数据
var_dump($_SESSION);
 
 
 
//unset掉某一个session属性
//$session->clear('name');
 
//删除当前session_id对应session文件
//$session->destroy();
//echo $session->get('sex');
 

redis中显示：

```

## 高阶篇

### GD库 图像处理扩展
```
[GD 和图像处理 函数](http://php.net/manual/zh/ref.image.php)
gd_info — 取得当前安装的 GD 库的信息
getimagesize — 取得图像大小
getimagesizefromstring — 从字符串中获取图像尺寸信息
image_type_to_extension — 取得图像类型的文件后缀
image_type_to_mime_type — 取得 getimagesize，exif_read_data，exif_thumbnail，exif_imagetype 所返回的图像类型的 MIME 类型
image2wbmp — 以 WBMP 格式将图像输出到浏览器或文件
imageaffine — 返回经过仿射变换后的图像，剪切区域可选
imageaffinematrixconcat — Concatenate two affine transformation matrices
imageaffinematrixget — Get an affine transformation matrix
imagealphablending — 设定图像的混色模式
imageantialias — 是否使用抗锯齿（antialias）功能
imagearc — 画椭圆弧
imagebmp — Output a BMP image to browser or file
imagechar — 水平地画一个字符
imagecharup — 垂直地画一个字符
imagecolorallocate — 为一幅图像分配颜色
imagecolorallocatealpha — 为一幅图像分配颜色 + alpha
imagecolorat — 取得某像素的颜色索引值
imagecolorclosest — 取得与指定的颜色最接近的颜色的索引值
imagecolorclosestalpha — 取得与指定的颜色加透明度最接近的颜色
imagecolorclosesthwb — 取得与给定颜色最接近的色度的黑白色的索引
imagecolordeallocate — 取消图像颜色的分配
imagecolorexact — 取得指定颜色的索引值
imagecolorexactalpha — 取得指定的颜色加透明度的索引值
imagecolormatch — 使一个图像中调色板版本的颜色与真彩色版本更能匹配
imagecolorresolve — 取得指定颜色的索引值或有可能得到的最接近的替代值
imagecolorresolvealpha — 取得指定颜色 + alpha 的索引值或有可能得到的最接近的替代值
imagecolorset — 给指定调色板索引设定颜色
imagecolorsforindex — 取得某索引的颜色
imagecolorstotal — 取得一幅图像的调色板中颜色的数目
imagecolortransparent — 将某个颜色定义为透明色
imageconvolution — 用系数 div 和 offset 申请一个 3x3 的卷积矩阵
imagecopy — 拷贝图像的一部分
imagecopymerge — 拷贝并合并图像的一部分
imagecopymergegray — 用灰度拷贝并合并图像的一部分
imagecopyresampled — 重采样拷贝部分图像并调整大小
imagecopyresized — 拷贝部分图像并调整大小
imagecreate — 新建一个基于调色板的图像
imagecreatefrombmp — 由文件或 URL 创建一个新图象。
imagecreatefromgd2 — 从 GD2 文件或 URL 新建一图像
imagecreatefromgd2part — 从给定的 GD2 文件或 URL 中的部分新建一图像
imagecreatefromgd — 从 GD 文件或 URL 新建一图像
imagecreatefromgif — 由文件或 URL 创建一个新图象。
imagecreatefromjpeg — 由文件或 URL 创建一个新图象。
imagecreatefrompng — 由文件或 URL 创建一个新图象。
imagecreatefromstring — 从字符串中的图像流新建一图像
imagecreatefromwbmp — 由文件或 URL 创建一个新图象。
imagecreatefromwebp — 由文件或 URL 创建一个新图象。
imagecreatefromxbm — 由文件或 URL 创建一个新图象。
imagecreatefromxpm — 由文件或 URL 创建一个新图象。
imagecreatetruecolor — 新建一个真彩色图像
imagecrop — Crop an image to the given rectangle
imagecropauto — Crop an image automatically using one of the available modes
imagedashedline — 画一虚线
imagedestroy — 销毁一图像
imageellipse — 画一个椭圆
imagefill — 区域填充
imagefilledarc — 画一椭圆弧且填充
imagefilledellipse — 画一椭圆并填充
imagefilledpolygon — 画一多边形并填充
imagefilledrectangle — 画一矩形并填充
imagefilltoborder — 区域填充到指定颜色的边界为止
imagefilter — 对图像使用过滤器
imageflip — Flips an image using a given mode
imagefontheight — 取得字体高度
imagefontwidth — 取得字体宽度
imageftbbox — 给出一个使用 FreeType 2 字体的文本框
imagefttext — 使用 FreeType 2 字体将文本写入图像
imagegammacorrect — 对 GD 图像应用 gamma 修正
imagegd2 — 将 GD2 图像输出到浏览器或文件
imagegd — 将 GD 图像输出到浏览器或文件
imagegetclip — Get the clipping rectangle
imagegif — 输出图象到浏览器或文件。
imagegrabscreen — Captures the whole screen
imagegrabwindow — Captures a window
imageinterlace — 激活或禁止隔行扫描
imageistruecolor — 检查图像是否为真彩色图像
imagejpeg — 输出图象到浏览器或文件。
imagelayereffect — 设定 alpha 混色标志以使用绑定的 libgd 分层效果
imageline — 画一条线段
imageloadfont — 载入一新字体
imageopenpolygon — Draws an open polygon
imagepalettecopy — 将调色板从一幅图像拷贝到另一幅
imagepalettetotruecolor — Converts a palette based image to true color
imagepng — 以 PNG 格式将图像输出到浏览器或文件
imagepolygon — 画一个多边形
imagepsbbox — 给出一个使用 PostScript Type1 字体的文本方框
imagepsencodefont — 改变字体中的字符编码矢量
imagepsextendfont — 扩充或精简字体
imagepsfreefont — 释放一个 PostScript Type 1 字体所占用的内存
imagepsloadfont — 从文件中加载一个 PostScript Type 1 字体
imagepsslantfont — 倾斜某字体
imagepstext — 用 PostScript Type1 字体把文本字符串画在图像上
imagerectangle — 画一个矩形
imageresolution — Get or set the resolution of the image
imagerotate — 用给定角度旋转图像
imagesavealpha — 设置标记以在保存 PNG 图像时保存完整的 alpha 通道信息（与单一透明色相反）
imagescale — Scale an image using the given new width and height
imagesetbrush — 设定画线用的画笔图像
imagesetclip — Set the clipping rectangle
imagesetinterpolation — Set the interpolation method
imagesetpixel — 画一个单一像素
imagesetstyle — 设定画线的风格
imagesetthickness — 设定画线的宽度
imagesettile — 设定用于填充的贴图
imagestring — 水平地画一行字符串
imagestringup — 垂直地画一行字符串
imagesx — 取得图像宽度
imagesy — 取得图像高度
imagetruecolortopalette — 将真彩色图像转换为调色板图像
imagettfbbox — 取得使用 TrueType 字体的文本的范围
imagettftext — 用 TrueType 字体向图像写入文本
imagetypes — 返回当前 PHP 版本所支持的图像类型
imagewbmp — 以 WBMP 格式将图像输出到浏览器或文件
imagewebp — 将 WebP 格式的图像输出到浏览器或文件
imagexbm — 将 XBM 图像输出到浏览器或文件
iptcembed — 将二进制 IPTC 数据嵌入到一幅 JPEG 图像中
iptcparse — 将二进制 IPTC 块解析为单个标记
jpeg2wbmp — 将 JPEG 图像文件转换为 WBMP 图像文件
png2wbmp — 将 PNG 图像文件转换为 WBMP 图像文件
```

### yaf
```
[Yet Another Framework](http://php.net/manual/zh/book.yaf.php)
使用框架会降低性能, 经常举例的就是Zend Framework，采用框架能提高开发效率, 损失点性能也是值得的。
有的项目组为了性能而选择某些框架，而另外一些项目组,，则为了更好的封装选择了另外的框架。
Yaf框架既不会有损性能, 又能提高开发效率。
Yaf有着和Zend Framework相似的API，相似的理念,，而同时又保持着对Bingo的兼容,，以此来提高开发效率,，规范开发习惯。本着对性能的追求, Yaf把框架中不易变的部分抽象出来，采用PHP扩展实现(c语言)，以此来保证性能。在作者自己做的简单测试中，Yaf和原生的PHP在同样功能下，性能损失小于10%，而和Zend Framework的对比中，Yaf的性能是Zend Framework的50-60倍。
Yaf是一个C语言编写的PHP框架。
Yaf的优点
用C语言开发的PHP框架，相比原生的PHP，几乎不会带来额外的性能开销。
所有的框架类，不需要编译，在PHP启动的时候加载，并常驻内存。
更短的内存周转周期，提高内存利用率，降低内存占用率。
灵巧的自动加载。支持全局和局部两种加载规则，方便类库共享。
高性能的视图引擎。
高度灵活可扩展的框架，支持自定义视图引擎，支持插件，支持自定义路由等等。
内建多种路由, 可以兼容目前常见的各种路由协议.
强大而又高度灵活的配置文件支持. 并支持缓存配置文件, 避免复杂的配置结构带来的性能损失.
在框架本身,对危险的操作习惯做了禁止.
更快的执行速度, 更少的内存占用.
名词解释
1、Yaf Yet Another Framework

| adv. | 用于否定句和疑问句，谈论尚未发生但可能发生的事; 现在; 即刻; 马上; 从现在起直至某一时间; 还; |

| conj. | 但是; 然而; |

[例句]They haven't finished yet

他们还没有完成。

2、API （Application Programming Interface，应用程序编程接口）是一些预先定义的函数，目的是提供应用程序与开发人员基于某软件或硬件得以访问一组例程的能力，而又无需访问源码，或理解内部工作机制的细节。

3、bootstrap v.独自创立; 靠一己之力做成; 附属于; 与…相联系;

```

### curl
```
[Client URL](http://php.net/manual/zh/book.curl.php)
Client URL 库
简介
安装／配置
需求
安装
运行时配置
资源类型
预定义常量
范例
curl 基础例子
cURL 函数
curl_close — 关闭 cURL 会话
curl_copy_handle — 复制一个cURL句柄和它的所有选项
curl_errno — 返回最后一次的错误代码
curl_error — 返回当前会话最后一次错误的字符串
curl_escape — 使用 URL 编码给定的字符串
curl_exec — 执行 cURL 会话
curl_file_create — 创建一个 CURLFile 对象
curl_getinfo — 获取一个cURL连接资源句柄的信息
curl_init — 初始化 cURL 会话
curl_multi_add_handle — 向curl批处理会话中添加单独的curl句柄
curl_multi_close — 关闭一组cURL句柄
curl_multi_errno — 返回上一次 curl 批处理的错误码
curl_multi_exec — 运行当前 cURL 句柄的子连接
curl_multi_getcontent — 如果设置了CURLOPT_RETURNTRANSFER，则返回获取的输出的文本流
curl_multi_info_read — 获取当前解析的cURL的相关传输信息
curl_multi_init — 返回一个新cURL批处理句柄
curl_multi_remove_handle — 移除cURL批处理句柄资源中的某个句柄资源
curl_multi_select — 等待所有cURL批处理中的活动连接
curl_multi_setopt — 为 cURL 并行处理设置一个选项
curl_multi_strerror — 返回字符串描述的错误代码
curl_pause — 暂停和取消暂停一个连接。
curl_reset — 重置一个 libcurl 会话句柄的所有的选项
curl_setopt_array — 为 cURL 传输会话批量设置选项
curl_setopt — 设置 cURL 传输选项
curl_share_close — 关闭 cURL 共享句柄
curl_share_errno — 返回共享 curl 句柄的最后一次错误号
curl_share_init — 初始化一个 cURL 共享句柄。
curl_share_setopt — 为 cURL 共享句柄设置选项。
curl_share_strerror — 返回错误号对应的错误消息
curl_strerror — 返回错误代码的字符串描述
curl_unescape — 解码给定的 URL 编码的字符串
curl_version — 获取 cURL 版本信息
CURLFile — CURLFile 类
CURLFile::__construct — 创建 CURLFile 对象
CURLFile::getFilename — 获取被上传文件的 文件名
CURLFile::getMimeType — 获取被上传文件的 MIME 类型
CURLFile::getPostFilename — 获取 POST 请求时使用的 文件名
CURLFile::setMimeType — 设置被上传文件的 MIME 类型
CURLFile::setPostFilename — 设置 POST 请求时使用的文件名
CURLFile::__wakeup — 反序列化句柄
```

### mysqli
```
[mysql增强版](http://php.net/manual/zh/book.mysqli.php)
MySQL增强版扩展
简介
Overview
Quick start guide
Dual procedural and object-oriented interface
Connections
Executing statements
Prepared Statements
Stored Procedures
Multiple Statements
API support for transactions
Metadata
安装／配置
需求
安装
运行时配置
资源类型
mysqli 扩展和持久化连接
预定义常量
Notes
MySQLi 扩展的功能概述
MySQLi — MySQLi类
mysqli::$affected_rows — Gets the number of affected rows in a previous MySQL operation
mysqli::autocommit — 打开或关闭本次数据库连接的自动命令提交事务模式
mysqli::begin_transaction — Starts a transaction
mysqli::change_user — Changes the user of the specified database connection
mysqli::character_set_name — 返回当前数据库连接的默认字符编码
mysqli::close — 关闭先前打开的数据库连接
mysqli::commit — 提交一个事务
mysqli::$connect_errno — Returns the error code from last connect call
mysqli::$connect_error — Returns a string description of the last connect error
mysqli::__construct — Open a new connection to the MySQL server
mysqli::debug — Performs debugging operations
mysqli::dump_debug_info — 将调试信息输出到日志
mysqli::errno — 返回最近函数调用的错误代码
mysqli::$error_list — Returns a list of errors from the last command executed
mysqli::$error — Returns a string description of the last error
mysqli::$field_count — Returns the number of columns for the most recent query
mysqli::get_charset — Returns a character set object
mysqli::$client_info — 获取 MySQL 客户端信息
mysqli_get_client_version — 作为一个整数返回MySQL客户端的版本
mysqli::get_connection_stats — 返回客户端连接的统计数据
mysqli::$host_info — 返回一个表述使用的连接类型的字符串
mysqli::$protocol_version — 返回MySQL使用的协议版本号
mysqli::$server_info — 返回MySQL服务器的版本号
mysqli::$server_version — 作为一个整数返回MySQL服务器的版本
mysqli::get_warnings — Get result of SHOW WARNINGS
mysqli::$info — 返回最近执行的 SQL 语句的信息
mysqli::init — 初始化 MySQLi 并返回一个资源类型的值，这个值可以作为 mysqli_real_connect() 函数的传入参数
mysqli::$insert_id — 返回最后一条插入语句产生的自增 ID
mysqli::kill — 让服务器杀掉一个 MySQL 线程
mysqli::more_results — 检查批量查询中是否还有查询结果
mysqli::multi_query — 执行查询
mysqli::next_result — 为读取 multi_query 执行之后的下一个结果集做准备
mysqli::options — 设置选项
mysqli::ping — ping 一个连接，或者如果连接处于断开状态，重新连接
mysqli::poll — 轮询连接
mysqli::prepare — 准备执行一个 SQL 语句
mysqli::query — 对数据库执行一次查询
mysqli::real_connect — 建立一个 MySQL 服务器连接
mysqli::real_escape_string — 根据当前连接的字符集，对于 SQL 语句中的特殊字符进行转义
mysqli::real_query — 执行一个mysql查询
mysqli::reap_async_query — 获取异步查询的结果
mysqli::refresh — 刷新
mysqli::release_savepoint — 从当前事务的保存点中移除一个命名保存点
mysqli::rollback — 回退当前事务
mysqli::rpl_query_type — 返回 RPL 查询类型
mysqli::savepoint — 在当前事务中增加一个命名保存点
mysqli::select_db — 选择用于数据库查询的默认数据库
mysqli::send_query — 发送请求并返回结果
mysqli::set_charset — 设置默认字符编码
mysqli::set_local_infile_default — 取消用户指定的回调函数
mysqli::set_local_infile_handler — 设置 LOAD DATA LOCAL INFILE 命令的回调函数
mysqli::$sqlstate — 返回上一次 SQL 操作的 SQLSTATE 错误信息
mysqli::ssl_set — 使用 SSL 建立到数据库之间的安全连接
mysqli::stat — 获取当前系统状态信息
mysqli::stmt_init — 初始化一条语句并返回一个用于mysqli_stmt_prepare(调用)的对象
mysqli::store_result — 转移上一次查询返回的结果集
mysqli::$thread_id — 返回当前连接的线程 ID
mysqli::thread_safe — 返回是否是线程安全的
mysqli::use_result — Initiate a result set retrieval
mysqli::$warning_count — Returns the number of warnings from the last query for the given link
MySQLi_STMT — MySQLi_STMT类
mysqli_stmt::$affected_rows — Returns the total number of rows changed, deleted, or inserted by the last executed statement
mysqli_stmt::attr_get — Used to get the current value of a statement attribute
mysqli_stmt::attr_set — Used to modify the behavior of a prepared statement
mysqli_stmt::bind_param — Binds variables to a prepared statement as parameters
mysqli_stmt::bind_result — Binds variables to a prepared statement for result storage
mysqli_stmt::close — Closes a prepared statement
mysqli_stmt::__construct — Constructs a new mysqli_stmt object
mysqli_stmt::data_seek — Seeks to an arbitrary row in statement result set
mysqli_stmt::$errno — Returns the error code for the most recent statement call
mysqli_stmt::$error_list — Returns a list of errors from the last statement executed
mysqli_stmt::$error — Returns a string description for last statement error
mysqli_stmt::execute — Executes a prepared Query
mysqli_stmt::fetch — Fetch results from a prepared statement into the bound variables
mysqli_stmt::$field_count — Returns the number of field in the given statement
mysqli_stmt::free_result — Frees stored result memory for the given statement handle
mysqli_stmt::get_result — Gets a result set from a prepared statement
mysqli_stmt::get_warnings — Get result of SHOW WARNINGS
mysqli_stmt::$insert_id — Get the ID generated from the previous INSERT operation
mysqli_stmt::more_results — Check if there are more query results from a multiple query
mysqli_stmt::next_result — Reads the next result from a multiple query
mysqli_stmt::$num_rows — Return the number of rows in statements result set
mysqli_stmt::$param_count — Returns the number of parameter for the given statement
mysqli_stmt::prepare — Prepare an SQL statement for execution
mysqli_stmt::reset — Resets a prepared statement
mysqli_stmt::result_metadata — Returns result set metadata from a prepared statement
mysqli_stmt::send_long_data — Send data in blocks
mysqli_stmt::$sqlstate — Returns SQLSTATE error from previous statement operation
mysqli_stmt::store_result — Transfers a result set from a prepared statement
mysqli_result — mysqli_result类
mysqli_result::$current_field — Get current field offset of a result pointer
mysqli_result::data_seek — Adjusts the result pointer to an arbitrary row in the result
mysqli_result::fetch_all — Fetches all result rows as an associative array, a numeric array, or both
mysqli_result::fetch_array — Fetch a result row as an associative, a numeric array, or both
mysqli_result::fetch_assoc — Fetch a result row as an associative array
mysqli_result::fetch_field_direct — Fetch meta-data for a single field
mysqli_result::fetch_field — Returns the next field in the result set
mysqli_result::fetch_fields — Returns an array of objects representing the fields in a result set
mysqli_result::fetch_object — Returns the current row of a result set as an object
mysqli_result::fetch_row — Get a result row as an enumerated array
mysqli_result::$field_count — Get the number of fields in a result
mysqli_result::field_seek — Set result pointer to a specified field offset
mysqli_result::free — Frees the memory associated with a result
mysqli_result::$lengths — Returns the lengths of the columns of the current row in the result set
mysqli_result::$num_rows — Gets the number of rows in a result
MySQLi_Driver — MySQLi_Driver类
mysqli_driver::embedded_server_end — Stop embedded server
mysqli_driver::embedded_server_start — Initialize and start embedded server
mysqli_driver::$report_mode — Enables or disables internal report functions
MySQLi_Warning — MySQLi_Warning类
mysqli_warning::__construct — The __construct purpose
mysqli_warning::next — Fetch next warning
mysqli_sql_exception — mysqli异常类
别名和过时的 Mysqli 函数
mysqli_bind_param — mysqli_stmt_bind_param 的别名
mysqli_bind_result — mysqli_stmt_bind_result 的别名
mysqli_client_encoding — mysqli_character_set_name 的别名
mysqli_connect — 别名 mysqli::__construct
mysqli::disable_reads_from_master — 在主从服务器结构中，禁用从主机读取数据
mysqli_disable_rpl_parse — 禁用RPL解析
mysqli_enable_reads_from_master — 开启从主机读取
mysqli_enable_rpl_parse — 开启RPL解析
mysqli_escape_string — 别名 mysqli_real_escape_string
mysqli_execute — mysqli_stmt_execute 的别名
mysqli_fetch — mysqli_stmt_fetch 的别名。
mysqli_get_cache_stats — 返回客户端Zval缓存统计信息
mysqli_get_client_stats — 返回客户端进程统计信息
mysqli_get_links_stats — 返回打开和缓存的链接相关信息
mysqli_get_metadata — mysqli_stmt_result_metadata 的别名
mysqli_master_query — 在主/从机制中强制在主机中执行一个查询
mysqli_param_count — mysqli_stmt_param_count 的别名
mysqli_report — 别名 mysqli_driver->report_mode
mysqli_rpl_parse_enabled — 检查是否开启了 RPL 解析
mysqli_rpl_probe — RPL 探测
mysqli_send_long_data — mysqli_stmt_send_long_data 的别名
mysqli::set_opt — Alias of mysqli_options
mysqli_slave_query — 在主/从机制中强制在从机上执行一个查询
```

### php7
```
[php7新特性](http://www.runoob.com/w3cnote/php7-new-features.html)
PHP 7 新特性
分类 PHP 常用实例
标量类型声明
PHP 7 中的函数的形参类型声明可以是标量了。在 PHP 5 中只能是类名、接口、array 或者 callable (PHP 5.4，即可以是函数，包括匿名函数)，现在也可以使用 string、int、float和 bool 了。

<?php
// 强制模式
function sumOfInts(int ...$ints)
{
    return array_sum($ints);
}

var_dump(sumOfInts(2, '3', 4.1));
以上实例会输出：

int(9)
需要注意的是上文提到的严格模式的问题在这里同样适用：强制模式（默认，既强制类型转换）下还是会对不符合预期的参数进行强制类型转换，严格模式下则触发 TypeError 的致命错误。

返回值类型声明
PHP 7 增加了对返回类型声明的支持。 类似于参数类型声明，返回类型声明指明了函数返回值的类型。可用的类型与参数声明中可用的类型相同。

<?php

function arraysSum(array ...$arrays): array
{
    return array_map(function(array $array): int {
        return array_sum($array);
    }, $arrays);
}

print_r(arraysSum([1,2,3], [4,5,6], [7,8,9]));
以上实例会输出：

Array
(
    [0] => 6
    [1] => 15
    [2] => 24
)
NULL 合并运算符
由于日常使用中存在大量同时使用三元表达式和 isset()的情况，NULL 合并运算符使得变量存在且值不为NULL， 它就会返回自身的值，否则返回它的第二个操作数。

实例如下：

<?php
// 如果 $_GET['user'] 不存在返回 'nobody'，否则返回 $_GET['user'] 的值
$username = $_GET['user'] ?? 'nobody';
// 类似的三元运算符
$username = isset($_GET['user']) ? $_GET['user'] : 'nobody';
?>
太空船操作符（组合比较符）
太空船操作符用于比较两个表达式。当$a大于、等于或小于$b时它分别返回-1、0或1。

实例如下：

<?php
// 整型
echo 1 <=> 1; // 0
echo 1 <=> 2; // -1
echo 2 <=> 1; // 1

// 浮点型
echo 1.5 <=> 1.5; // 0
echo 1.5 <=> 2.5; // -1
echo 2.5 <=> 1.5; // 1
 
// 字符串
echo "a" <=> "a"; // 0
echo "a" <=> "b"; // -1
echo "b" <=> "a"; // 1
?>
通过 define() 定义常量数组
实例如下：

<?php
define('ANIMALS', [
    'dog',
    'cat',
    'bird'
]);

echo ANIMALS[1]; // 输出 "cat"
?>
匿名类
现在支持通过new class 来实例化一个匿名类，实例如下：

<?php
interface Logger {
    public function log(string $msg);
}

class Application {
    private $logger;

    public function getLogger(): Logger {
         return $this->logger;
    }

    public function setLogger(Logger $logger) {
         $this->logger = $logger;
    }
}

$app = new Application;
$app->setLogger(new class implements Logger {
    public function log(string $msg) {
        echo $msg;
    }
});

var_dump($app->getLogger());
?>
以上实例会输出：

object(class@anonymous)#2 (0) {
}
Unicode codepoint 转译语法
这接受一个以16进制形式的 Unicode codepoint，并打印出一个双引号或heredoc包围的 UTF-8 编码格式的字符串。 可以接受任何有效的 codepoint，并且开头的 0 是可以省略的。

echo "\u{aa}";
echo "\u{0000aa}";
echo "\u{9999}";
以上实例会输出：

ª
ª (same as before but with optional leading 0's)
香
Closure::call()
Closure::call() 现在有着更好的性能，简短干练的暂时绑定一个方法到对象上闭包并调用它。

<?php
class A {private $x = 1;}

// Pre PHP 7 代码
$getXCB = function() {return $this->x;};
$getX = $getXCB->bindTo(new A, 'A'); // intermediate closure
echo $getX();

// PHP 7+ 代码
$getX = function() {return $this->x;};
echo $getX->call(new A);
以上实例会输出：

1
1
为unserialize()提供过滤
这个特性旨在提供更安全的方式解包不可靠的数据。它通过白名单的方式来防止潜在的代码注入。

<?php

// 转换对象为 __PHP_Incomplete_Class 对象
$data = unserialize($foo, ["allowed_classes" => false]);

// 转换对象为 __PHP_Incomplete_Class 对象，除了 MyClass 和 MyClass2
$data = unserialize($foo, ["allowed_classes" => ["MyClass", "MyClass2"]);

// 默认接受所有类
$data = unserialize($foo, ["allowed_classes" => true]);
IntlChar
新增加的 IntlChar 类旨在暴露出更多的 ICU 功能。这个类自身定义了许多静态方法用于操作多字符集的 unicode 字符。

<?php
printf('%x', IntlChar::CODEPOINT_MAX);
echo IntlChar::charName('@');
var_dump(IntlChar::ispunct('!'));
以上实例会输出：

10ffff
COMMERCIAL AT
bool(true)
若要使用此类，请先安装Intl扩展

预期
预期是向后兼用并增强之前的 assert() 的方法。 它使得在生产环境中启用断言为零成本，并且提供当断言失败时抛出特定异常的能力。

<?php
ini_set('assert.exception', 1);

class CustomError extends AssertionError {}

assert(false, new CustomError('Some error message'));
?>
以上实例会输出：

Fatal error: Uncaught CustomError: Some error message
use 加强
从同一 namespace 导入的类、函数和常量现在可以通过单个 use 语句 一次性导入了。

<?php

//  PHP 7 之前版本用法
use some\namespace\ClassA;
use some\namespace\ClassB;
use some\namespace\ClassC as C;

use function some\namespace\fn_a;
use function some\namespace\fn_b;
use function some\namespace\fn_c;

use const some\namespace\ConstA;
use const some\namespace\ConstB;
use const some\namespace\ConstC;

// PHP 7+ 用法
use some\namespace\{ClassA, ClassB, ClassC as C};
use function some\namespace\{fn_a, fn_b, fn_c};
use const some\namespace\{ConstA, ConstB, ConstC};
?>
Generator 加强
增强了Generator的功能，这个可以实现很多先进的特性

<?php
<?php

function gen()
{
    yield 1;
    yield 2;

    yield from gen2();
}

function gen2()
{
    yield 3;
    yield 4;
}

foreach (gen() as $val)
{
    echo $val, PHP_EOL;
}

?>
以上实例会输出：

1
2
3
4
整除
新增了整除函数 intdiv(),使用实例：

<?php
var_dump(intdiv(10, 3));
?>
以上实例会输出：

int(3)
```

### PHP 数组底层实现 （HashTable + Linked list）
```
https://zhuanlan.zhihu.com/p/97762122
https://www.jb51.net/article/168406.htm
```

### Copy on write 原理，何时 GC
```
https://www.jb51.net/article/50079.htm
https://segmentfault.com/a/1190000014024336
```

### PHP 进程模型，进程通讯方式，进程线程区别
```
http://www.gxlcms.com/PHPjiqiao-378142.html
```

### yield 核心原理是什么
```
https://www.php.cn/faq/453725.html
```

### PDO prepare 原理
```
https://www.cnblogs.com/DataArt/p/10240829.html
https://www.jb51.net/article/56612.htm
```

### PHP 7 与 PHP 5 有什么区别
```
https://www.cnblogs.com/FLy-1992/p/11647839.html
https://www.jb51.net/article/171609.htm
https://zhuanlan.zhihu.com/p/96785667
```

### Swoole 适用场景，协程实现方式
```
https://blog.csdn.net/assasin0308/article/details/93649725
https://segmentfault.com/a/1190000019089997?utm_source=tag-newest
```

## 前端篇

### 原生获取 DOM 节点，属性
```
https://www.cnblogs.com/zero18/p/10998642.html
```

### 盒子模型
```
https://www.runoob.com/css/css-boxmodel.html
```

### CSS 文件、style 标签、行内 style 属性优先级
```
http://www.liangshunet.com/ca/201705/479457580.htm
```

### HTML 与 JS 运行顺序（页面 JS 从上到下）
```
https://www.cnblogs.com/xiaoxiaoqiang001/p/5331322.html
```

### JS 数组操作
```
https://www.cnblogs.com/lzm1989/p/5967815.html
```

### 类型判断
```
https://www.cnblogs.com/wilfredo/p/10489142.html
```

### this 作用域
```
https://www.cnblogs.com/wxy1233/p/9892264.html
```

### .map() 与 this 具体使用场景分析
```
https://www.cnblogs.com/mhtss/p/11334034.html
https://blog.csdn.net/weixin_42881768/article/details/104648611
```

### Cookie 读写
```
https://www.cnblogs.com/rxbook/p/11773553.html
```

### JQuery 操作
```
https://www.cnblogs.com/gaohuayan/p/11304745.html
```

### Ajax 请求（同步、异步区别）随机数禁止缓存
```
https://www.cnblogs.com/liu-zhao/p/7055250.html
```

### Bootstrap 有什么好处
```
https://www.php.cn/bootstrap/425318.html
```

### 跨域请求 N 种解决方案
```
https://www.cnblogs.com/momo798/p/6164124.html
```

### ES6
```
https://www.runoob.com/w3cnote/es6-tutorial.html
```

### 模块化
```
https://www.jianshu.com/p/8573cdcde863
```

### 打包
```
http://www.fly63.com/article/detial/3628
```

### 构建工具
```
https://www.cnblogs.com/lihuijuan/p/9296315.html
```

### vue、react、webpack、
```
http://www.97yrbl.com/portal.php?mod=view&aid=353
```

### 前端 mvc 
```
https://www.zhihu.com/question/267581572
https://segmentfault.com/a/1190000009127861
```

### 优化
```
https://www.cnblogs.com/tianshu/p/10555921.html
```

### 如何处理负载、高并发
```
从低成本、高性能和高扩张性的角度来说有如下处理方案：
1. HTML静态化
其实大家都知道，效率最高、消耗最小的就是纯静态化的html页面，
所以我们尽可能使我们的 网站上的页面采用静态页面来实现，这个最简单的方法其实也是最有效的方法。
2. 图片服务器分离
把图片单独存储，尽量减少图片等大流量的开销，可以放在一些相关的平台上，如骑牛等
3. 数据库集群和库表散列及缓存
数据库的并发连接为100，一台数据库远远不够，可以从读写分离、主从复制，数据库集群方面来着手。
另外尽量减少数据库的访问，可以使用缓存数据库如memcache、redis。
4. 镜像：
尽量减少下载，可以把不同的请求分发到多个镜像端。
5. 负载均衡：
Apache的最大并发连接为1500，只能增加服务器，可以从硬件上着手，如F5服务器。
当然硬件的成本比较高，我们往往从软件方面着手。
###*负载均衡**建立在现有网络结构之上，
它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力，
同时能够提高网络的灵活性和可用性。目前使用最为广泛的负载均衡软件是Nginx、LVS、HAProxy。
- 知识点： [反向代理](https://www.cnblogs.com/Anker/p/6056540.html)
```

### 浏览器单域名并发数限制
```
https://www.cnblogs.com/sunsky303/p/8862128.html
```

### 静态资源缓存 304 （If-Modified-Since 以及 Etag 原理）
```
https://blog.csdn.net/lihongjun_0204/article/details/84096679
https://blog.csdn.net/weixin_43915587/article/details/95499873
```
### 多个小图标合并使用 position 定位技术 减少请求
```
https://blog.csdn.net/ldl_xz/article/details/84804133
```
### 静态资源合为单次请求 并压缩
```
https://www.cnblogs.com/LO-ME/p/7523596.html
```
### CDN
```
https://www.bejson.com/othertools/libcdn/
```
### 静态资源延迟加载技术、预加载技术
```
https://www.cnblogs.com/leyan/p/6085148.html
```
### keep-alive
```
https://www.jianshu.com/p/9523bb439950
```

### CSS 在头部，JS 在尾部的优化（原理）
```
https://www.cnblogs.com/straybirds/p/11108124.html
```

## 网络篇
### IP 地址转 INT
```
https://www.cnblogs.com/zhshlimi/p/10715911.html
https://www.cnblogs.com/shanyansheng/p/5580232.html
```
### 192.168.0.1/16 是什么意思
```
https://blog.csdn.net/aerchi/article/details/39396423?t=1476605876076
```
### DNS 主要作用是什么？
```
https://www.php.cn/faq/442356.html
```
### IPv4 与 v6 区别
```
https://www.php.cn/windows-414619.html
```

## 网络编程篇

### TCP 三次握手流程
```
https://blog.csdn.net/xuezhiwu001/article/details/78587439
```

### TCP、UDP 区别，分别适用场景
```
https://www.cnblogs.com/liangyc/p/11628208.html
```

### 有什么办法能保证 UDP 高可用性(了解)
```
https://blog.csdn.net/qq_37651267/article/details/93368908
```

### TCP 粘包如何解决？
```
https://blog.csdn.net/feng020a/article/details/60587726
```

### 为什么需要心跳？
```
https://www.cnblogs.com/coderzh/p/WhyHeartBeatNeeded.html
```

### 什么是长连接？
```
https://www.jb51.net/article/135006.htm
```

### HTTPS 是怎么保证安全的？
```
https://www.php.cn/safe/452112.html
```

### 流与数据报的区别
```
https://phpor.net/blog/post/1223
```

### 进程间通信几种方式，最快的是哪种？
```
https://blog.csdn.net/rorntuck7/article/details/84565211
```

### `fork()` 会发生什么？
```
https://zhidao.baidu.com/question/268326157628607205.html
```

## API 篇

### RESTful 是什么
```
https://www.runoob.com/php/php-restful.html
```

### 如何在不支持 `DELETE` 请求的浏览器上兼容 `DELETE` 请求
```
https://www.cnblogs.com/snandy/archive/2012/04/20/2459065.html
```

### 常见 API 的 `APP_ID` `APP_SECRET` 主要作用是什么？阐述下流程 
```
https://blog.csdn.net/qq_24656927/article/details/54910875
```

### API 请求如何保证数据不被篡改？
```
https://www.oschina.net/question/228530_2288017
```

### JSON 和 JSONP 的区别
```
https://www.cnblogs.com/bjfy/p/5461571.html
```

### 数据加密和验签的区别
```
https://www.jb51.net/article/62091.htm
```

### RSA 是什么
```
https://www.cnblogs.com/hzijone/p/6306818.html
```

### API 版本兼容怎么处理
```
https://www.cnblogs.com/vus520/p/3152772.html
```

### 限流（木桶、令牌桶）
```
https://www.jianshu.com/p/9f76dd2757c7
```

### OAuth 2 主要用在哪些场景下
```
https://blog.csdn.net/u33445687/article/details/107655728
```

### JWT
```
https://www.cnblogs.com/heyue0117/p/11917540.html
```

### PHP 中 `json_encode(['key'=>123]);` 与 `return json_encode([]);` 区别，会产生什么问题？如何解决
```
https://www.cnblogs.com/dadiaomengmei/p/11212344.html
```

## 加分项
### 了解常用语言特性，及不同场景适用性。
```
```

### PHP VS Golang
```
https://www.php.cn/be/go/421167.html
```

### PHP VS Python
```
https://blog.csdn.net/csdnnews/article/details/88373787
```

### PHP VS JAVA
```
https://www.cnblogs.com/itplay/p/10727690.html
```

### 了解 PHP 扩展开发
```
https://www.cnblogs.com/tosser/p/11564818.html
```

### 熟练掌握 C/C++
```
```
